{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#the-well-15tb-of-physics-simulations","title":"The Well: 15TB of Physics Simulations","text":"<p>Welcome to the Well, a large-scale collection of machine learning datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain scientists and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite for accelerating research in machine learning and computational sciences.</p>"},{"location":"#tap-into-the-well","title":"Tap into the Well","text":"<p>Once the Well package installed and the data downloaded you can use them in your training pipeline.</p> <pre><code>from the_well.data import WellDataset\nfrom torch.utils.data import DataLoader\n\ntrainset = WellDataset(\n    well_base_path=\"path/to/base\",\n    well_dataset_name=\"name_of_the_dataset\",\n    well_split_name=\"train\"\n)\ntrain_loader = DataLoader(trainset)\n\nfor batch in train_loader:\n    ...\n</code></pre> <p>For more information regarding the interface, please refer to the API and the tutorials.</p>"},{"location":"#installation","title":"Installation","text":"<p>If you plan to use The Well datasets to train or evaluate deep learning models, we recommend to use a machine with enough computing resources. We also recommend creating a new Python (&gt;=3.10) environment to install the Well. For instance, with venv:</p> <pre><code>python -m venv path/to/env\nsource path/to/env/activate/bin\n</code></pre>"},{"location":"#from-pypi","title":"From PyPI","text":"<p>The Well package can be installed directly from PyPI.</p> <pre><code>pip install the_well\n</code></pre>"},{"location":"#from-source","title":"From Source","text":"<p>It can also be installed from source. For this, clone the repository and install the package with its dependencies.</p> <pre><code>git clone https://github.com/PolymathicAI/the_well\ncd the_well\npip install .\n</code></pre> <p>Depending on your acceleration hardware, you can specify <code>--extra-index-url</code> to install the relevant PyTorch version. For example, use</p> <pre><code>pip install . --extra-index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>to install the dependencies built for CUDA 12.1.</p>"},{"location":"#benchmark-dependencies","title":"Benchmark Dependencies","text":"<p>If you want to run the benchmarks, you should install additional dependencies.</p> <pre><code>pip install the_well[benchmark]\n</code></pre>"},{"location":"#downloading-the-data","title":"Downloading the Data","text":"<p>The Well datasets range between 6.9GB and 5.1TB of data each, for a total of 15TB for the full collection. Ensure that your system has enough free disk space to accomodate the datasets you wish to download.</p> <p>Once <code>the_well</code> is installed, you can use the <code>the-well-download</code> command to download any dataset of The Well.</p> <pre><code>the-well-download --base-path path/to/base --dataset active_matter --split train\n</code></pre> <p>If <code>--dataset</code> and <code>--split</code> are omitted, all datasets and splits will be downloaded. This could take a while!</p>"},{"location":"#streaming-from-hugging-face","title":"Streaming from Hugging Face","text":"<p>Most of the Well datasets are also hosted on Hugging Face. Data can be streamed directly from the hub using the following code.</p> <pre><code>from the_well.data import WellDataset\nfrom torch.utils.data import DataLoader\n\n# The following line may take a couple of minutes to instantiate the datamodule\ntrainset = WellDataset(\n    well_base_path=\"hf://datasets/polymathic-ai/\",  # access from HF hub\n    well_dataset_name=\"active_matter\",\n    well_split_name=\"train\",\n)\ntrain_loader = DataLoader(trainset)\n\nfor batch in train_loader:\n    ...\n</code></pre> <p>For better performance in large training, we advise downloading the data locally instead of streaming it over the network.</p>"},{"location":"#benchmark","title":"Benchmark","text":"<p>The repository allows benchmarking surrogate models on the different datasets that compose the Well. Some state-of-the-art models are already implemented in <code>models</code>, while dataset classes handle the raw data of the Well. The benchmark relies on a training script that uses hydra to instantiate various classes (e.g. dataset, model, optimizer) from configuration files.</p> <p>For instance, to run the training script of default FNO architecture on the active matter dataset, launch the following commands:</p> <pre><code>cd the_well/benchmark\npython train.py experiment=fno server=local data=active_matter\n</code></pre> <p>Each argument corresponds to a specific configuration file. In the command above <code>server=local</code> indicates the training script to use <code>local.yaml</code>, which just declares the relative path to the data. The configuration can be overridden directly or edited with new YAML files. Please refer to hydra documentation for editing configuration.</p> <p>You can use this command within a sbatch script to launch the training with Slurm.</p>"},{"location":"#citation","title":"Citation","text":"<p>This project has been led by the Polymathic AI organization, in collaboration with researchers from the Flatiron Institute, University of Colorado Boulder, University of Cambridge, New York University, Rutgers University, Cornell University, University of Tokyo, Los Alamos Natioinal Laboratory, University of Califronia, Berkeley, Princeton University, CEA DAM, and University of Li\u00e8ge.</p> <p>If you find this project useful for your research, please consider citing</p> <pre><code>@article{ohana2024well,\n  title={The well: a large-scale collection of diverse physics simulations for machine learning},\n  author={Ohana, Ruben and McCabe, Michael and Meyer, Lucas and Morel, Rudy and Agocs, Fruzsina and Beneitez, Miguel and Berger, Marsha and Burkhart, Blakesly and Dalziel, Stuart and Fielding, Drummond and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={37},\n  pages={44989--45037},\n  year={2024}\n}\n</code></pre>"},{"location":"#contact","title":"Contact","text":"<p>For questions regarding this project, please contact Ruben Ohana and Michael McCabe at \\(\\small\\texttt{\\{rohana,mmccabe\\}@flatironinstitute.org}\\).</p>"},{"location":"#bug-reports-and-feature-requests","title":"Bug Reports and Feature Requests","text":"<p>To report a bug (in the data or the code), request a feature or simply ask a question, you can open an issue on the repository.</p>"},{"location":"#useful-links","title":"Useful Links","text":"<ul> <li> The Well repository on GitHub</li> <li> More information about the Well in the NeurIPS paper</li> <li> More about our work on the Polymathic AI website</li> <li> Datasets of the Well are available on Hugging Face</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#data-classes","title":"Data Classes","text":"<p>The Well provides two main class <code>WellDataset</code> and <code>WellDataModule</code> to handle the raw data that are stored in <code>.hdf5</code> files. The <code>WellDataset</code> implements a map-style PyTorch Dataset. The <code>WellDataModule</code> provides dataloaders for training, validation, and test. The tutorial provides a guide on how to use these classes in a training pipeline.</p>"},{"location":"api/#dataset","title":"Dataset","text":"<p>The <code>WellDataset</code> is a map-style dataset. It converts the <code>.hdf5</code> file structure expected by the Well into <code>torch.Tensor</code> data. It first processes metadata from the <code>.hdf5</code> attributes to allow for retrieval of individual samples.</p>"},{"location":"api/#the_well.data.WellDataset","title":"<code>the_well.data.WellDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Generic dataset for any Well data. Returns data in B x T x H [x W [x D]] x C format.</p> <p>Train/Test/Valid is assumed to occur on a folder level.</p> <p>Takes in path to directory of HDF5 files to construct dset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Path to directory of HDF5 files, one of path or well_base_path+well_dataset_name must be specified</p> <code>None</code> <code>normalization_path</code> <code>str</code> <p>Path to normalization constants - assumed to be in same format as constructed data.</p> <code>'../stats.yaml'</code> <code>well_base_path</code> <code>Optional[str]</code> <p>Path to well dataset directory, only used with dataset_name</p> <code>None</code> <code>well_dataset_name</code> <code>Optional[str]</code> <p>Name of well dataset to load - overrides path if specified</p> <code>None</code> <code>well_split_name</code> <code>Literal['train', 'valid', 'test', None]</code> <p>Name of split to load - options are 'train', 'valid', 'test'</p> <code>None</code> <code>include_filters</code> <code>List[str]</code> <p>Only include files whose name contains at least one of these strings</p> <code>[]</code> <code>exclude_filters</code> <code>List[str]</code> <p>Exclude any files whose name contains at least one of these strings</p> <code>[]</code> <code>use_normalization</code> <code>bool</code> <p>Whether to normalize data in the dataset</p> <code>False</code> <code>normlization_type</code> <p>What type of dataset normalization. Callable Options: ZSCORE and RMS</p> required <code>n_steps_input</code> <code>int</code> <p>Number of steps to include in each sample</p> <code>1</code> <code>n_steps_output</code> <code>int</code> <p>Number of steps to include in y</p> <code>1</code> <code>min_dt_stride</code> <code>int</code> <p>Minimum stride between samples</p> <code>1</code> <code>max_dt_stride</code> <code>int</code> <p>Maximum stride between samples</p> <code>1</code> <code>flatten_tensors</code> <code>bool</code> <p>Whether to flatten tensor valued field into channels</p> <code>True</code> <code>cache_small</code> <code>bool</code> <p>Whether to cache small tensors in memory for faster access</p> <code>True</code> <code>max_cache_size</code> <code>float</code> <p>Maximum numel of constant tensor to cache</p> <code>1000000000.0</code> <code>return_grid</code> <code>bool</code> <p>Whether to return grid coordinates</p> <code>True</code> <code>boundary_return_type</code> <code>str</code> <p>options=['padding', 'mask', 'exact', 'none'] How to return boundary conditions. Currently only padding supported.</p> <code>'padding'</code> <code>full_trajectory_mode</code> <code>bool</code> <p>Overrides to return full trajectory starting from t0 instead of samples     for long run validation.</p> <code>False</code> <code>name_override</code> <code>Optional[str]</code> <p>Override name of dataset (used for more precise logging)</p> <code>None</code> <code>transform</code> <code>Optional[Augmentation]</code> <p>Transform to apply to data. In the form <code>f(data: TrajectoryData, metadata: TrajectoryMetadata) -&gt; TrajectoryData</code>, where <code>data</code> contains a piece of trajectory (fields, scalars, BCs, ...) and <code>metadata</code> contains additional informations, including the dataset itself.</p> <code>None</code> <code>min_std</code> <code>float</code> <p>Minimum standard deviation for field normalization. If a field standard deviation is lower than this value, it is replaced by this value.</p> <code>0.0001</code> <code>storage_options</code> <p>Option for the ffspec storage.</p> <code>None</code> Source code in <code>the_well/data/datasets.py</code> <pre><code>class WellDataset(Dataset):\n    \"\"\"\n    Generic dataset for any Well data. Returns data in B x T x H [x W [x D]] x C format.\n\n    Train/Test/Valid is assumed to occur on a folder level.\n\n    Takes in path to directory of HDF5 files to construct dset.\n\n    Args:\n        path:\n            Path to directory of HDF5 files, one of path or well_base_path+well_dataset_name\n            must be specified\n        normalization_path:\n            Path to normalization constants - assumed to be in same format as constructed data.\n        well_base_path:\n            Path to well dataset directory, only used with dataset_name\n        well_dataset_name:\n            Name of well dataset to load - overrides path if specified\n        well_split_name:\n            Name of split to load - options are 'train', 'valid', 'test'\n        include_filters:\n            Only include files whose name contains at least one of these strings\n        exclude_filters:\n            Exclude any files whose name contains at least one of these strings\n        use_normalization:\n            Whether to normalize data in the dataset\n        normlization_type:\n            What type of dataset normalization. Callable Options: ZSCORE and RMS\n        n_steps_input:\n            Number of steps to include in each sample\n        n_steps_output:\n            Number of steps to include in y\n        min_dt_stride:\n            Minimum stride between samples\n        max_dt_stride:\n            Maximum stride between samples\n        flatten_tensors:\n            Whether to flatten tensor valued field into channels\n        cache_small:\n            Whether to cache small tensors in memory for faster access\n        max_cache_size:\n            Maximum numel of constant tensor to cache\n        return_grid:\n            Whether to return grid coordinates\n        boundary_return_type: options=['padding', 'mask', 'exact', 'none']\n            How to return boundary conditions. Currently only padding supported.\n        full_trajectory_mode:\n            Overrides to return full trajectory starting from t0 instead of samples\n                for long run validation.\n        name_override:\n            Override name of dataset (used for more precise logging)\n        transform:\n            Transform to apply to data. In the form `f(data: TrajectoryData, metadata:\n            TrajectoryMetadata) -&gt; TrajectoryData`, where `data` contains a piece of\n            trajectory (fields, scalars, BCs, ...) and `metadata` contains additional\n            informations, including the dataset itself.\n        min_std:\n            Minimum standard deviation for field normalization. If a field standard\n            deviation is lower than this value, it is replaced by this value.\n        storage_options :\n            Option for the ffspec storage.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: Optional[str] = None,\n        normalization_path: str = \"../stats.yaml\",\n        well_base_path: Optional[str] = None,\n        well_dataset_name: Optional[str] = None,\n        well_split_name: Literal[\"train\", \"valid\", \"test\", None] = None,\n        include_filters: List[str] = [],\n        exclude_filters: List[str] = [],\n        use_normalization: bool = False,\n        normalization_type: Optional[Callable[..., Any]] = None,\n        max_rollout_steps=100,\n        n_steps_input: int = 1,\n        n_steps_output: int = 1,\n        min_dt_stride: int = 1,\n        max_dt_stride: int = 1,\n        flatten_tensors: bool = True,\n        cache_small: bool = True,\n        max_cache_size: float = 1e9,\n        return_grid: bool = True,\n        boundary_return_type: str = \"padding\",\n        full_trajectory_mode: bool = False,\n        name_override: Optional[str] = None,\n        transform: Optional[\"Augmentation\"] = None,\n        min_std: float = 1e-4,\n        storage_options: Optional[Dict] = None,\n    ):\n        super().__init__()\n        assert path is not None or (\n            well_base_path is not None and well_dataset_name is not None\n        ), \"Must specify path or well_base_path and well_dataset_name\"\n        if path is not None:\n            self.data_path = path\n            self.normalization_path = os.path.join(path, normalization_path)\n            if well_split_name is not None:\n                self.data_path = os.path.join(path, \"data\", well_split_name)\n\n        else:\n            assert is_dataset_in_the_well(well_dataset_name), (\n                f\"Dataset name {well_dataset_name} not in the expected list {WELL_DATASETS}.\"\n            )\n            self.data_path = os.path.join(\n                well_base_path, well_dataset_name, \"data\", well_split_name\n            )\n            self.normalization_path = os.path.join(\n                well_base_path, well_dataset_name, \"stats.yaml\"\n            )\n\n        self.fs, _ = fsspec.url_to_fs(self.data_path, **(storage_options or {}))\n\n        # Input checks\n        if boundary_return_type is not None and boundary_return_type not in [\"padding\"]:\n            raise NotImplementedError(\"Only padding boundary conditions supported\")\n        if not flatten_tensors:\n            raise NotImplementedError(\"Only flattened tensors supported right now\")\n\n        # Copy params\n        self.well_dataset_name = well_dataset_name\n        self.use_normalization = use_normalization\n        self.normalization_type = normalization_type\n        self.include_filters = include_filters\n        self.exclude_filters = exclude_filters\n        self.max_rollout_steps = max_rollout_steps\n        self.n_steps_input = n_steps_input\n        self.n_steps_output = n_steps_output  # Gets overridden by full trajectory mode\n        self.min_dt_stride = min_dt_stride\n        self.max_dt_stride = max_dt_stride\n        self.flatten_tensors = flatten_tensors\n        self.return_grid = return_grid\n        self.boundary_return_type = boundary_return_type\n        self.full_trajectory_mode = full_trajectory_mode\n        self.cache_small = cache_small\n        self.max_cache_size = max_cache_size\n        self.transform = transform\n        if self.min_dt_stride &lt; self.max_dt_stride and self.full_trajectory_mode:\n            raise ValueError(\n                \"Full trajectory mode not supported with variable stride lengths\"\n            )\n        # Check the directory has hdf5 that meet our exclusion criteria\n        sub_files = self.fs.glob(self.data_path + \"/*.h5\") + self.fs.glob(\n            self.data_path + \"/*.hdf5\"\n        )\n        # Check filters - only use file if include_filters are present and exclude_filters are not\n        if len(self.include_filters) &gt; 0:\n            retain_files = []\n            for include_string in self.include_filters:\n                retain_files += [f for f in sub_files if include_string in f]\n            sub_files = retain_files\n        if len(self.exclude_filters) &gt; 0:\n            for exclude_string in self.exclude_filters:\n                sub_files = [f for f in sub_files if exclude_string not in f]\n        assert len(sub_files) &gt; 0, \"No HDF5 files found in path {}\".format(\n            self.data_path\n        )\n        self.files_paths = sub_files\n        self.files_paths.sort()\n        self.caches = [{} for _ in self.files_paths]\n        # Build multi-index\n        self.metadata = self._build_metadata()\n        # Override name if necessary for logging\n        if name_override is not None:\n            self.dataset_name = name_override\n\n        # Initialize normalization classes if True\n        if use_normalization and normalization_type:\n            try:\n                with self.fs.open(self.normalization_path, mode=\"r\") as f:\n                    stats = yaml.safe_load(f)\n\n                if stats:\n                    self.norm = normalization_type(\n                        stats, self.core_field_names, self.core_constant_field_names\n                    )\n                else:\n                    warnings.warn(\n                        f\"Normalization file {self.normalization_path} is empty. Proceeding without normalization.\",\n                        UserWarning,\n                    )\n                    self.norm = None\n            except Exception as e:\n                warnings.warn(\n                    f\"Error loading normalization file {self.normalization_path}: {e}. Proceeding without normalization.\",\n                    UserWarning,\n                )\n                self.norm = None\n        else:\n            self.norm = None\n\n    def _build_metadata(self):\n        \"\"\"Builds multi-file indices and checks that folder contains consistent dataset\"\"\"\n        self.n_files = len(self.files_paths)\n        self.n_trajectories_per_file = []\n        self.n_steps_per_trajectory = []\n        self.n_windows_per_trajectory = []\n        self.file_index_offsets = [0]  # Used to track where each file starts\n        # Things where we just care every file has same value\n        size_tuples = set()\n        names = set()\n        ndims = set()\n        bcs = set()\n        lowest_steps = 1e9  # Note - we should never have 1e9 steps\n        for index, file in enumerate(self.files_paths):\n            with (\n                self.fs.open(file, \"rb\", **IO_PARAMS[\"fsspec_params\"]) as f,\n                h5.File(f, \"r\", **IO_PARAMS[\"h5py_params\"]) as _f,\n            ):\n                grid_type = _f.attrs[\"grid_type\"]\n                # Run sanity checks - all files should have same ndims, size_tuple, and names\n                trajectories = int(_f.attrs[\"n_trajectories\"])\n                # Number of steps is always last dim of time\n                steps = _f[\"dimensions\"][\"time\"].shape[-1]\n                size_tuple = [\n                    _f[\"dimensions\"][d].shape[-1]\n                    for d in _f[\"dimensions\"].attrs[\"spatial_dims\"]\n                ]\n                ndims.add(_f.attrs[\"n_spatial_dims\"])\n                names.add(_f.attrs[\"dataset_name\"])\n                size_tuples.add(tuple(size_tuple))\n                # Fast enough that I'd rather check each file rather than processing extra files before checking\n                assert len(names) == 1, \"Multiple dataset names found in specified path\"\n                assert len(ndims) == 1, \"Multiple ndims found in specified path\"\n                assert len(size_tuples) == 1, (\n                    \"Multiple resolutions found in specified path\"\n                )\n\n                # Track lowest amount of steps in case we need to use full_trajectory_mode\n                lowest_steps = min(lowest_steps, steps)\n\n                windows_per_trajectory = raw_steps_to_possible_sample_t0s(\n                    steps, self.n_steps_input, self.n_steps_output, self.min_dt_stride\n                )\n                assert windows_per_trajectory &gt; 0, (\n                    f\"{steps} steps is not enough steps for file {file}\"\n                    f\" to allow {self.n_steps_input} input and {self.n_steps_output} output steps\"\n                    f\" with a minimum stride of {self.min_dt_stride}\"\n                )\n                self.n_trajectories_per_file.append(trajectories)\n                self.n_steps_per_trajectory.append(steps)\n                self.n_windows_per_trajectory.append(windows_per_trajectory)\n                self.file_index_offsets.append(\n                    self.file_index_offsets[-1] + trajectories * windows_per_trajectory\n                )\n                # Check BCs\n                for bc in _f[\"boundary_conditions\"].keys():\n                    bcs.add(_f[\"boundary_conditions\"][bc].attrs[\"bc_type\"])\n\n                if index == 0:\n                    # Populate scalar names\n                    self.scalar_names = []\n                    self.constant_scalar_names = []\n\n                    for scalar in _f[\"scalars\"].attrs[\"field_names\"]:\n                        if _f[\"scalars\"][scalar].attrs[\"time_varying\"]:\n                            self.scalar_names.append(scalar)\n                        else:\n                            self.constant_scalar_names.append(scalar)\n\n                    # Populate field names\n                    self.field_names = {i: [] for i in range(3)}\n                    self.constant_field_names = {i: [] for i in range(3)}\n\n                    # Store the core names without the tensor indices appended\n                    self.core_field_names = []\n                    self.core_constant_field_names = []\n                    seen = set()\n\n                    for i in range(3):\n                        ti = f\"t{i}_fields\"\n                        # if _f[ti][field].attrs[\"symmetric\"]:\n                        # itertools.combinations_with_replacement\n                        ti_field_dims = [\n                            \"\".join(xyz)\n                            for xyz in itertools.product(\n                                _f[\"dimensions\"].attrs[\"spatial_dims\"],\n                                repeat=i,\n                            )\n                        ]\n\n                        for field in _f[ti].attrs[\"field_names\"]:\n                            for dims in ti_field_dims:\n                                field_name = f\"{field}_{dims}\" if dims else field\n\n                                if _f[ti][field].attrs[\"time_varying\"]:\n                                    self.field_names[i].append(field_name)\n                                    if field not in seen:\n                                        seen.add(field)\n                                        self.core_field_names.append(field)\n                                else:\n                                    self.constant_field_names[i].append(field_name)\n                                    if field not in seen:\n                                        seen.add(field)\n                                        self.core_constant_field_names.append(field)\n\n        # Full trajectory mode overrides the above and just sets each sample to \"full\"\n        # trajectory where full = min(lowest_steps_per_file, max_rollout_steps)\n        if self.full_trajectory_mode:\n            self.n_steps_output = (\n                lowest_steps // self.min_dt_stride\n            ) - self.n_steps_input\n            assert self.n_steps_output &gt; 0, (\n                f\"Full trajectory mode not supported for dataset {names[0]} with {lowest_steps} minimum steps\"\n                f\" and a minimum stride of {self.min_dt_stride} and {self.n_steps_input} input steps\"\n            )\n            self.n_windows_per_trajectory = [1] * self.n_files\n            self.n_steps_per_trajectory = [lowest_steps] * self.n_files\n            self.file_index_offsets = np.cumsum([0] + self.n_trajectories_per_file)\n\n        # Just to make sure it doesn't put us in file -1\n        self.file_index_offsets[0] = -1\n        self.files: List[h5.File | None] = [\n            None for _ in self.files_paths\n        ]  # We open file references as they come\n        # Dataset length is last number of samples\n        self.len = self.file_index_offsets[-1]\n        self.n_spatial_dims = int(ndims.pop())  # Number of spatial dims\n        self.size_tuple = tuple(map(int, size_tuples.pop()))  # Size of spatial dims\n        self.dataset_name = names.pop()  # Name of dataset\n        # BCs\n        self.num_bcs = len(bcs)  # Number of boundary condition type included in data\n        self.bc_types = list(bcs)  # List of boundary condition types\n\n        return WellMetadata(\n            dataset_name=self.dataset_name,\n            n_spatial_dims=self.n_spatial_dims,\n            grid_type=grid_type,\n            spatial_resolution=self.size_tuple,\n            scalar_names=self.scalar_names,\n            constant_scalar_names=self.constant_scalar_names,\n            field_names=self.field_names,\n            constant_field_names=self.constant_field_names,\n            boundary_condition_types=self.bc_types,\n            n_files=self.n_files,\n            n_trajectories_per_file=self.n_trajectories_per_file,\n            n_steps_per_trajectory=self.n_steps_per_trajectory,\n        )\n\n    def _open_file(self, file_ind: int):\n        _file = h5.File(\n            self.fs.open(\n                self.files_paths[file_ind], \"rb\", **IO_PARAMS[\"fsspec_params\"]\n            ),\n            \"r\",\n            **IO_PARAMS[\"h5py_params\"],\n        )\n        self.files[file_ind] = _file\n\n    def _check_cache(self, cache: Dict[str, Any], name: str, data: Any):\n        if self.cache_small and data.numel() &lt; self.max_cache_size:\n            cache[name] = data\n\n    def _pad_axes(\n        self,\n        field_data: Any,\n        use_dims,\n        time_varying: bool = False,\n        tensor_order: int = 0,\n    ):\n        \"\"\"Repeats data over axes not used in storage\"\"\"\n        # Look at which dimensions currently are not used and tile based on their sizes\n        expand_dims = (1,) if time_varying else ()\n        expand_dims = expand_dims + tuple(\n            [\n                self.size_tuple[i] if not use_dim else 1\n                for i, use_dim in enumerate(use_dims)\n            ]\n        )\n        expand_dims = expand_dims + (1,) * tensor_order\n        return torch.tile(field_data, expand_dims)\n\n    def _reconstruct_fields(self, file, cache, sample_idx, time_idx, n_steps, dt):\n        \"\"\"Reconstruct space fields starting at index sample_idx, time_idx, with\n        n_steps and dt stride.\"\"\"\n        variable_fields = {0: {}, 1: {}, 2: {}}\n        constant_fields = {0: {}, 1: {}, 2: {}}\n        # Iterate through field types and apply appropriate transforms to stack them\n        for i, order_fields in enumerate([\"t0_fields\", \"t1_fields\", \"t2_fields\"]):\n            field_names = file[order_fields].attrs[\"field_names\"]\n            for field_name in field_names:\n                field = file[order_fields][field_name]\n                use_dims = field.attrs[\"dim_varying\"]\n                # If the field is in the cache, use it, otherwise go through read/pad\n                if field_name in cache:\n                    field_data = cache[field_name]\n                else:\n                    field_data = field\n                    # Index is built gradually since there can be different numbers of leading fields\n                    multi_index = ()\n                    if field.attrs[\"sample_varying\"]:\n                        multi_index = multi_index + (sample_idx,)\n                    if field.attrs[\"time_varying\"]:\n                        multi_index = multi_index + (\n                            slice(time_idx, time_idx + n_steps * dt, dt),\n                        )\n                    field_data = field_data[multi_index]\n                    field_data = torch.as_tensor(field_data)\n                    # Normalize\n                    if self.use_normalization and self.norm:\n                        field_data = self.norm.normalize(field_data, field_name)\n                    # If constant, try to cache\n                    if (\n                        not field.attrs[\"time_varying\"]\n                        and not field.attrs[\"sample_varying\"]\n                    ):\n                        self._check_cache(cache, field_name, field_data)\n\n                # Expand dims\n                field_data = self._pad_axes(\n                    field_data,\n                    use_dims,\n                    time_varying=field.attrs[\"time_varying\"],\n                    tensor_order=i,\n                )\n\n                if field.attrs[\"time_varying\"]:\n                    variable_fields[i][field_name] = field_data\n                else:\n                    constant_fields[i][field_name] = field_data\n\n        return (variable_fields, constant_fields)\n\n    def _reconstruct_scalars(self, file, cache, sample_idx, time_idx, n_steps, dt):\n        \"\"\"Reconstruct scalar values (not fields) starting at index sample_idx, time_idx, with\n        n_steps and dt stride.\"\"\"\n        variable_scalars = {}\n        constant_scalars = {}\n        for scalar_name in file[\"scalars\"].attrs[\"field_names\"]:\n            scalar = file[\"scalars\"][scalar_name]\n\n            if scalar_name in cache:\n                scalar_data = cache[scalar_name]\n            else:\n                scalar_data = scalar\n                # Build index gradually to account for different leading dims\n                multi_index = ()\n                if scalar.attrs[\"sample_varying\"]:\n                    multi_index = multi_index + (sample_idx,)\n                if scalar.attrs[\"time_varying\"]:\n                    multi_index = multi_index + (\n                        slice(time_idx, time_idx + n_steps * dt, dt),\n                    )\n                scalar_data = scalar_data[multi_index]\n                scalar_data = torch.as_tensor(scalar_data)\n                # If constant, try to cache\n                if (\n                    not scalar.attrs[\"time_varying\"]\n                    and not scalar.attrs[\"sample_varying\"]\n                ):\n                    self._check_cache(cache, scalar_name, scalar_data)\n\n            if scalar.attrs[\"time_varying\"]:\n                variable_scalars[scalar_name] = scalar_data\n            else:\n                constant_scalars[scalar_name] = scalar_data\n\n        return (variable_scalars, constant_scalars)\n\n    def _reconstruct_grids(self, file, cache, sample_idx, time_idx, n_steps, dt):\n        \"\"\"Reconstruct grid values starting at index sample_idx, time_idx, with\n        n_steps and dt stride.\"\"\"\n        # Time\n        if \"time_grid\" in cache:\n            time_grid = cache[\"time_grid\"]\n        elif file[\"dimensions\"][\"time\"].attrs[\"sample_varying\"]:\n            time_grid = torch.tensor(file[\"dimensions\"][\"time\"][sample_idx, :])\n        else:\n            time_grid = torch.tensor(file[\"dimensions\"][\"time\"][:])\n            self._check_cache(cache, \"time_grid\", time_grid)\n        # We have already sampled leading index if it existed so timegrid should be 1D\n        time_grid = time_grid[time_idx : time_idx + n_steps * dt : dt]\n        # Nothing should depend on absolute time - might change if we add weather\n        time_grid = time_grid - time_grid.min()\n\n        # Space - TODO - support time-varying grids or non-tensor product grids\n        if \"space_grid\" in cache:\n            space_grid = cache[\"space_grid\"]\n        else:\n            space_grid = []\n            sample_invariant = True\n            for dim in file[\"dimensions\"].attrs[\"spatial_dims\"]:\n                if file[\"dimensions\"][dim].attrs[\"sample_varying\"]:\n                    sample_invariant = False\n                    coords = torch.tensor(file[\"dimensions\"][dim][sample_idx])\n                else:\n                    coords = torch.tensor(file[\"dimensions\"][dim][:])\n                space_grid.append(coords)\n            space_grid = torch.stack(torch.meshgrid(*space_grid, indexing=\"ij\"), -1)\n            if sample_invariant:\n                self._check_cache(cache, \"space_grid\", space_grid)\n        return space_grid, time_grid\n\n    def _padding_bcs(self, file, cache, sample_idx, time_idx, n_steps, dt):\n        \"\"\"Handles BC case where BC corresponds to a specific padding type\n\n        Note/TODO - currently assumes boundaries to be axis-aligned and cover the entire\n        domain. This is a simplification that will need to be addressed in the future.\n        \"\"\"\n        if \"boundary_output\" in cache:\n            boundary_output = cache[\"boundary_output\"]\n        else:\n            bcs = file[\"boundary_conditions\"]\n            dim_indices = {\n                dim: i for i, dim in enumerate(file[\"dimensions\"].attrs[\"spatial_dims\"])\n            }\n            boundary_output = torch.ones(\n                self.n_spatial_dims, 2\n            )  # Open unless otherwise specified\n            for bc_name in bcs.keys():\n                bc = bcs[bc_name]\n                bc_type = bc.attrs[\"bc_type\"].upper()  # Enum is in upper case\n                if len(bc.attrs[\"associated_dims\"]) &gt; 1:\n                    warnings.warn(\n                        \"Only axis-aligned boundary fully supported. Boundary for axis counted as `open` or `periodic` if any part of it is and `wall` otherwise.\"\n                        \"If this does not fit your desired usecase, set `boundary_return_type=None`.\",\n                        RuntimeWarning,\n                    )\n                for dim in bc.attrs[\"associated_dims\"]:\n                    # Check all entries at the boundary - if any `open` or `periodic`, set that. However, for wall, the full boundary must be wall\n                    first_slice = tuple(\n                        slice(None) if dim != other_dim else 0\n                        for other_dim in bc.attrs[\"associated_dims\"]\n                    )\n                    last_slice = tuple(\n                        slice(None) if dim != other_dim else -1\n                        for other_dim in bc.attrs[\"associated_dims\"]\n                    )\n                    agg_op = np.min if bc_type == \"WALL\" else np.max\n                    mask = bc[\"mask\"][:]\n                    if agg_op(mask[first_slice]):\n                        boundary_output[dim_indices[dim]][0] = BoundaryCondition[\n                            bc_type\n                        ].value\n                    if agg_op(mask[last_slice]):\n                        boundary_output[dim_indices[dim]][1] = BoundaryCondition[\n                            bc_type\n                        ].value\n            self._check_cache(cache, \"boundary_output\", boundary_output)\n        return boundary_output\n\n    def _reconstruct_bcs(self, file, cache, sample_idx, time_idx, n_steps, dt):\n        \"\"\"Needs work to support arbitrary BCs.\n\n        Currently supports finite set of boundary condition types that describe\n        the geometry of the domain. Implements these as mask channels. The total\n        number of channels is determined by the number of BC types in the\n        data.\n\n        #TODO generalize boundary types\n        \"\"\"\n        if self.boundary_return_type == \"padding\":\n            return self._padding_bcs(file, cache, sample_idx, time_idx, n_steps, dt)\n        else:\n            raise NotImplementedError()\n\n    def _load_one_sample(self, index):\n        # Find specific file and local index\n        file_idx = int(\n            np.searchsorted(self.file_index_offsets, index, side=\"right\") - 1\n        )  # which file we are on\n        windows_per_trajectory = self.n_windows_per_trajectory[file_idx]\n        local_idx = index - max(\n            self.file_index_offsets[file_idx], 0\n        )  # First offset is -1\n        sample_idx = local_idx // windows_per_trajectory\n        time_idx = local_idx % windows_per_trajectory\n        # open hdf5 file (and cache the open object)\n        if self.files[file_idx] is None:\n            self._open_file(file_idx)\n\n        # If we gave a stride range, decide the largest size we can use given the sample location\n        dt = self.min_dt_stride\n        if self.max_dt_stride &gt; self.min_dt_stride:\n            effective_max_dt = maximum_stride_for_initial_index(\n                time_idx,\n                self.n_steps_per_trajectory[file_idx],\n                self.n_steps_input,\n                self.n_steps_output,\n            )\n            effective_max_dt = min(effective_max_dt, self.max_dt_stride)\n            if effective_max_dt &gt; self.min_dt_stride:\n                # Randint is non-inclusive on the upper bound\n                dt = np.random.randint(self.min_dt_stride, effective_max_dt + 1)\n        # Fetch the data\n        data = {}\n\n        output_steps = min(self.n_steps_output, self.max_rollout_steps)\n        data[\"variable_fields\"], data[\"constant_fields\"] = self._reconstruct_fields(\n            self.files[file_idx],\n            self.caches[file_idx],\n            sample_idx,\n            time_idx,\n            self.n_steps_input + output_steps,\n            dt,\n        )\n        data[\"variable_scalars\"], data[\"constant_scalars\"] = self._reconstruct_scalars(\n            self.files[file_idx],\n            self.caches[file_idx],\n            sample_idx,\n            time_idx,\n            self.n_steps_input + output_steps,\n            dt,\n        )\n\n        if self.boundary_return_type is not None:\n            data[\"boundary_conditions\"] = self._reconstruct_bcs(\n                self.files[file_idx],\n                self.caches[file_idx],\n                sample_idx,\n                time_idx,\n                self.n_steps_input + output_steps,\n                dt,\n            )\n\n        if self.return_grid:\n            data[\"space_grid\"], data[\"time_grid\"] = self._reconstruct_grids(\n                self.files[file_idx],\n                self.caches[file_idx],\n                sample_idx,\n                time_idx,\n                self.n_steps_input + output_steps,\n                dt,\n            )\n        return data, file_idx, sample_idx, time_idx, dt\n\n    def _preprocess_data(\n        self, data: TrajectoryData, traj_metadata: TrajectoryMetadata\n    ) -&gt; TrajectoryData:\n        \"\"\"Preprocess the data before applying transformations. Identity in Well\"\"\"\n        return data\n\n    def _postprocess_data(\n        self, data: TrajectoryData, traj_metadata: TrajectoryMetadata\n    ) -&gt; TrajectoryData:\n        \"\"\"Postprocess the data after applying transformations. Flattens fields and scalars into single channel dim.\"\"\"\n        # Start with field data\n        for key in (\"variable_fields\", \"constant_fields\"):\n            # Flatten all tensor fields\n            data[key] = [\n                field.unsqueeze(-1).flatten(-order - 1)\n                for order, fields in data[key].items()\n                for _, field in fields.items()\n            ]\n            # Then concatenate them along new single channel\n            if data[key]:\n                data[key] = torch.concatenate(data[key], dim=-1)\n            else:\n                data[key] = torch.tensor([])\n        # Then do the same for scalars but no flattening since no tensor-order\n        for key in (\"variable_scalars\", \"constant_scalars\"):\n            data[key] = [scalar.unsqueeze(-1) for _, scalar in data[key].items()]\n            if data[key]:\n                data[key] = torch.concatenate(data[key], dim=-1)\n            else:\n                data[key] = torch.tensor([])\n\n        return data\n\n    def _construct_sample(\n        self, data: TrajectoryData, traj_metadata: TrajectoryMetadata\n    ) -&gt; Dict[str, torch.Tensor]:\n        # Input/Output split\n        sample = {\n            \"input_fields\": data[\"variable_fields\"][\n                : self.n_steps_input\n            ],  # Ti x H x W x C\n            \"output_fields\": data[\"variable_fields\"][\n                self.n_steps_input :\n            ],  # To x H x W x C\n            \"constant_fields\": data[\"constant_fields\"],  # H x W x C\n            \"input_scalars\": data[\"variable_scalars\"][: self.n_steps_input],  # Ti x C\n            \"output_scalars\": data[\"variable_scalars\"][self.n_steps_input :],  # To x C\n            \"constant_scalars\": data[\"constant_scalars\"],  # C\n        }\n\n        if self.boundary_return_type is not None:\n            sample[\"boundary_conditions\"] = data[\"boundary_conditions\"]  # N x 2\n\n        if self.return_grid:\n            sample[\"space_grid\"] = data[\"space_grid\"]  # H x W x D\n            sample[\"input_time_grid\"] = data[\"time_grid\"][: self.n_steps_input]  # Ti\n            sample[\"output_time_grid\"] = data[\"time_grid\"][self.n_steps_input :]  # To\n\n        return {k: v for k, v in sample.items() if v.numel() &gt; 0}\n\n    def __getitem__(self, index):\n        data, file_idx, sample_idx, time_idx, dt = self._load_one_sample(index)\n        # Break out into sub-processes to make inheritance easier\n        data = cast(TrajectoryData, data)\n        traj_metadata = TrajectoryMetadata(\n            dataset=self,\n            file_idx=file_idx,\n            sample_idx=sample_idx,\n            time_idx=time_idx,\n            time_stride=dt,\n        )\n        # Apply any type of pre-processing that needs to be applied before augmentation\n        data = self._preprocess_data(data, traj_metadata)\n        # Apply augmentations and other transformations\n        if self.transform is not None:\n            data = self.transform(data, traj_metadata)\n        # Convert ingestable format - in this class this flattens the fields\n        data = self._postprocess_data(data, traj_metadata)\n        # Break apart into x, y\n        sample = self._construct_sample(data, traj_metadata)\n        # Return only non-empty keys - maybe change this later\n        return sample\n\n    def __len__(self):\n        return self.len\n\n    def to_xarray(self, backend: Literal[\"numpy\", \"dask\"] = \"dask\"):\n        \"\"\"Export the dataset to an Xarray Dataset by stacking all HDF5 files as Xarray datasets\n        along the existing 'sample' dimension.\n\n        Args:\n            backend: 'numpy' for eager loading, 'dask' for lazy loading.\n\n        Returns:\n            xarray.Dataset:\n                The stacked Xarray Dataset.\n\n        Examples:\n            To convert a dataset and plot the pressure for 5 different times for a single trajectory:\n            &gt;&gt;&gt; ds = dataset.to_xarray()\n            &gt;&gt;&gt; ds.pressure.isel(sample=0, time=[0, 10, 20, 30, 40]).plot(col='time', col_wrap=5)\n        \"\"\"\n\n        import xarray as xr\n\n        datasets = []\n        total_samples = 0\n        for file_idx in range(len(self.files_paths)):\n            if self.files[file_idx] is None:\n                self._open_file(file_idx)\n            ds = hdf5_to_xarray(self.files[file_idx], backend=backend)\n            # Ensure 'sample' dimension is always present\n            if \"sample\" not in ds.sizes:\n                ds = ds.expand_dims(\"sample\")\n            # Adjust the 'sample' coordinate\n            if \"sample\" in ds.coords:\n                n_samples = ds.sizes[\"sample\"]\n                ds = ds.assign_coords(sample=ds.coords[\"sample\"] + total_samples)\n                total_samples += n_samples\n            datasets.append(ds)\n\n        combined_ds = xr.concat(datasets, dim=\"sample\")\n        return combined_ds\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}: {self.data_path}&gt;\"\n</code></pre>"},{"location":"api/#the_well.data.WellDataset.to_xarray","title":"<code>to_xarray(backend='dask')</code>","text":"<p>Export the dataset to an Xarray Dataset by stacking all HDF5 files as Xarray datasets along the existing 'sample' dimension.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Literal['numpy', 'dask']</code> <p>'numpy' for eager loading, 'dask' for lazy loading.</p> <code>'dask'</code> <p>Returns:</p> Type Description <p>xarray.Dataset: The stacked Xarray Dataset.</p> <p>Examples:</p> <p>To convert a dataset and plot the pressure for 5 different times for a single trajectory:</p> <pre><code>&gt;&gt;&gt; ds = dataset.to_xarray()\n&gt;&gt;&gt; ds.pressure.isel(sample=0, time=[0, 10, 20, 30, 40]).plot(col='time', col_wrap=5)\n</code></pre> Source code in <code>the_well/data/datasets.py</code> <pre><code>def to_xarray(self, backend: Literal[\"numpy\", \"dask\"] = \"dask\"):\n    \"\"\"Export the dataset to an Xarray Dataset by stacking all HDF5 files as Xarray datasets\n    along the existing 'sample' dimension.\n\n    Args:\n        backend: 'numpy' for eager loading, 'dask' for lazy loading.\n\n    Returns:\n        xarray.Dataset:\n            The stacked Xarray Dataset.\n\n    Examples:\n        To convert a dataset and plot the pressure for 5 different times for a single trajectory:\n        &gt;&gt;&gt; ds = dataset.to_xarray()\n        &gt;&gt;&gt; ds.pressure.isel(sample=0, time=[0, 10, 20, 30, 40]).plot(col='time', col_wrap=5)\n    \"\"\"\n\n    import xarray as xr\n\n    datasets = []\n    total_samples = 0\n    for file_idx in range(len(self.files_paths)):\n        if self.files[file_idx] is None:\n            self._open_file(file_idx)\n        ds = hdf5_to_xarray(self.files[file_idx], backend=backend)\n        # Ensure 'sample' dimension is always present\n        if \"sample\" not in ds.sizes:\n            ds = ds.expand_dims(\"sample\")\n        # Adjust the 'sample' coordinate\n        if \"sample\" in ds.coords:\n            n_samples = ds.sizes[\"sample\"]\n            ds = ds.assign_coords(sample=ds.coords[\"sample\"] + total_samples)\n            total_samples += n_samples\n        datasets.append(ds)\n\n    combined_ds = xr.concat(datasets, dim=\"sample\")\n    return combined_ds\n</code></pre>"},{"location":"api/#datamodule","title":"DataModule","text":"<p>The <code>WellDataModule</code> provides the different dataloaders required for training, validation, and testing. It has two kinds of dataloaders: the default one that yields batches of a fixed time horizon, and rollout ones that yields batches to evaluate rollout performances.</p>"},{"location":"api/#the_well.data.WellDataModule","title":"<code>the_well.data.WellDataModule</code>","text":"<p>               Bases: <code>AbstractDataModule</code></p> <p>Data module class to yield batches of samples.</p> <p>Parameters:</p> Name Type Description Default <code>well_base_path</code> <code>str</code> <p>Path to the data folder containing the splits (train, validation, and test).</p> required <code>well_dataset_name</code> <code>str</code> <p>Name of the well dataset to use.</p> required <code>batch_size</code> <code>int</code> <p>Size of the batches yielded by the dataloaders</p> required <code>include_filters</code> <code>List[str]</code> <p>Only file names containing any of these strings will be included.</p> <code>[]</code> <code>exclude_filters</code> <code>List[str]</code> <p>File names containing any of these strings will be excluded.</p> <code>[]</code> <code>use_normalization</code> <code>bool</code> <p>Whether to use normalization on the data.</p> <code>False</code> <code>normalization_type</code> <code>Optional[Callable[..., Any]]</code> <p>What kind of normalization to use if use_normalization is True. Currently supports zscore and rms.</p> <code>None</code> <code>train_dataset</code> <code>Callable[..., Any]</code> <p>What type of training dataset type. WellDataset or DeltaWellDataset options.</p> <code>WellDataset</code> <code>max_rollout_steps</code> <code>int</code> <p>Maximum number of steps to use for the rollout dataset. Mostly for memory reasons.</p> <code>100</code> <code>n_steps_input</code> <code>int</code> <p>Number of steps to use as input.</p> <code>1</code> <code>n_steps_output</code> <code>int</code> <p>Number of steps to use as output.</p> <code>1</code> <code>min_dt_stride</code> <code>int</code> <p>Minimum stride in time to use for the dataset.</p> <code>1</code> <code>max_dt_stride</code> <code>int</code> <p>Maximum stride in time to use for the dataset. If this is greater than min, randomly choose between them.     Note that this is unused for validation/test which uses \"min_dt_stride\" for both the min and max.</p> <code>1</code> <code>world_size</code> <code>int</code> <p>Number of GPUs in use for distributed training.</p> <code>1</code> <code>data_workers</code> <code>int</code> <p>Number of workers to use for data loading.</p> <code>4</code> <code>rank</code> <code>int</code> <p>Rank of the current process in distributed training.</p> <code>1</code> <code>transform</code> <code>Optional[Augmentation]</code> <p>Augmentation to apply to the data. If None, no augmentation is applied.</p> <code>None</code> <code>dataset_kws</code> <code>Optional[Dict[Literal['train', 'val', 'rollout_val', 'test', 'rollout_test'], Dict[str, Any]]]</code> <p>Additional keyword arguments to pass to each dataset, as a dict of dicts.</p> <code>None</code> <code>storage_kwargs</code> <code>Optional[Dict]</code> <p>Storage options passed to fsspec for accessing the raw data.</p> <code>None</code> Source code in <code>the_well/data/datamodule.py</code> <pre><code>class WellDataModule(AbstractDataModule):\n    \"\"\"Data module class to yield batches of samples.\n\n    Args:\n        well_base_path:\n            Path to the data folder containing the splits (train, validation, and test).\n        well_dataset_name:\n            Name of the well dataset to use.\n        batch_size:\n            Size of the batches yielded by the dataloaders\n        ---\n        include_filters:\n            Only file names containing any of these strings will be included.\n        exclude_filters:\n            File names containing any of these strings will be excluded.\n        use_normalization:\n            Whether to use normalization on the data.\n        normalization_type:\n            What kind of normalization to use if use_normalization is True. Currently supports zscore and rms.\n        train_dataset:\n            What type of training dataset type. WellDataset or DeltaWellDataset options.\n        max_rollout_steps:\n            Maximum number of steps to use for the rollout dataset. Mostly for memory reasons.\n        n_steps_input:\n            Number of steps to use as input.\n        n_steps_output:\n            Number of steps to use as output.\n        min_dt_stride:\n            Minimum stride in time to use for the dataset.\n        max_dt_stride:\n            Maximum stride in time to use for the dataset. If this is greater than min, randomly choose between them.\n                Note that this is unused for validation/test which uses \"min_dt_stride\" for both the min and max.\n        world_size:\n            Number of GPUs in use for distributed training.\n        data_workers:\n            Number of workers to use for data loading.\n        rank:\n            Rank of the current process in distributed training.\n        transform:\n            Augmentation to apply to the data. If None, no augmentation is applied.\n        dataset_kws:\n            Additional keyword arguments to pass to each dataset, as a dict of dicts.\n        storage_kwargs:\n            Storage options passed to fsspec for accessing the raw data.\n    \"\"\"\n\n    def __init__(\n        self,\n        well_base_path: str,\n        well_dataset_name: str,\n        batch_size: int,\n        include_filters: List[str] = [],\n        exclude_filters: List[str] = [],\n        use_normalization: bool = False,\n        normalization_type: Optional[Callable[..., Any]] = None,\n        train_dataset: Callable[..., Any] = WellDataset,\n        max_rollout_steps: int = 100,\n        n_steps_input: int = 1,\n        n_steps_output: int = 1,\n        min_dt_stride: int = 1,\n        max_dt_stride: int = 1,\n        world_size: int = 1,\n        data_workers: int = 4,\n        rank: int = 1,\n        boundary_return_type: Literal[\"padding\", None] = \"padding\",\n        transform: Optional[Augmentation] = None,\n        dataset_kws: Optional[\n            Dict[\n                Literal[\"train\", \"val\", \"rollout_val\", \"test\", \"rollout_test\"],\n                Dict[str, Any],\n            ]\n        ] = None,\n        storage_kwargs: Optional[Dict] = None,\n    ):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"always\")  # Ensure warnings are always displayed\n\n            if use_normalization:\n                warnings.warn(\n                    \"`use_normalization` parameter will be removed in a future version. \"\n                    \"For proper normalizing, set both use_normalization=True and normalization_type to either ZScoreNormalization or RMSNormalization.\"\n                    \"Default behavior is `normalization_type=ZScoreNormalization` and `use_normalization=True`.\"\n                    \"To switch off normalization instead, please set use_normalization=False in the config.yaml file\",\n                    DeprecationWarning,\n                )\n                if normalization_type is None:\n                    warnings.warn(\n                        \"use_normalization=True, but normalization_type is None. \"\n                        \"Defaulting to ZScoreNormalization.\",\n                        UserWarning,\n                    )\n                    normalization_type = ZScoreNormalization  # Default fallback\n\n            elif normalization_type is not None:\n                warnings.warn(\n                    \"Inconsistent normalization settings: `use_normalization=False`, but `normalization_type` is set. \"\n                    \"Defaulting `normalization_type=None` and `use_normalization=False`.\",\n                    UserWarning,\n                )\n                normalization_type = None\n\n        # DeltaWellDataset only for training for delta case, WellDataset for everything else\n        self.train_dataset = train_dataset(\n            well_base_path=well_base_path,\n            well_dataset_name=well_dataset_name,\n            well_split_name=\"train\",\n            include_filters=include_filters,\n            exclude_filters=exclude_filters,\n            use_normalization=use_normalization,\n            normalization_type=normalization_type,\n            n_steps_input=n_steps_input,\n            n_steps_output=n_steps_output,\n            storage_options=storage_kwargs,\n            min_dt_stride=min_dt_stride,\n            max_dt_stride=max_dt_stride,\n            boundary_return_type=boundary_return_type,\n            transform=transform,\n            **(\n                dataset_kws[\"train\"]\n                if dataset_kws is not None and \"train\" in dataset_kws\n                else {}\n            ),\n        )\n        self.val_dataset = WellDataset(\n            well_base_path=well_base_path,\n            well_dataset_name=well_dataset_name,\n            well_split_name=\"valid\",\n            include_filters=include_filters,\n            exclude_filters=exclude_filters,\n            n_steps_input=n_steps_input,\n            n_steps_output=n_steps_output,\n            storage_options=storage_kwargs,\n            min_dt_stride=min_dt_stride,\n            max_dt_stride=min_dt_stride,\n            boundary_return_type=boundary_return_type,\n            **(\n                dataset_kws[\"val\"]\n                if dataset_kws is not None and \"val\" in dataset_kws\n                else {}\n            ),\n        )\n        self.rollout_val_dataset = WellDataset(\n            well_base_path=well_base_path,\n            well_dataset_name=well_dataset_name,\n            well_split_name=\"valid\",\n            include_filters=include_filters,\n            exclude_filters=exclude_filters,\n            max_rollout_steps=max_rollout_steps,\n            n_steps_input=n_steps_input,\n            n_steps_output=n_steps_output,\n            full_trajectory_mode=True,\n            storage_options=storage_kwargs,\n            min_dt_stride=min_dt_stride,\n            max_dt_stride=min_dt_stride,\n            boundary_return_type=boundary_return_type,\n            **(\n                dataset_kws[\"rollout_val\"]\n                if dataset_kws is not None and \"rollout_val\" in dataset_kws\n                else {}\n            ),\n        )\n        self.test_dataset = WellDataset(\n            well_base_path=well_base_path,\n            well_dataset_name=well_dataset_name,\n            well_split_name=\"test\",\n            include_filters=include_filters,\n            exclude_filters=exclude_filters,\n            n_steps_input=n_steps_input,\n            n_steps_output=n_steps_output,\n            storage_options=storage_kwargs,\n            min_dt_stride=min_dt_stride,\n            max_dt_stride=min_dt_stride,\n            boundary_return_type=boundary_return_type,\n            **(\n                dataset_kws[\"test\"]\n                if dataset_kws is not None and \"test\" in dataset_kws\n                else {}\n            ),\n        )\n        self.rollout_test_dataset = WellDataset(\n            well_base_path=well_base_path,\n            well_dataset_name=well_dataset_name,\n            well_split_name=\"test\",\n            include_filters=include_filters,\n            exclude_filters=exclude_filters,\n            max_rollout_steps=max_rollout_steps,\n            n_steps_input=n_steps_input,\n            n_steps_output=n_steps_output,\n            full_trajectory_mode=True,\n            storage_options=storage_kwargs,\n            min_dt_stride=min_dt_stride,\n            max_dt_stride=min_dt_stride,\n            boundary_return_type=boundary_return_type,\n            **(\n                dataset_kws[\"rollout_test\"]\n                if dataset_kws is not None and \"rollout_test\" in dataset_kws\n                else {}\n            ),\n        )\n        self.well_base_path = well_base_path\n        self.well_dataset_name = well_dataset_name\n        self.batch_size = batch_size\n        self.world_size = world_size\n        self.data_workers = data_workers\n        self.rank = rank\n\n    @property\n    def is_distributed(self) -&gt; bool:\n        return self.world_size &gt; 1\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"Generate a dataloader for training data.\n\n        Returns:\n            A dataloader\n        \"\"\"\n        sampler = None\n        if self.is_distributed:\n            sampler = DistributedSampler(\n                self.train_dataset,\n                num_replicas=self.world_size,\n                rank=self.rank,\n                shuffle=True,\n            )\n            logger.debug(\n                f\"Use {sampler.__class__.__name__} \"\n                f\"({self.rank}/{self.world_size}) for training data\"\n            )\n        shuffle = sampler is None\n\n        return DataLoader(\n            self.train_dataset,\n            num_workers=self.data_workers,\n            pin_memory=True,\n            batch_size=self.batch_size,\n            shuffle=shuffle,\n            drop_last=True,\n            sampler=sampler,\n        )\n\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Generate a dataloader for validation data.\n\n        Returns:\n            A dataloader\n        \"\"\"\n        sampler = None\n        if self.is_distributed:\n            sampler = DistributedSampler(\n                self.val_dataset,\n                num_replicas=self.world_size,\n                rank=self.rank,\n                shuffle=True,\n            )\n            logger.debug(\n                f\"Use {sampler.__class__.__name__} \"\n                f\"({self.rank}/{self.world_size}) for validation data\"\n            )\n        shuffle = sampler is None  # Most valid epochs are short\n        return DataLoader(\n            self.val_dataset,\n            num_workers=self.data_workers,\n            pin_memory=True,\n            batch_size=self.batch_size,\n            shuffle=shuffle,\n            drop_last=True,\n            sampler=sampler,\n        )\n\n    def rollout_val_dataloader(self) -&gt; DataLoader:\n        \"\"\"Generate a dataloader for rollout validation data.\n\n        Returns:\n            A dataloader\n        \"\"\"\n        sampler = None\n        if self.is_distributed:\n            sampler = DistributedSampler(\n                self.rollout_val_dataset,\n                num_replicas=self.world_size,\n                rank=self.rank,\n                shuffle=True,  # Since we're subsampling, don't want continuous\n            )\n            logger.debug(\n                f\"Use {sampler.__class__.__name__} \"\n                f\"({self.rank}/{self.world_size}) for rollout validation data\"\n            )\n        shuffle = sampler is None  # Most valid epochs are short\n        return DataLoader(\n            self.rollout_val_dataset,\n            num_workers=self.data_workers,\n            pin_memory=True,\n            batch_size=1,\n            shuffle=shuffle,  # Shuffling because most batches we take a small subsample\n            drop_last=True,\n            sampler=sampler,\n        )\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Generate a dataloader for test data.\n\n        Returns:\n            A dataloader\n        \"\"\"\n        sampler = None\n        if self.is_distributed:\n            sampler = DistributedSampler(\n                self.test_dataset,\n                num_replicas=self.world_size,\n                rank=self.rank,\n                shuffle=False,\n            )\n            logger.debug(\n                f\"Use {sampler.__class__.__name__} \"\n                f\"({self.rank}/{self.world_size}) for test data\"\n            )\n        return DataLoader(\n            self.test_dataset,\n            num_workers=self.data_workers,\n            pin_memory=True,\n            batch_size=self.batch_size,\n            shuffle=False,\n            drop_last=True,\n            sampler=sampler,\n        )\n\n    def rollout_test_dataloader(self) -&gt; DataLoader:\n        \"\"\"Generate a dataloader for rollout test data.\n\n        Returns:\n            A dataloader\n        \"\"\"\n        sampler = None\n        if self.is_distributed:\n            sampler = DistributedSampler(\n                self.rollout_test_dataset,\n                num_replicas=self.world_size,\n                rank=self.rank,\n                shuffle=False,\n            )\n            logger.debug(\n                f\"Use {sampler.__class__.__name__} \"\n                f\"({self.rank}/{self.world_size}) for rollout test data\"\n            )\n        return DataLoader(\n            self.rollout_test_dataset,\n            num_workers=self.data_workers,\n            pin_memory=True,\n            batch_size=1,  # min(self.batch_size, len(self.rollout_test_dataset)),\n            shuffle=False,\n            drop_last=True,\n            sampler=sampler,\n        )\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.__class__.__name__}: {self.well_dataset_name} on {self.well_base_path}&gt;\"\n</code></pre>"},{"location":"api/#the_well.data.WellDataModule.rollout_test_dataloader","title":"<code>rollout_test_dataloader()</code>","text":"<p>Generate a dataloader for rollout test data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A dataloader</p> Source code in <code>the_well/data/datamodule.py</code> <pre><code>def rollout_test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Generate a dataloader for rollout test data.\n\n    Returns:\n        A dataloader\n    \"\"\"\n    sampler = None\n    if self.is_distributed:\n        sampler = DistributedSampler(\n            self.rollout_test_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=False,\n        )\n        logger.debug(\n            f\"Use {sampler.__class__.__name__} \"\n            f\"({self.rank}/{self.world_size}) for rollout test data\"\n        )\n    return DataLoader(\n        self.rollout_test_dataset,\n        num_workers=self.data_workers,\n        pin_memory=True,\n        batch_size=1,  # min(self.batch_size, len(self.rollout_test_dataset)),\n        shuffle=False,\n        drop_last=True,\n        sampler=sampler,\n    )\n</code></pre>"},{"location":"api/#the_well.data.WellDataModule.rollout_val_dataloader","title":"<code>rollout_val_dataloader()</code>","text":"<p>Generate a dataloader for rollout validation data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A dataloader</p> Source code in <code>the_well/data/datamodule.py</code> <pre><code>def rollout_val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Generate a dataloader for rollout validation data.\n\n    Returns:\n        A dataloader\n    \"\"\"\n    sampler = None\n    if self.is_distributed:\n        sampler = DistributedSampler(\n            self.rollout_val_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=True,  # Since we're subsampling, don't want continuous\n        )\n        logger.debug(\n            f\"Use {sampler.__class__.__name__} \"\n            f\"({self.rank}/{self.world_size}) for rollout validation data\"\n        )\n    shuffle = sampler is None  # Most valid epochs are short\n    return DataLoader(\n        self.rollout_val_dataset,\n        num_workers=self.data_workers,\n        pin_memory=True,\n        batch_size=1,\n        shuffle=shuffle,  # Shuffling because most batches we take a small subsample\n        drop_last=True,\n        sampler=sampler,\n    )\n</code></pre>"},{"location":"api/#the_well.data.WellDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Generate a dataloader for test data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A dataloader</p> Source code in <code>the_well/data/datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Generate a dataloader for test data.\n\n    Returns:\n        A dataloader\n    \"\"\"\n    sampler = None\n    if self.is_distributed:\n        sampler = DistributedSampler(\n            self.test_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=False,\n        )\n        logger.debug(\n            f\"Use {sampler.__class__.__name__} \"\n            f\"({self.rank}/{self.world_size}) for test data\"\n        )\n    return DataLoader(\n        self.test_dataset,\n        num_workers=self.data_workers,\n        pin_memory=True,\n        batch_size=self.batch_size,\n        shuffle=False,\n        drop_last=True,\n        sampler=sampler,\n    )\n</code></pre>"},{"location":"api/#the_well.data.WellDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Generate a dataloader for training data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A dataloader</p> Source code in <code>the_well/data/datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Generate a dataloader for training data.\n\n    Returns:\n        A dataloader\n    \"\"\"\n    sampler = None\n    if self.is_distributed:\n        sampler = DistributedSampler(\n            self.train_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=True,\n        )\n        logger.debug(\n            f\"Use {sampler.__class__.__name__} \"\n            f\"({self.rank}/{self.world_size}) for training data\"\n        )\n    shuffle = sampler is None\n\n    return DataLoader(\n        self.train_dataset,\n        num_workers=self.data_workers,\n        pin_memory=True,\n        batch_size=self.batch_size,\n        shuffle=shuffle,\n        drop_last=True,\n        sampler=sampler,\n    )\n</code></pre>"},{"location":"api/#the_well.data.WellDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Generate a dataloader for validation data.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>A dataloader</p> Source code in <code>the_well/data/datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Generate a dataloader for validation data.\n\n    Returns:\n        A dataloader\n    \"\"\"\n    sampler = None\n    if self.is_distributed:\n        sampler = DistributedSampler(\n            self.val_dataset,\n            num_replicas=self.world_size,\n            rank=self.rank,\n            shuffle=True,\n        )\n        logger.debug(\n            f\"Use {sampler.__class__.__name__} \"\n            f\"({self.rank}/{self.world_size}) for validation data\"\n        )\n    shuffle = sampler is None  # Most valid epochs are short\n    return DataLoader(\n        self.val_dataset,\n        num_workers=self.data_workers,\n        pin_memory=True,\n        batch_size=self.batch_size,\n        shuffle=shuffle,\n        drop_last=True,\n        sampler=sampler,\n    )\n</code></pre>"},{"location":"api/#metrics","title":"Metrics","text":"<p>The Well package implements a series of metrics to assess the performances of a trained model.</p>"},{"location":"api/#the_well.benchmark.metrics","title":"<code>the_well.benchmark.metrics</code>","text":""},{"location":"api/#the_well.benchmark.metrics.LInfinity","title":"<code>LInfinity</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class LInfinity(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        L-Infinity Norm\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n\n        Returns:\n            L-Infinity norm between x and y.\n        \"\"\"\n        spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n        return torch.max(\n            torch.abs(x - y).flatten(start_dim=spatial_dims[0], end_dim=-2), dim=-2\n        ).values\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.LInfinity.eval","title":"<code>eval(x, y, meta)</code>  <code>staticmethod</code>","text":"<p>L-Infinity Norm</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>L-Infinity norm between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    L-Infinity Norm\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n\n    Returns:\n        L-Infinity norm between x and y.\n    \"\"\"\n    spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n    return torch.max(\n        torch.abs(x - y).flatten(start_dim=spatial_dims[0], end_dim=-2), dim=-2\n    ).values\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.MAE","title":"<code>MAE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class MAE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Mean Absolute Error\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n\n        Returns:\n            Mean absolute error between x and y.\n        \"\"\"\n        n_spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n        return torch.mean((x - y).abs(), dim=n_spatial_dims)\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.MAE.eval","title":"<code>eval(x, y, meta)</code>  <code>staticmethod</code>","text":"<p>Mean Absolute Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Mean absolute error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Mean Absolute Error\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n\n    Returns:\n        Mean absolute error between x and y.\n    \"\"\"\n    n_spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n    return torch.mean((x - y).abs(), dim=n_spatial_dims)\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.MSE","title":"<code>MSE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class MSE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Mean Squared Error\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n\n        Returns:\n            Mean squared error between x and y.\n        \"\"\"\n        n_spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n        return torch.mean((x - y) ** 2, dim=n_spatial_dims)\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.MSE.eval","title":"<code>eval(x, y, meta)</code>  <code>staticmethod</code>","text":"<p>Mean Squared Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Mean Squared Error\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n\n    Returns:\n        Mean squared error between x and y.\n    \"\"\"\n    n_spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n    return torch.mean((x - y) ** 2, dim=n_spatial_dims)\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.NMSE","title":"<code>NMSE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class NMSE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n        eps: float = 1e-7,\n        norm_mode: str = \"norm\",\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Normalized Mean Squared Error\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n            eps: Small value to avoid division by zero. Default is 1e-7.\n            norm_mode:\n                Mode for computing the normalization factor. Can be 'norm' or 'std'. Default is 'norm'.\n\n        Returns:\n            Normalized mean squared error between x and y.\n        \"\"\"\n        n_spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n        if norm_mode == \"norm\":\n            norm = torch.mean(y**2, dim=n_spatial_dims)\n        elif norm_mode == \"std\":\n            norm = torch.std(y, dim=n_spatial_dims) ** 2\n        else:\n            raise ValueError(f\"Invalid norm_mode: {norm_mode}\")\n        return MSE.eval(x, y, meta) / (norm + eps)\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.NMSE.eval","title":"<code>eval(x, y, meta, eps=1e-07, norm_mode='norm')</code>  <code>staticmethod</code>","text":"<p>Normalized Mean Squared Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero. Default is 1e-7.</p> <code>1e-07</code> <code>norm_mode</code> <code>str</code> <p>Mode for computing the normalization factor. Can be 'norm' or 'std'. Default is 'norm'.</p> <code>'norm'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n    eps: float = 1e-7,\n    norm_mode: str = \"norm\",\n) -&gt; torch.Tensor:\n    \"\"\"\n    Normalized Mean Squared Error\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n        eps: Small value to avoid division by zero. Default is 1e-7.\n        norm_mode:\n            Mode for computing the normalization factor. Can be 'norm' or 'std'. Default is 'norm'.\n\n    Returns:\n        Normalized mean squared error between x and y.\n    \"\"\"\n    n_spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n    if norm_mode == \"norm\":\n        norm = torch.mean(y**2, dim=n_spatial_dims)\n    elif norm_mode == \"std\":\n        norm = torch.std(y, dim=n_spatial_dims) ** 2\n    else:\n        raise ValueError(f\"Invalid norm_mode: {norm_mode}\")\n    return MSE.eval(x, y, meta) / (norm + eps)\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.NRMSE","title":"<code>NRMSE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class NRMSE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n        eps: float = 1e-7,\n        norm_mode: str = \"norm\",\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Normalized Root Mean Squared Error\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n            eps: Small value to avoid division by zero. Default is 1e-7.\n            norm_mode : Mode for computing the normalization factor. Can be 'norm' or 'std'. Default is 'norm'.\n\n        Returns:\n            Normalized root mean squared error between x and y.\n\n        \"\"\"\n        return torch.sqrt(NMSE.eval(x, y, meta, eps=eps, norm_mode=norm_mode))\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.NRMSE.eval","title":"<code>eval(x, y, meta, eps=1e-07, norm_mode='norm')</code>  <code>staticmethod</code>","text":"<p>Normalized Root Mean Squared Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid division by zero. Default is 1e-7.</p> <code>1e-07</code> <code>norm_mode</code> <p>Mode for computing the normalization factor. Can be 'norm' or 'std'. Default is 'norm'.</p> <code>'norm'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized root mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n    eps: float = 1e-7,\n    norm_mode: str = \"norm\",\n) -&gt; torch.Tensor:\n    \"\"\"\n    Normalized Root Mean Squared Error\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n        eps: Small value to avoid division by zero. Default is 1e-7.\n        norm_mode : Mode for computing the normalization factor. Can be 'norm' or 'std'. Default is 'norm'.\n\n    Returns:\n        Normalized root mean squared error between x and y.\n\n    \"\"\"\n    return torch.sqrt(NMSE.eval(x, y, meta, eps=eps, norm_mode=norm_mode))\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.PearsonR","title":"<code>PearsonR</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class PearsonR(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n        eps: float = 1e-7,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Pearson Correlation Coefficient\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n\n        Returns:\n            Pearson correlation coefficient between x and y.\n        \"\"\"\n        x_flat = torch.flatten(x, start_dim=-meta.n_spatial_dims - 1, end_dim=-2)\n        y_flat = torch.flatten(y, start_dim=-meta.n_spatial_dims - 1, end_dim=-2)\n\n        # Calculate means along flattened axis\n        x_mean = torch.mean(x_flat, dim=-2, keepdim=True)\n        y_mean = torch.mean(y_flat, dim=-2, keepdim=True)\n\n        # Calculate covariance\n        covariance = torch.mean((x_flat - x_mean) * (y_flat - y_mean), dim=-2)\n        # Calculate standard deviations\n        std_x = torch.std(x_flat, dim=-2)\n        std_y = torch.std(y_flat, dim=-2)\n\n        # Calculate Pearson correlation coefficient\n        correlation = covariance / (std_x * std_y + eps)\n        return correlation\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.PearsonR.eval","title":"<code>eval(x, y, meta, eps=1e-07)</code>  <code>staticmethod</code>","text":"<p>Pearson Correlation Coefficient</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Pearson correlation coefficient between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n    eps: float = 1e-7,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Pearson Correlation Coefficient\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n\n    Returns:\n        Pearson correlation coefficient between x and y.\n    \"\"\"\n    x_flat = torch.flatten(x, start_dim=-meta.n_spatial_dims - 1, end_dim=-2)\n    y_flat = torch.flatten(y, start_dim=-meta.n_spatial_dims - 1, end_dim=-2)\n\n    # Calculate means along flattened axis\n    x_mean = torch.mean(x_flat, dim=-2, keepdim=True)\n    y_mean = torch.mean(y_flat, dim=-2, keepdim=True)\n\n    # Calculate covariance\n    covariance = torch.mean((x_flat - x_mean) * (y_flat - y_mean), dim=-2)\n    # Calculate standard deviations\n    std_x = torch.std(x_flat, dim=-2)\n    std_y = torch.std(y_flat, dim=-2)\n\n    # Calculate Pearson correlation coefficient\n    correlation = covariance / (std_x * std_y + eps)\n    return correlation\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.RMSE","title":"<code>RMSE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class RMSE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Root Mean Squared Error\n\n        Args:\n            x: torch.Tensor | np.ndarray\n                Input tensor.\n            y: torch.Tensor | np.ndarray\n                Target tensor.\n            meta: WellMetadata\n                Metadata for the dataset.\n\n        Returns:\n            Root mean squared error between x and y.\n        \"\"\"\n        return torch.sqrt(MSE.eval(x, y, meta))\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.RMSE.eval","title":"<code>eval(x, y, meta)</code>  <code>staticmethod</code>","text":"<p>Root Mean Squared Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>torch.Tensor | np.ndarray Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>torch.Tensor | np.ndarray Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>WellMetadata Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Root mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Root Mean Squared Error\n\n    Args:\n        x: torch.Tensor | np.ndarray\n            Input tensor.\n        y: torch.Tensor | np.ndarray\n            Target tensor.\n        meta: WellMetadata\n            Metadata for the dataset.\n\n    Returns:\n        Root mean squared error between x and y.\n    \"\"\"\n    return torch.sqrt(MSE.eval(x, y, meta))\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.VMSE","title":"<code>VMSE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class VMSE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Variance Scaled Mean Squared Error\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n\n        Returns:\n            Variance mean squared error between x and y.\n        \"\"\"\n        return NMSE.eval(x, y, meta, norm_mode=\"std\")\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.VMSE.eval","title":"<code>eval(x, y, meta)</code>  <code>staticmethod</code>","text":"<p>Variance Scaled Mean Squared Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Variance mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Variance Scaled Mean Squared Error\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n\n    Returns:\n        Variance mean squared error between x and y.\n    \"\"\"\n    return NMSE.eval(x, y, meta, norm_mode=\"std\")\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.VRMSE","title":"<code>VRMSE</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>class VRMSE(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor | np.ndarray,\n        y: torch.Tensor | np.ndarray,\n        meta: WellMetadata,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Root Variance Scaled Mean Squared Error\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n\n        Returns:\n            Root variance mean squared error between x and y.\n        \"\"\"\n        return NRMSE.eval(x, y, meta, norm_mode=\"std\")\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.VRMSE.eval","title":"<code>eval(x, y, meta)</code>  <code>staticmethod</code>","text":"<p>Root Variance Scaled Mean Squared Error</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor | ndarray</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor | ndarray</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Root variance mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spatial.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor | np.ndarray,\n    y: torch.Tensor | np.ndarray,\n    meta: WellMetadata,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Root Variance Scaled Mean Squared Error\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n\n    Returns:\n        Root variance mean squared error between x and y.\n    \"\"\"\n    return NRMSE.eval(x, y, meta, norm_mode=\"std\")\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.binned_spectral_mse","title":"<code>binned_spectral_mse</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>the_well/benchmark/metrics/spectral.py</code> <pre><code>class binned_spectral_mse(Metric):\n    @staticmethod\n    def eval(\n        x: torch.Tensor,\n        y: torch.Tensor,\n        meta: WellMetadata,\n        bins: torch.Tensor = None,\n        fourier_input: bool = False,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Binned Spectral Mean Squared Error.\n        Corresponds to MSE computed after filtering over wavenumber bins in the Fourier domain.\n\n        Default binning is a set of three (approximately) logspaced from 0 to pi.\n\n        Note that, MSE(x, y) should match the sum over frequency bins of the spectral MSE.\n\n        Args:\n            x: Input tensor.\n            y: Target tensor.\n            meta: Metadata for the dataset.\n            bins:\n                Tensor of bin edges. If None, we use a default binning that is a set of three (approximately) logspaced from 0 to pi. The default is None.\n            fourier_input:\n                If True, x and y are assumed to be the Fourier transform of the input data. The default is False.\n\n        Returns:\n            The power spectrum mean squared error between x and y.\n        \"\"\"\n        spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n        spatial_shape = tuple(x.shape[dim] for dim in spatial_dims)\n        prod_spatial_shape = np.prod(np.array(spatial_shape))\n        ndims = meta.n_spatial_dims\n\n        if bins is None:  # Default binning\n            bins = torch.logspace(\n                np.log10(2 * np.pi / max(spatial_shape)),\n                np.log10(np.pi * np.sqrt(ndims) + 1e-6),\n                4,\n            ).to(x.device)  # Low, medium, and high frequency bins\n            bins[0] = 0.0  # We start from zero\n        _, ps_res_mean, _, counts = power_spectrum(\n            x - y, meta, bins=bins, fourier_input=fourier_input, return_counts=True\n        )\n\n        # TODO - MAJOR DESIGN VIOLATION - BUT ITS FASTER TO IMPLEMENT THIS WAY TODAY...\n        _, ps_true_mean, _, true_counts = power_spectrum(\n            y, meta, bins=bins, fourier_input=fourier_input, return_counts=True\n        )\n\n        # Compute the mean squared error per bin (stems from Plancherel's formula)\n        mse_per_bin = ps_res_mean * counts[:-1].unsqueeze(-1) / prod_spatial_shape**2\n        true_energy_per_min = (\n            ps_true_mean * true_counts[:-1].unsqueeze(-1) / prod_spatial_shape**2\n        )\n        nmse_per_bin = mse_per_bin / (true_energy_per_min + 1e-7)\n\n        mse_dict = {\n            f\"spectral_error_mse_per_bin_{i}\": mse_per_bin[..., i, :]\n            for i in range(mse_per_bin.shape[-2])\n        }\n        nmse_dict = {\n            f\"spectral_error_nmse_per_bin_{i}\": nmse_per_bin[..., i, :]\n            for i in range(nmse_per_bin.shape[-2])\n        }\n        out_dict = mse_dict\n        # Hacked to add this here for now - should be split with taking PS as input\n        out_dict |= nmse_dict\n        # TODO Figure out better way to handle multi-output losses\n        return out_dict\n</code></pre>"},{"location":"api/#the_well.benchmark.metrics.binned_spectral_mse.eval","title":"<code>eval(x, y, meta, bins=None, fourier_input=False)</code>  <code>staticmethod</code>","text":"<p>Binned Spectral Mean Squared Error. Corresponds to MSE computed after filtering over wavenumber bins in the Fourier domain.</p> <p>Default binning is a set of three (approximately) logspaced from 0 to pi.</p> <p>Note that, MSE(x, y) should match the sum over frequency bins of the spectral MSE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <code>y</code> <code>Tensor</code> <p>Target tensor.</p> required <code>meta</code> <code>WellMetadata</code> <p>Metadata for the dataset.</p> required <code>bins</code> <code>Tensor</code> <p>Tensor of bin edges. If None, we use a default binning that is a set of three (approximately) logspaced from 0 to pi. The default is None.</p> <code>None</code> <code>fourier_input</code> <code>bool</code> <p>If True, x and y are assumed to be the Fourier transform of the input data. The default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The power spectrum mean squared error between x and y.</p> Source code in <code>the_well/benchmark/metrics/spectral.py</code> <pre><code>@staticmethod\ndef eval(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    meta: WellMetadata,\n    bins: torch.Tensor = None,\n    fourier_input: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Binned Spectral Mean Squared Error.\n    Corresponds to MSE computed after filtering over wavenumber bins in the Fourier domain.\n\n    Default binning is a set of three (approximately) logspaced from 0 to pi.\n\n    Note that, MSE(x, y) should match the sum over frequency bins of the spectral MSE.\n\n    Args:\n        x: Input tensor.\n        y: Target tensor.\n        meta: Metadata for the dataset.\n        bins:\n            Tensor of bin edges. If None, we use a default binning that is a set of three (approximately) logspaced from 0 to pi. The default is None.\n        fourier_input:\n            If True, x and y are assumed to be the Fourier transform of the input data. The default is False.\n\n    Returns:\n        The power spectrum mean squared error between x and y.\n    \"\"\"\n    spatial_dims = tuple(range(-meta.n_spatial_dims - 1, -1))\n    spatial_shape = tuple(x.shape[dim] for dim in spatial_dims)\n    prod_spatial_shape = np.prod(np.array(spatial_shape))\n    ndims = meta.n_spatial_dims\n\n    if bins is None:  # Default binning\n        bins = torch.logspace(\n            np.log10(2 * np.pi / max(spatial_shape)),\n            np.log10(np.pi * np.sqrt(ndims) + 1e-6),\n            4,\n        ).to(x.device)  # Low, medium, and high frequency bins\n        bins[0] = 0.0  # We start from zero\n    _, ps_res_mean, _, counts = power_spectrum(\n        x - y, meta, bins=bins, fourier_input=fourier_input, return_counts=True\n    )\n\n    # TODO - MAJOR DESIGN VIOLATION - BUT ITS FASTER TO IMPLEMENT THIS WAY TODAY...\n    _, ps_true_mean, _, true_counts = power_spectrum(\n        y, meta, bins=bins, fourier_input=fourier_input, return_counts=True\n    )\n\n    # Compute the mean squared error per bin (stems from Plancherel's formula)\n    mse_per_bin = ps_res_mean * counts[:-1].unsqueeze(-1) / prod_spatial_shape**2\n    true_energy_per_min = (\n        ps_true_mean * true_counts[:-1].unsqueeze(-1) / prod_spatial_shape**2\n    )\n    nmse_per_bin = mse_per_bin / (true_energy_per_min + 1e-7)\n\n    mse_dict = {\n        f\"spectral_error_mse_per_bin_{i}\": mse_per_bin[..., i, :]\n        for i in range(mse_per_bin.shape[-2])\n    }\n    nmse_dict = {\n        f\"spectral_error_nmse_per_bin_{i}\": nmse_per_bin[..., i, :]\n        for i in range(nmse_per_bin.shape[-2])\n    }\n    out_dict = mse_dict\n    # Hacked to add this here for now - should be split with taking PS as input\n    out_dict |= nmse_dict\n    # TODO Figure out better way to handle multi-output losses\n    return out_dict\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>To showcase the dataset and the associated benchmarking library, we provide a set of simple baselines time-boxed to 12 hours on a single NVIDIA H100 to demonstrate the effectiveness of naive approaches on these challenging problems and motivate the development of more sophisticated approaches. These baselines are trained on the forward problem - predicting the next snapshot of a given simulation from a short history of 4 time-steps. The models used here are the Fourier Neural Operator, Tucker-Factorized FNO, U-net and a modernized U-net using ConvNext blocks. The neural operator models are implemented using the  neuraloperator  library.</p> <p>We emphasize that these settings are not selected to explore peak performance of modern machine learning, but rather that they reflect reasonable compute budgets and off-the-shelf choices that might be selected by a domain scientist exploring machine learning for their problems. Therefore, we focus on popular models using settings that are either defaults or commonly tuned.</p>"},{"location":"benchmarks/#benchmarked-model-checkpoints","title":"Benchmarked Model Checkpoints","text":"<p>Most of the checkpoints of the models are available on Hugging Face. To load a specific checkpoint follow the example below of the FNO model trained on the <code>active_matter</code> dataset.</p> <pre><code>from the_well.benchmark.models import FNO\n\nmodel = FNO.from_pretrained(\"polymathic-ai/FNO-active_matter\")\n</code></pre>"},{"location":"benchmarks/#test-results","title":"Test results","text":"Dataset FNO TFNO U-net CNextU-net <code>acoustic_scattering_maze</code> 0.5062 0.5057 0.0351 0.0153 <code>active_matter</code> 0.3691 0.3598 0.2489 0.1034 <code>convective_envelope_rsg</code> 0.0269 0.0283 0.0555 0.0799 <code>euler_multi_quadrants_periodicBC</code> 0.4081 0.4163 0.1834 0.1531 <code>gray_scott_reaction_diffusion</code> 0.1365 0.3633 0.2252 0.1761 <code>helmholtz_staircase</code> 0.00046 0.00346 0.01931 0.02758 <code>MHD_64</code> 0.3605 0.3561 0.1798 0.1633 <code>planetswe</code> 0.1727 0.0853 0.3620 0.3724 <code>post_neutron_star_merger</code> 0.3866 0.3793 - - <code>rayleigh_benard</code> 0.8395 0.6566 1.4860 0.6699 <code>rayleigh_taylor_instability</code> (At = 0.25) &gt;10 &gt;10 &gt;10 &gt;10 <code>shear_flow</code> 1.189 1.472 3.447 0.8080 <code>supernova_explosion_64</code> 0.3783 0.3785 0.3063 0.3181 <code>turbulence_gravity_cooling</code> 0.2429 0.2673 0.6753 0.2096 <code>turbulent_radiative_layer_2D</code> 0.5001 0.5016 0.2418 0.1956 <code>turbulent_radiative_layer_3D</code> 0.5278 0.5187 0.3728 0.3667 <code>viscoelastic_instability</code> 0.7212 0.7102 0.4185 0.2499 <p>Table 1: Model Performance Comparison - VRMSE metrics on test sets (lower is better) for models performing best on the validation set (results below). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1. Test set results for models performing best on the validation set.</p>"},{"location":"benchmarks/#validation-results","title":"Validation results","text":"Dataset FNO TFNO U-net CNextU-net <code>acoustic_scattering_maze</code> 0.5033 0.5034 0.0395 0.0196 <code>active_matter</code> 0.3157 0.3342 0.2609 0.0953 <code>convective_envelope_rsg</code> 0.0224 0.0195 0.0701 0.0663 <code>euler_multi_quadrants_periodicBC</code> 0.3993 0.4110 0.2046 0.1228 <code>gray_scott_reaction_diffusion</code> 0.2044 0.1784 0.5870 0.3596 <code>helmholtz_staircase</code> 0.00160 0.00031 0.01655 0.00146 <code>MHD_64</code> 0.3352 0.3347 0.1988 0.1487 <code>planetswe</code> 0.0855 0.1061 0.3498 0.3268 <code>post_neutron_star_merger</code> 0.4144 0.4064 - - <code>rayleigh_benard</code> 0.6049 0.8568 0.8448 0.4807 <code>rayleigh_taylor_instability</code> (At = 0.25) 0.4013 0.2251 0.6140 0.3771 <code>shear_flow</code> 0.4450 0.3626 0.836 0.3972 <code>supernova_explosion_64</code> 0.3804 0.3645 0.3242 0.2801 <code>turbulence_gravity_cooling</code> 0.2381 0.2789 0.3152 0.2093 <code>turbulent_radiative_layer_2D</code> 0.4906 0.4938 0.2394 0.1247 <code>turbulent_radiative_layer_3D</code> 0.5199 0.5174 0.3635 0.3562 <code>viscoelastic_instability</code> 0.7195 0.7021 0.3147 0.1966 <p>Table 2: Dataset and model comparison in VRMSE metric on the validation sets, best result in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"benchmarks/#rollout-loss-612","title":"Rollout loss (6:12)","text":"Dataset FNO \\(\\phantom{T}\\) (6:12) TFNO (6:12) U-net (6:12) CNextU-net (6:12) <code>acoustic_scattering_maze</code> 1.06 1.13 0.56 0.78 <code>active_matter</code> \\(&gt;\\)10 7.52 2.53 2.11 <code>convective_envelope_rsg</code> 0.28 0.32 0.76 1.15 <code>euler_multi_quadrants_periodicBC</code> 1.13 1.23 1.02 4.98 <code>gray_scott_reaction_diffusion</code> 0.89 1.54 0.57 0.29 <code>helmholtz_staircase</code> 0.002 0.011 0.057 0.110 <code>MHD_64</code> 1.24 1.25 1.65 1.30 <code>planetswe</code> 0.81 0.29 1.18 0.42 <code>post_neutron_star_merger</code> 0.76 0.70 --- --- <code>rayleigh_benard</code> \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 <code>rayleigh_taylor_instability</code> \\(&gt;\\)10 6.72 \\(&gt;\\)10 \\(&gt;\\)10 <code>shear_flow</code> \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 2.33 <code>supernova_explosion_64</code> 2.41 1.86 0.94 1.12 <code>turbulence_gravity_cooling</code> 3.55 4.49 7.14 1.30 <code>turbulent_radiative_layer_2D</code> 1.79 6.01 0.66 0.54 <code>turbulent_radiative_layer_3D</code> 0.81 \\(&gt;\\)10 0.95 0.77 <code>viscoelastic_instability</code> 4.11 0.93 0.89 0.52"},{"location":"benchmarks/#rollout-loss-1330","title":"Rollout loss (13:30)","text":"Dataset FNO (13:30) TFNO (13:30) U-net (13:30) CNextU-net (13:30) <code>acoustic_scattering_maze</code> 1.72 1.23 0.92 1.13 <code>active_matter</code> \\(&gt;\\)10 4.72 2.62 2.71 <code>convective_envelope_rsg</code> 0.47 0.65 2.16 1.59 <code>euler_multi_quadrants_periodicBC</code> 1.37 1.52 1.63 \\(&gt;\\)10 <code>gray_scott_reaction_diffusion</code> \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 7.62 <code>helmholtz_staircase</code> 0.003 0.019 0.097 0.194 <code>MHD_64</code> 1.61 1.81 4.66 2.23 <code>planetswe</code> 2.96 0.55 1.92 0.52 <code>post_neutron_star_merger</code> 1.05 1.05 --- --- <code>rayleigh_benard</code> \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 <code>rayleigh_taylor_instability</code> \\(&gt;\\)10 \\(&gt;\\)10 2.84 7.43 <code>shear_flow</code> \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 \\(&gt;\\)10 <code>supernova_explosion_64</code> \\(&gt;\\)10 \\(&gt;\\)10 1.69 4.55 <code>turbulence_gravity_cooling</code> 5.63 6.95 4.15 2.09 <code>turbulent_radiative_layer_2D</code> 3.54 \\(&gt;\\)10 1.04 1.01 <code>turbulent_radiative_layer_3D</code> 0.94 \\(&gt;\\)10 1.09 0.86 <code>viscoelastic_instability</code> --- --- --- --- <p>Table: Time-Averaged Losses by Window - VRMSE metrics on test sets (lower is better), averaged over time windows (6:12) and (13:30). Best results are shown in bold for (6:12) and underlined for (13:30). VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"data_format/","title":"Data format","text":"<p>The raw data of the Well are stored as HDF5 files, internally organized following a shared specification. The data stored in these files have been generated on uniform grids and sampled at constant time intervals. These files include all available state variables or spatially varying coefficients associated with a given set of dynamics in numpy arrays of shape (<code>n_traj</code>, <code>n_steps</code>, <code>coord1</code>, <code>coord2</code>, <code>(coord3)</code>) in single precision fp32. We distinguish between scalar, vector, and tensor-valued fields due to their different transformation properties.</p> <p>The specification is described below with example entries for a hypothetical 2D (\\(D=2\\)) simulation with dimension B x T x W x H. Note that this uses HDF5 Groups, Datasets, and attributes (denoted by \"@\"):</p> <pre><code>root: Group\n  @simulation_parameters: list[str] = ['ParamA', ...]\n  @ParamA: float = 1.0\n  ... # Additional listed parameters\n  @dataset_name: str = 'ExampleDSet'\n  @grid_type: str = 'cartesian' # \"cartesian/spherical currently supported\"\n  @n_spatial_dims: int = 2 # Should match number of provided spatial dimensions.\n  @n_trajectories: int = B # \"Batch\" dimension of dataset\n\n  -dimensions: Group\n    @spatial_dims: list[str] = ['x', 'y'] # Names match datasets below.\n    time: Dataset = float32(T)\n      @sample_varying = False # Does this value vary between trajectories?\n    -x: Dataset = float32(W) # Grid coordinates in x\n      @sample_varying = False\n      @time_varying = False # True not currently supported\n    -y = float32(H) # Grid coordinates in y\n      @sample_varying = False\n      @time_varying = False\n\n  -boundary_conditions: Group # Internal and external boundary conditions\n    -X_boundary: Group\n      @associated_dims: list[str] = ['x'] # Defined on x\n      # If associated with set values for given field.\n      @associated_fields: list[str] = []\n      # Geometric description of BC. Currently support periodic/wall/open\n      @bc_type = 'periodic'\n      @sample_varying = False\n      @time_varying = False\n      -mask: Dataset = bool(W) # True on coordinates where boundary is defined.\n      -values: Dataset = float32(NumTrue(mask)) # Values defined on mask points\n\n  scalars: Group # Non-spatially varying scalars.\n    @field_names: list[str] = ['ParamA', 'OtherScalar', ...]\n    ParamA: Dataset = float32(1)\n      @sample_varying = False # Does this vary between trajectories?\n      @time_varying = False # Does this vary over time?\n    OtherScalar: Dataset = float32(T)\n      @sample_varying = False\n      @time_varying = True\n\n  t0_fields: Group\n    # field_names should list all datasets in this category\n    @field_names: list[str] = ['FieldA', 'FieldB', 'FieldC', ...]\n    -FieldA: Dataset = float32(BxTxWxH)\n      @dim_varying = [ True  True]\n      @sample_varying = True\n      @time_varying = True\n    -FieldB: Dataset = float32(TxWxH)\n      @dim_varying = [ True  True]\n      @sample_varying = True\n      @time_varying = False\n    -FieldC: Dataset = float32(BxTxH)\n      @dim_varying = [ True  False]\n      @sample_varying = True\n      @time_varying = True\n    ... # Additional fields\n\n  -t1_fields: Group\n    @field_names = ['VFieldA', ...]\n    -VFieldA: Dataset = float32(BxTxWxHxD)\n      @dim_varying = [ True  True]\n      @sample_varying = True\n      @time_varying = True\n    ... # Additional fields\n\n  -t2_fields: Group\n    @field_names: list[str] = ['TFieldA', ...]\n    - TFieldA: Dataset = float32(BxTxWxHxD^2)\n      @antisymmetric = False\n      @dim_varying = [ True  True]\n      @sample_varying = True\n      @symmetric = True # Whether tensor is symmetric\n      @time_varying = True\n    ... # Additional fields\n</code></pre>"},{"location":"datasets_overview/","title":"Overview of the Dataset Collection","text":"<p>The Well is composed of 16 datasets totaling 15TB of data with individual datasets ranging from 6.9GB to 5.1TB. The data is provided on uniform grids and sampled at constant time intervals. Data and associated metadata are stored in self-documenting <code>HDF5</code> and metadata <code>dataset_name.yaml</code> files. All datasets use a shared data specification described in [api.md#data-format]. These files include all available state variables or spatially varying coefficients associated with a given set of dynamics in <code>numpy</code> arrays of shape <code>(n_traj, n_steps, coord1, coord2, (coord3))</code> in single precision <code>fp32</code>. We distinguish between scalar (<code>t0_fields</code>), vector (<code>t1_fields</code>), and tensor-valued fields (<code>t2_fields</code>) due to their different transformation properties. Each file is randomly split into training/testing/validation sets with a respective split of 0.8/0.1/0.1 * <code>n_traj</code>. Details of individual datasets are given in the following table:</p> Dataset CS Resolution (pixels) n_steps n_traj <code>acoustic_scattering</code> Cartesian 2D 256 \u00d7 256 100 8,000 <code>active_matter</code> Cartesian 2D 256 \u00d7 256 81 360 <code>convective_envelope_rsg</code> Spherical 256 \u00d7 128 \u00d7 256 100 29 <code>euler_multi_quadrants</code> Cartesian 2D 512 \u00d7 512 100 10,000 <code>gray_scott_reaction_diffusion</code> Cartesian 2D 128 \u00d7 128 1,001 1,200 <code>helmholtz_staircase</code> Cartesian 2D 1,024 \u00d7 256 50 512 <code>MHD</code> Cartesian 3D 64\u00b3 and 256\u00b3 100 100 <code>planetswe</code> Angular 256 \u00d7 512 1,008 120 <code>post_neutron_star_merger</code> Log-Spherical 192 \u00d7 128 \u00d7 66 181 8 <code>rayleigh_benard</code> Cartesian 2D 512 \u00d7 128 200 1,750 <code>rayleigh_taylor_instability</code> Cartesian 3D 128 \u00d7 128 \u00d7 128 120 45 <code>shear_flow</code> Cartesian 2D 128 \u00d7 256 200 1,120 <code>supernova_explosion</code> Cartesian 3D 64\u00b3 and 128\u00b3 59 1,000 <code>turbulence_gravity_cooling</code> Cartesian 3D 64 \u00d7 64 \u00d7 64 50 2,700 <code>turbulent_radiative_layer_2D</code> Cartesian 2D 128 \u00d7 384 101 90 <code>turbulent_radiative_layer_3D</code> Cartesian 3D 128 \u00d7 128 \u00d7 256 101 90 <code>viscoelastic_instability</code> Cartesian 2D 512 \u00d7 512 variable 260 <p>Table: Dataset description, coordinate system (CS), resolution of snapshots, n_steps (number of time-steps per trajectory), and n_traj (total number of trajectories in the dataset).</p> Dataset Size (GB) Run time (h) Hardware Software <code>acoustic_discontinuous</code> 157 0.25 64 C Clawpack <code>acoustic_inclusions</code> 283 0.25 64 C Clawpack <code>acoustic_maze</code> 311 0.33 64 C Clawpack <code>active_matter</code> 51.3 0.33 A100 GPU Python <code>convective_envelope_rsg</code> 570 1460 80 C Athena++ <code>euler</code> 5170 80* 160 C* ClawPack <code>helmholtz_staircase</code> 52 0.11 64 C Python <code>MHD_256</code> 4580 48 64 C Fortran MPI <code>MHD_64</code> 72 -- -- -- <code>gray_scott_reaction_diffusion</code> 154 33* 40 C Matlab <code>planetswe</code> 186 0.75 64 C Dedalus <code>post_neutron_star_merger</code> 110 505* 300 C* \u03bdbhlight <code>rayleigh_benard</code> 358 60* 768 C* Dedalus <code>rayleigh_taylor_instability</code> 256 65* 128 C* TurMix3D <code>shear_flow</code> 115 5* 448 C* Dedalus <code>supernova_explosion_128</code> 754 4* 1040 C* ASURA-FDPS <code>supernova_explosion_64</code> 268 4* 1040 C* ASURA-FDPS <code>turbulence_gravity_cooling</code> 829 577* 1040 C* ASURA-FDPS <code>turbulent_radiative_layer_2D</code> 6.9 2* 48 C Athena++ <code>turbulent_radiative_layer_3D</code> 745 271* 128 C Athena++ <code>viscoelastic_instability</code> 66 34* 64 C Dedalus <p>Table: Information about the different dataset generation. In the running time and hardware columns, * denotes a total for all the runs. Otherwise, these figures are given for running one simulation only. For hardware, C denotes the number of Cores. Computation was performed on nodes equipped with either 2 48-core AMD Genoa or 2 32-core Intel Icelake.</p>"},{"location":"datasets/MHD_256/","title":"Magnetohydrodynamics (MHD) compressible turbulence","text":"<p>NOTE: This dataset is available in two different resolutions \\(256^3\\) for <code>MHD_256</code> and \\(64^3\\) for <code>MHD_64</code>. The data was first generated at \\(256^3\\) and then downsampled to \\(64^3\\) after anti-aliasing with an ideal low-pass filter. The data is available in both resolutions.</p> <p>One line description of the data: This is an MHD fluid flows in the compressible limit (subsonic, supersonic, sub-Alfvenic, super-Alfvenic).</p> <p>Longer description of the data: An essential component of the solar wind, galaxy formation, and of interstellar medium (ISM) dynamics is magnetohydrodynamic (MHD) turbulence. This dataset consists of isothermal MHD simulations without self-gravity (such as found in the diffuse ISM) initially generated with resolution \\(256^3\\) and then downsampled to \\(64^3\\) after anti-aliasing with an ideal low-pass filter.</p> <p>Associated paper: Paper.</p> <p>Domain expert: Blakesley Burkhart, Center for Computational Astrophysics, Flatiron Institute &amp; Rutgers University.</p> <p>Code or software used to generate the data: Fortran + MPI.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v}) &amp;= 0 \\\\ \\frac{\\partial \\rho \\mathbf{v}}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v} \\mathbf{v} - \\mathbf{B} \\mathbf{B}) + \\nabla p &amp;= 0 \\\\ \\frac{\\partial \\mathbf{B}}{\\partial t} - \\nabla \\times (\\mathbf{v} \\times \\mathbf{B}) &amp;= 0 \\end{align*} \\] <p>where \\(\\rho\\) is the density, \\(\\mathbf{v}\\) is the velocity, \\(\\mathbf{B}\\) is the magnetic field, \\(\\mathbf{I}\\) the identity matrix and \\(p\\) is the gas pressure.</p> <p></p>"},{"location":"datasets/MHD_256/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 100 timesteps of 256 \\(\\times\\) 256 \\(\\times\\) 256 cubes.</p> <p>Fields available in the data: Density (scalar field), velocity (vector field), magnetic field (vector field).</p> <p>Number of trajectories: 10 Initial conditions x 10 combination of parameters = 100 trajectories.</p> <p>Estimated size of the ensemble of all simulations: 4.58TB.</p> <p>Grid type: uniform grid, cartesian coordinates.</p> <p>Initial conditions: uniform IC.</p> <p>Boundary conditions: periodic boundary conditions.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 0.01 (arbitrary units).</p> <p>Total time range (\\(t\\_{min}\\) to \\(t\\_{max}\\)): \\(t\\_{min} = 0\\), \\(t\\_{max} = 1\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): dimensionless so $ L_x = L_y = L_z = 256$ pixels.</p> <p>Set of coefficients or non-dimensional parameters evaluated: all combinations of \\(\\mathcal{M}_s=\\){0.5, 0.7, 1.5, 2.0 7.0} and \\(\\mathcal{M}_A =\\){0.7, 2.0}.</p> <p>Approximate time to generate the data: 48 hours per simulation.</p> <p>Hardware used to generate the data: 64 cores.</p>"},{"location":"datasets/MHD_256/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).</p> <p>How to evaluate a new simulator operating in this space: Check metrics such as Power spectrum, two points correlation function.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{burkhart2020catalogue,\n  title={The catalogue for astrophysical turbulence simulations (cats)},\n  author={Burkhart, B and Appel, SM and Bialy, S and Cho, J and Christensen, AJ and Collins, D and Federrath, Christoph and Fielding, DB and Finkbeiner, D and Hill, AS and others},\n  journal={The Astrophysical Journal},\n  volume={905},\n  number={1},\n  pages={14},\n  year={2020},\n  publisher={IOP Publishing}\n}\n</code></pre>"},{"location":"datasets/MHD_64/","title":"Magnetohydrodynamics (MHD) compressible turbulence","text":"<p>NOTE: This dataset is available in two different resolutions \\(256^3\\) for <code>MHD_256</code> and \\(64^3\\) for <code>MHD_64</code>. The data was first generated at \\(256^3\\) and then downsampled to \\(64^3\\) after anti-aliasing with an ideal low-pass filter. The data is available in both resolutions.</p> <p>One line description of the data: This is an MHD fluid flows in the compressible limit (subsonic, supersonic, sub-Alfvenic, super-Alfvenic).</p> <p>Longer description of the data: An essential component of the solar wind, galaxy formation, and of interstellar medium (ISM) dynamics is magnetohydrodynamic (MHD) turbulence. This dataset consists of isothermal MHD simulations without self-gravity (such as found in the diffuse ISM) initially generated with resolution \\(256^3\\) and then downsampled to \\(64^3\\) after anti-aliasing with an ideal low-pass filter. This dataset is the downsampled version.</p> <p>Associated paper: Paper</p> <p>Domain expert: Blakesley Burkhart, CCA, Flatiron Institute &amp; Rutgers University.</p> <p>Code or software used to generate the data: Fortran + MPI.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v}) &amp;= 0 \\\\ \\frac{\\partial \\rho \\mathbf{v}}{\\partial t} + \\nabla \\cdot (\\rho \\mathbf{v} \\mathbf{v} - \\mathbf{B} \\mathbf{B}) + \\nabla p &amp;= 0 \\\\ \\frac{\\partial \\mathbf{B}}{\\partial t} - \\nabla \\times (\\mathbf{v} \\times \\mathbf{B}) &amp;= 0 \\end{align*} \\] <p>where \\(\\rho\\) is the density, \\(\\mathbf{v}\\) is the velocity, \\(\\mathbf{B}\\) is the magnetic field, \\(\\mathbf{I}\\) the identity matrix and \\(p\\) is the gas pressure.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>MHD_64</code> 0.3605 3561 0.1798 \\(\\mathbf{0.1633}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/MHD_64/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 100 timesteps of 64 \\(\\times\\) 64 \\(\\times\\) 64 cubes.</p> <p>Fields available in the data: Density (scalar field), velocity (vector field), magnetic field (vector field).</p> <p>Number of trajectories: 10 Initial conditions x 10 combination of parameters = 100 trajectories.</p> <p>Estimated size of the ensemble of all simulations: 71.6 GB.</p> <p>Grid type: uniform grid, cartesian coordinates.</p> <p>Initial conditions: uniform IC.</p> <p>Boundary conditions: periodic boundary conditions.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 0.01 (arbitrary units).</p> <p>Total time range (\\(t\\_{min}\\) to \\(t\\_{max}\\)): \\(t\\_{min} = 0\\), \\(t\\_{max} = 1\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): dimensionless so 64 pixels.</p> <p>Set of coefficients or non-dimensional parameters evaluated: all combinations of \\(\\mathcal{M}_s=\\){0.5, 0.7, 1.5, 2.0 7.0} and \\(\\mathcal{M}_A =\\){0.7, 2.0}.</p> <p>Approximate time and hardware used to generate the data: Downsampled from <code>MHD_256</code> after applying ideal low-pass filter.</p>"},{"location":"datasets/MHD_64/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).</p> <p>How to evaluate a new simulator operating in this space: Check metrics such as Power spectrum, two-points correlation function.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{burkhart2020catalogue,\n  title={The catalogue for astrophysical turbulence simulations (cats)},\n  author={Burkhart, B and Appel, SM and Bialy, S and Cho, J and Christensen, AJ and Collins, D and Federrath, Christoph and Fielding, DB and Finkbeiner, D and Hill, AS and others},\n  journal={The Astrophysical Journal},\n  volume={905},\n  number={1},\n  pages={14},\n  year={2020},\n  publisher={IOP Publishing}\n}\n</code></pre>"},{"location":"datasets/acoustic_scattering_discontinuous/","title":"Acoustic Scattering - Single Discontinuity","text":"<p>One line description of the data: Simple acoustic wave propogation over a domain split into two continuously varying sub-domains with a single discountinuous interface.</p> <p>Longer description of the data: These variable-coefficient acoustic equations describe the propogation of an acoustic pressure wave through domains consisting of multiple materials with different scattering properties. This problem emerges in source optimization and it's inverse - that of identifying the material properties from the scattering of the wave - is a vital problem in geology and radar design. This is the simplest of three scenarios. In this case, we have a variable number of initial point sources and single discontinuity separating two sub-domains. Within each subdomain, the density of the underlying material varies smoothly.</p> <p>Domain expert: Michael McCabe, Polymathic AI.</p> <p>Code or software used to generate the data: Clawpack, adapted from this example.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{ \\partial p}{\\partial t} + K(x, y) \\left( \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} \\right) &amp;= 0 \\\\ \\frac{ \\partial u  }{\\partial t} + \\frac{1}{\\rho(x, y)} \\frac{\\partial p}{\\partial x} &amp;= 0 \\\\ \\frac{ \\partial v  }{\\partial t} + \\frac{1}{\\rho(x, y)} \\frac{\\partial p}{\\partial v} &amp;= 0 \\end{align*} \\] <p>with \\(\\rho\\) the material density, \\(u, v\\) the velocity in the \\(x, y\\) directions respectively, \\(p\\) the pressure, and \\(K\\) the bulk modulus.</p> <p>Example material densities can be seen below:</p> <p></p>"},{"location":"datasets/acoustic_scattering_discontinuous/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 101 steps of 256 \\(\\times\\) 256 images.</p> <p>Fields available in the data: pressure (scalar field), material density (constant scalar field), material speed of sound (constant scalar field), velocity field (vector field).</p> <p>Number of trajectories: 2000.</p> <p>Estimated size of the ensemble of all simulations: 157.7 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Flat pressure static field with 1-4 high pressure rings randomly placed in domain. The rings are defined with variable intensity \\(\\sim \\mathcal U(.5, 2)\\) and radius \\(\\sim \\mathcal U(.06, .15)\\).</p> <p>Boundary conditions: Open domain in \\(y\\), reflective walls in \\(x\\).</p> <p>Simulation time-step: Variable based on CFL with safety factor .25.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 2/101.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): [0, 2]</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\)): [-1, 1] x [-1, 1]</p> <p>Set of coefficients or non-dimensional parameters evaluated:</p> <ul> <li> <p>\\(K\\) is fixed at 4.0.</p> </li> <li> <p>\\(\\rho\\) is the primary coefficient here. Each side is generated with one of the following distributions:</p> </li> <li>Gaussian Bump - Peak density samples from \\(\\sim\\mathcal U(1, 7)\\) and \\(\\sigma \\sim\\mathcal U(.1, 5)\\) with the center of the bump uniformly sampled from the extent of the subdomain.</li> <li>Linear gradient - Four corners sampled with \\(\\rho \\sim \\mathcal U(1, 7)\\). Inner density is bilinearly interpolated.</li> <li>Constant - Constant \\(\\rho \\sim\\mathcal U(1, 7)\\).</li> <li>Smoothed Gaussian Noise - Constant background sampled \\(\\rho \\sim\\mathcal U(1, 7)\\) with IID standard normal noise applied. This is then smoothed by a Gaussian filter of varying sigma \\(\\sigma \\sim\\mathcal U(5, 10)\\).</li> </ul> <p>Approximate time to generate the data: ~15 minutes per simulation.</p> <p>Hardware used to generate the data and precision used for generating the data: 64 Intel Icelake cores per simulation. Generated in double precision.</p>"},{"location":"datasets/acoustic_scattering_discontinuous/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>Wave propogation through discontinuous media. Most existing machine learning datasets for computational physics are highly smooth and the acoustic challenges presented here offer challenging discontinuous scenarios that approximate complicated geometry through the variable density.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{mandli2016clawpack,\n  title={Clawpack: building an open source ecosystem for solving hyperbolic PDEs},\n  author={Mandli, Kyle T and Ahmadia, Aron J and Berger, Marsha and Calhoun, Donna and George, David L and Hadjimichael, Yiannis and Ketcheson, David I and Lemoine, Grady I and LeVeque, Randall J},\n  journal={PeerJ Computer Science},\n  volume={2},\n  pages={e68},\n  year={2016},\n  publisher={PeerJ Inc.}\n}\n</code></pre>"},{"location":"datasets/acoustic_scattering_inclusions/","title":"Acoustic Scattering - Inclusions","text":"<p>One line description of the data: Simple acoustic wave propogation over a domain split into two continuously varying sub-domains with a single discountinuous interface. With additive randomly generating inclusions (materials of significantly different density).</p> <p>Longer description of the data: These variable-coefficient acoustic equations describe the propogation of an acoustic pressure wave through domains consisting of multiple materials with different scattering properties. This problem emerges in source optimization and it's inverse - that of identifying the material properties from the scattering of the wave - is a vital problem in geology and radar design. In this case, we have a variable number of initial point sources and a domain with random inclusions. These types of problems are of particular interest in geology where the inverse scattering is used to identify mineral deposits.</p> <p>Domain expert: Michael McCabe, Polymathic AI.</p> <p>Code or software used to generate the data: Clawpack, adapted from: this example.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{ \\partial p}{\\partial t} + K(x, y) \\left( \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} \\right) &amp;= 0 \\\\ \\frac{ \\partial u  }{\\partial t} + \\frac{1}{\\rho(x, y)} \\frac{\\partial p}{\\partial x} &amp;= 0 \\\\ \\frac{ \\partial v  }{\\partial t} + \\frac{1}{\\rho(x, y)} \\frac{\\partial p}{\\partial v} &amp;= 0 \\end{align*} \\] <p>with \\(\\rho\\) the material density, \\(u, v\\) the velocity in the \\(x, y\\) directions respectively, \\(p\\) the pressure, and \\(K\\) the bulk modulus.</p> <p>Example material densities can be seen below:</p> <p></p>"},{"location":"datasets/acoustic_scattering_inclusions/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 101 steps of 256 \\(\\times\\) 256 images.</p> <p>Fields available in the data: pressure (scalar field), material density (constant scalar field), material speed of sound (constant scalar field), velocity field (vector field).</p> <p>Number of trajectories: 4000.</p> <p>Estimated size of the ensemble of all simulations: 283.8 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Flat pressure static field with 1-4 high pressure rings randomly placed in domain. The rings are defined with variable intensity \\(\\sim \\mathcal U(.5, 2)\\) and radius \\(\\sim \\mathcal U(.06, .15)\\).</p> <p>Boundary conditions: Open domain in \\(y\\), reflective walls in \\(x\\).</p> <p>Simulation time-step: Variable based on CFL with safety factor .25.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 2/101.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): [0, 2.].</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\)): [-1, 1] x [-1, 1].</p> <p>Set of coefficients or non-dimensional parameters evaluated:</p> <ul> <li> <p>\\(K\\) is fixed at 4.0.</p> </li> <li> <p>\\(\\rho\\) is the primary coefficient here. This is a superset of the single discontinuity example so the background is first generated two splits with one of the following distributions:</p> </li> <li>Gaussian Bump - Peak density samples from \\(\\sim\\mathcal U(1, 7)\\) and \\(\\sigma \\sim\\mathcal U(.1, 5)\\) with the center of the bump uniformly sampled from the extent of the subdomain.</li> <li>Linear gradient - Four corners sampled with \\(\\rho \\sim \\mathcal U(1, 7)\\). Inner density is bilinearly interpolated.</li> <li>Constant - Constant \\(\\rho \\sim\\mathcal U(1, 7)\\).</li> <li>Smoothed Gaussian Noise - Constant background sampled \\(\\rho \\sim\\mathcal U(1, 7)\\) with IID standard normal noise applied. This is then smoothed by a Gaussian filter of varying sigma \\(\\sigma \\sim\\mathcal U(5, 10)\\).</li> </ul> <p>Inclusions are then added as 1-15 random ellipsoids with center uniformly sampled from the domain and height/width sampled uniformly from [.05, .6]. The ellipsoid is then rotated randomly with angle sampled [-45, 45]. For the inclusions, \\(Ln(\\rho)\\sim \\mathcal U(-1, 10)\\).</p> <p>Approximate time to generate the data: ~15 minutes per simulation.</p> <p>Hardware used to generate the data and precision used for generating the data: 64 Intel Icelake cores per simulation. Generated in double precision.</p>"},{"location":"datasets/acoustic_scattering_inclusions/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>Wave propogation through discontinuous media. Most existing machine learning datasets for computational physics are highly smooth and the acoustic challenges presented here offer challenging discontinuous scenarios that approximate complicated geometry through the variable density. The inclusions change wave propogation speed but only in small, irregular areas.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{mandli2016clawpack,\n  title={Clawpack: building an open source ecosystem for solving hyperbolic PDEs},\n  author={Mandli, Kyle T and Ahmadia, Aron J and Berger, Marsha and Calhoun, Donna and George, David L and Hadjimichael, Yiannis and Ketcheson, David I and Lemoine, Grady I and LeVeque, Randall J},\n  journal={PeerJ Computer Science},\n  volume={2},\n  pages={e68},\n  year={2016},\n  publisher={PeerJ Inc.}\n}\n</code></pre>"},{"location":"datasets/acoustic_scattering_maze/","title":"Acoustic Scattering - Maze","text":"<p>One line description of the data: Simple acoustic wave propogation through maze-like structures.</p> <p>Longer description of the data: These variable-coefficient acoustic equations describe the propogation of an acoustic pressure wave through maze-like domains. Pressure waves emerge from point sources and propogate through domains consisting of low density maze paths and orders of magnitude higher density maze walls. This is built primarily as a challenge for machine learning methods, though has similar properties to optimal placement problems like WiFi in a building.</p> <p>Domain expert: Michael McCabe, Polymathic AI.</p> <p>Code or software used to generate the data: Clawpack,  adapted from this example.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{ \\partial p}{\\partial t} + K(x, y) \\left( \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} \\right) &amp;= 0 \\\\ \\frac{ \\partial u  }{\\partial t} + \\frac{1}{\\rho(x, y)} \\frac{\\partial p}{\\partial x} &amp;= 0 \\\\ \\frac{ \\partial v  }{\\partial t} + \\frac{1}{\\rho(x, y)} \\frac{\\partial p}{\\partial y} &amp;= 0 \\end{align*} \\] <p>with \\(\\rho\\) the material density, \\(u, v\\) the velocity in the \\(x, y\\) directions respectively, \\(p\\) the pressure, and \\(K\\) the bulk modulus.</p> <p>Example material densities can be seen below:</p> <p></p> <p>Traversal can be seen:</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>acoustic_scattering_maze</code> 0.5062 0.5057 0.0351 \\(\\mathbf{0.0153}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/acoustic_scattering_maze/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 201 steps of 256 \\(\\times\\) 256 images.</p> <p>Fields available in the data: pressure (scalar field), material density (constant scalar field), material speed of sound (constant scalar field), velocity field (vector field).</p> <p>Number of trajectories: 2000.</p> <p>Estimated size of the ensemble of all simulations: 311.3 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Flat pressure static field with 1-6 high pressure rings randomly placed along paths of maze. The rings are defined with variable intensity \\(\\sim \\mathcal U(3., 5.)\\) and radius \\(\\sim \\mathcal U(.01, .04)\\). Any overlap with walls is removed.</p> <p>Boundary conditions: Open domain in \\(y\\), reflective walls in \\(x\\).</p> <p>Simulation time-step: Variable based on CFL with safety factor .25.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 2/201.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): [0, 4.].</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\)): [-1, 1] x [-1, 1].</p> <p>Set of coefficients or non-dimensional parameters evaluated:</p> <ul> <li> <p>\\(K\\) is fixed at 4.0.</p> </li> <li> <p>\\(\\rho\\) is the primary coefficient here. We generated a maze with initial width between 6 and 16 pixels and upsample it via nearest neighbor resampling to create a 256 x 256 maze. The walls are set to \\(\\rho=10^6\\) while paths are set to  \\(\\rho=3\\).</p> </li> </ul> <p>Approximate time to generate the data: ~20 minutes per simulation.</p> <p>Hardware used to generate the data and precision used for generating the data: 64 Intel Icelake cores per simulation. Generated in double precision.</p>"},{"location":"datasets/acoustic_scattering_maze/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>This is an example of simple dynamics in complicated geometry. The sharp discontinuities can be a significant problem for machine learning models, yet they are a common feature in many real-world physics. While visually the walls appear to stop the signal, it is actually simply the case that the speed of sound is much much lower inside the walls leading to partial reflection/absorbtion at the interfaces.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{mandli2016clawpack,\n  title={Clawpack: building an open source ecosystem for solving hyperbolic PDEs},\n  author={Mandli, Kyle T and Ahmadia, Aron J and Berger, Marsha and Calhoun, Donna and George, David L and Hadjimichael, Yiannis and Ketcheson, David I and Lemoine, Grady I and LeVeque, Randall J},\n  journal={PeerJ Computer Science},\n  volume={2},\n  pages={e68},\n  year={2016},\n  publisher={PeerJ Inc.}\n}\n</code></pre>"},{"location":"datasets/active_matter/","title":"Active matter simulations","text":"<p>One line description of the data:  Modeling and simulation of biological active matter.</p> <p>Longer description of the data: Simulation of a continuum theory describing the dynamics of \\(N\\) rod-like active particles immersed in a Stokes fluid having linear dimension \\(L\\) and colume \\(L^2\\).</p> <p>Associated paper: Paper.</p> <p>Domain expert: Suryanarayana Maddu, Center for Computaional Biology, Flatiron Institute.</p> <p>Code or software used to generate the data: Github repository.</p> <p>Equations: Equations (1) to (5) of the associated paper.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>active_matter</code> 0.3691 0.3598 0.2489 \\(\\mathbf{0.1034}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/active_matter/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 81 time-steps of 256 \\(\\times\\) 256 images per trajectory.</p> <p>Fields available in the data: concentration (scalar field), velocity (vector field), orientation tensor (tensor field), strain-rate tensor (tensor field).</p> <p>Number of trajectories: \\(5\\) trajectories per parameter-set, each trajectory being generated with a different initialization of the state field { \\(c,D,U\\) }.</p> <p>Size of the ensemble of all simulations: 51.3 GB.</p> <p>Grid type: Uniform grid, cartesian coordinates.</p> <p>Initial conditions: The concentration is set to constant value \\(c(x,t)=1\\) and the orientation tensor is initialized as plane-wave perturbation about the isotropic state.</p> <p>Boundary conditions: Periodic boundary conditions.</p> <p>Simulation time-step: \\(3.90625\\times 10^{-4}\\) seconds.</p> <p>Data are stored separated by ( \\(\\Delta t\\) ): 0.25 seconds.</p> <p>Total time range ( \\(t_{min}\\) to \\(t_{max}\\) ): \\(0\\) to \\(20\\) seconds.</p> <p>Spatial domain size ( \\(L_x\\), \\(L_y\\) ): \\(L_x=10\\) and \\(L_y=10\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(\\alpha =\\) {-1,-2,-3,-4,-5}; \\(\\beta  =\\) {0.8}; \\(\\zeta =\\) {1,3,5,7,9,11,13,15,17}.</p> <p>Approximate time and hardware to generate the data: 20 minutes per simulation on an A100 GPU in double precision. There is a total of 225 simulations, which is approximately 75 hours.</p>"},{"location":"datasets/active_matter/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: How is energy being transferred between scales? How is vorticity coupled to the orientation field? Where does the transition from isotropic state to nematic state occur with the change in alignment ( \\(\\zeta\\) ) or dipole strength (\\(\\alpha\\))?</p> <p>How to evaluate a new simulator operating in this space: Reproducing some summary statistics like power spectra and average scalar order parameters. Additionally, being able to accurately capture the phase transition from isotropic to nematic state.</p> <p>Please cite the associated paper if you use this data in your research: <pre><code>@article{maddu2024learning,\n  title={Learning fast, accurate, and stable closures of a kinetic theory of an active fluid},\n  author={Maddu, Suryanarayana and Weady, Scott and Shelley, Michael J},\n  journal={Journal of Computational Physics},\n  volume={504},\n  pages={112869},\n  year={2024},\n  publisher={Elsevier}\n}\n</code></pre></p>"},{"location":"datasets/convective_envelope_rsg/","title":"Red Supergiant Convective Envelope","text":"<p>One line description of the data: 3D radiation hydrodynamic simulations of the convective envelope of red supergiant stars.</p> <p>Longer description of the data: Massive stars evolve into red supergiants, which have large radii and luminosities, and low-density, turbulent, convective envelopes. These simulations model the (inherently 3D) convective properties and gives insight into the progenitors of supernovae explosions.</p> <p>Associated paper: Paper.</p> <p>Domain experts: Yan-Fei Jiang (CCA, Flatiron Institute), Jared Goldberg (CCA, Flatiron Institute), Jeff Shen (Princeton University).</p> <p>Code or software used to generate the data: Athena++.</p> <p>Equations</p> \\[ \\begin{align*} \\frac{\\partial\\rho}{\\partial t}+\\mathbf{\\nabla}\\cdot(\\rho\\mathbf{v})&amp;=0\\\\ \\frac{\\partial(\\rho\\mathbf{v})}{\\partial t}+\\mathbf{\\nabla}\\cdot({\\rho\\mathbf{v}\\mathbf{v}+{{\\sf P_{\\rm gas}}}}) &amp;=-\\mathbf{G}_r-\\rho\\mathbf{\\nabla}\\Phi \\end{align*} \\] \\[ \\begin{align*} \\frac{\\partial{E}}{\\partial t}+\\mathbf{\\nabla}\\cdot\\left[(E+ P_{\\rm gas})\\mathbf{v}\\right] &amp;= -c G^0_r -\\rho\\mathbf{v}\\cdot\\mathbf{\\nabla}\\Phi \\\\ \\frac{\\partial I}{\\partial t}+c\\mathbf{n}\\cdot\\mathbf{\\nabla} I &amp;= S(I,\\mathbf{n}) \\end{align*} \\] <p>where</p> <ul> <li>\\(\\rho\\) = gas density.</li> <li>\\(\\mathbf{v}\\) = flow velocity.</li> <li>\\({\\sf P_{\\rm gas}}\\) = gas pressure tensor.</li> <li>\\(P_{\\rm gas}\\) = gas pressure scalar.</li> <li>\\(E\\) = total gas energy density: \\(E = E_g + \\rho v^2 / 2\\), where \\(E_g = 3 P_{\\rm gas} / 2\\) = gas internal energy density.</li> <li>\\(G^0_r\\) and \\(\\mathbf{G}_r\\) = time-like and space-like components of the radiation four-force.</li> <li>\\(I\\) = frequency integrated intensity, which is a function of time, spatial coordinate, and photon propagation direction \\(\\mathbf{n}\\).</li> <li>\\(\\mathbf{n}\\) = photon propagation direction.</li> </ul> <p></p> Dataset FNO TFNO Unet CNextU-net <code>convective_envelope_rsg</code> \\(\\mathbf{0.0269}\\) 0.0283 0.0555 0.0799 <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/convective_envelope_rsg/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 100 time-steps of 256 \\(\\times\\) 128 \\(\\times\\) 256 images per trajectory.</p> <p>Fields available in the data: energy (scalar field), density (scalar field), pressure (scalar field), velocity (vector field).</p> <p>Number of trajectories: 29 (they are cuts of one long trajectory, long trajectory available on demand).</p> <p>Estimated size of the ensemble of all simulations: 570 GB.</p> <p>Grid type: spherical coordinates, uniform in \\((\\log r, \\theta,\\phi)\\). Simulations are done for a portion of a sphere (not the whole sphere), so the simulation volume is like a spherical cake slice.</p> <p>Initial and boundary conditions: The temperature at the inner boundary (IB) is first set to equal that of the appropriate radius coordinate in the MESA (1D) model (\\(400\\~R_\\odot\\) and \\(300\\~R_\\odot\\)) and the density selected to approximately recover the initial total mass of the star in the simulation (\\(15.4\\~M_\\odot\\) and \\(14\\~M_\\odot\\)). Between \\(300\\~R_\\odot\\) and \\(400\\~R_\\odot\\), the initial profile is constructed with the radiative luminosity to be \\(10^5\\~L_\\odot\\), and this is kept fixed in the IB.</p> <p>Simulation time-step: ~2 days.</p> <p>Data are stored separated by (\\(\\Delta t\\)): units here are sort of arbitrary, \\(\\Delta t= 8\\).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 2\\), \\(t_{max} = 23402\\) (arbitrary).</p> <p>Spatial domain size: \\(R\\) from \\(300-6700~{\\rm R_\\odot}\\), \u03b8 from \\(\u03c0/4\u22123\u03c0/4\\) and \\(\\phi\\) from \\(0\u2212\u03c0\\), with \\(\u03b4r/r \u2248 0.01\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated:</p> Simulation radius of inner boundary \\(R_{IB}/R_\\odot\\) radius of outer boundary \\(R_{OB}/R_\\odot\\) heat source resolution (r \u00d7 \u03b8 \u00d7 \\(\\phi\\)) duration core mass \\(mc/M_\\odot\\) final mass \\(M_{\\rm final}/M_\\odot\\) Whole simulation (to obtain the 29 trajectories) 300 6700 fixed L 256 \u00d7 128 \u00d7 256 5766 days 10.79 12.9 <p>Approximate time to generate the data: 2 months on 80 nodes, or approximately 10 million CPU hours.</p> <p>Hardware used to generate the data: 80x NASA Pleiades Skylake CPU nodes.</p> <p>Additional information about the simulation: The radial extent of the simulation domain extends from \\(300~{\\rm R_\\odot}\\) at the simulation inner boundary to \\(6700~{\\rm R_\\odot}\\) at the simulation outer boundary, with logarithmic cell spacing in radius. The typical radius of the photosphere (or \"surface\") of the star is between \\(\\approx 800 - 1000 ~{\\rm R_\\odot}\\), fluctuating in space and time. Convection develops only at locations inside the star, within the first hundred radial zones or so. Some material from the star occasionally reaches larger radial distances. Outside of the stellar photosphere (\"surface\"), a density floor is set at $ \\approx 10^{-16} g/cm^3$, and the material far outside the stellar photosphere generally reflects the infalling motion of gas and density floor material with very little mass, perturbed by the activity of the stellar surface. Additionally, because the temperature and density is very low, the opacities are not well-characterized in this material. So, while the RHD equations are still solved in this region of the simulation domain, one should not interpret things outside \\(\\approx 1500 R_\\odot\\) as physically meaningful.</p>"},{"location":"datasets/convective_envelope_rsg/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are captured in the data: turbulence and convection (inherently 3D processes), variability. Note that the stellar surface only extends out to roughly 1000 \\(R_\\odot\\), inside of which the interesting physics occurs.</p> <p>How to evaluate a new simulator operating in this space: can it predict behaviour of simulation in convective steady-state, given only a few snapshots at the beginning of the simulation? can it properly model convection and turbulence?</p> <p>Caveats: complicated geometry, size of a slice in R varies with R (think of this as a slice of cake, where the parts of the slice closer to the outside have more area/volume than the inner parts), simulation reaches convective steady-state at some point and no longer \"evolves\".</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{goldberg2022numerical,\n  title={Numerical simulations of convective three-dimensional red supergiant envelopes},\n  author={Goldberg, Jared A and Jiang, Yan-Fei and Bildsten, Lars},\n  journal={The Astrophysical Journal},\n  volume={929},\n  number={2},\n  pages={156},\n  year={2022},\n  publisher={IOP Publishing}\n}\n</code></pre>"},{"location":"datasets/dummy_placeholder/","title":"Dummy Dataset","text":"<p>This is a place holder for dummy datasets for testing and development purposes.</p>"},{"location":"datasets/euler_multi_quadrants_openBC/","title":"Euler Multi-quadrants - Riemann problems (compressible, inviscid fluid)","text":"<p>NOTE: this dataset is distributed in two separate datasets: <code>euler_multi_quadrants_openBC</code> with open boundary conditions and <code>euler_multi_quadrants_periodicBC</code> with periodic boundary conditions.</p> <p>One line description of the data:  Evolution of different gases starting with piecewise constant initial data in quadrants.</p> <p>Longer description of the data:  The evolution can give rise to shocks, rarefaction waves, contact discontinuities, interaction with each other and domain walls.</p> <p>Associated paper: Paper.</p> <p>Domain experts: Marsha Berger(Flatiron Institute &amp; NYU), Ruben Ohana (CCM, Flatiron Institute &amp; Polymathic AI), Michael McCabe (Polymathic AI).</p> <p>Code or software used to generate the data: Clawpack (AMRClaw).</p> <p>Equation: Euler equations for a compressible gas:</p> \\[ \\begin{align*} U_t + F(U)_x + G(U)_y &amp;= 0 \\\\ \\textrm{where} \\quad U = \\begin{bmatrix} \\rho \\\\ \\rho u \\\\ \\rho v \\\\ e \\end{bmatrix}, \\quad F(U) = \\begin{bmatrix} \\rho u \\\\ \\rho u^2 + p \\\\ \\rho u v \\\\ u(e + p) \\end{bmatrix},&amp; \\quad G(U) = \\begin{bmatrix} \\rho v \\\\ \\rho u v \\\\ \\rho v^2 + p \\\\ v(e + p) \\end{bmatrix}, \\quad \\\\ e = \\frac{p}{(\\gamma - 1)} + \\frac{\\rho (u^2 + v^2)}{2}&amp;, \\quad p = A\\rho^{\\gamma}. \\end{align*} \\] <p>with \\(\\rho\\) the density, \\(u\\) and \\(v\\) the \\(x\\) and \\(y\\) velocity components, \\(e\\) the energy, \\(p\\) the pressure, \\(\\gamma\\) the gas constant, and \\(A&gt;0\\) is a function of entropy.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>euler_multi-quadrants_periodicBC</code> 0.4081 0.4163 0.1834 \\(\\mathbf{0.1531}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/euler_multi_quadrants_openBC/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 100 timesteps of 512x512 images.</p> <p>Fields available in the data: density (scalar field), energy (scalar field), pressure (scalar field), momentum (vector field).</p> <p>Number of trajectories: 500 per set of parameters, 10 000 in total.</p> <p>Estimated size of the ensemble of all simulations: 5.17 TB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Randomly generated initial quadrants.</p> <p>Boundary conditions: Periodic or open.</p> <p>Simulation time-step: variable.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 0.015s (1.5s for 100 timesteps).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max}=1.5s\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\)): \\(L_x = 1\\) and  \\(L_y = 1\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: all combinations of \\(\\gamma\\) constant of the gas at a certain temperature: \\(\\gamma=\\){1.13,1.22,1.3,1.33,1.365,1.4,1.404,1.453,1.597,1.76} and boundary conditions: {extrap, periodic}.</p> <p>Approximate time to generate the data: 80 hours on 160 CPU cores for all data (periodic and open BC).</p> <p>Hardware used to generate the data and precision used for generating the data: Icelake nodes, double precision.</p>"},{"location":"datasets/euler_multi_quadrants_openBC/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: capture the shock formations and interactions. Multiscale shocks.</p> <p>How to evaluate a new simulator operating in this space: the new simulator should predict the shock at the right location and time, and the right shock strength, as compared to a pressure gauge monitoring the exact solution.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{mandli2016clawpack,\n  title={Clawpack: building an open source ecosystem for solving hyperbolic PDEs},\n  author={Mandli, Kyle T and Ahmadia, Aron J and Berger, Marsha and Calhoun, Donna and George, David L and Hadjimichael, Yiannis and Ketcheson, David I and Lemoine, Grady I and LeVeque, Randall J},\n  journal={PeerJ Computer Science},\n  volume={2},\n  pages={e68},\n  year={2016},\n  publisher={PeerJ Inc.}\n}\n</code></pre>"},{"location":"datasets/euler_multi_quadrants_periodicBC/","title":"Euler Multi-quadrants - Riemann problems (compressible, inviscid fluid)","text":"<p>NOTE: this dataset is distributed in two separate datasets: <code>euler_multi_quadrants_openBC</code> with open boundary conditions and <code>euler_multi_quadrants_periodicBC</code> with periodic boundary conditions.</p> <p>One line description of the data:  Evolution of different gases starting with piecewise constant initial data in quadrants.</p> <p>Longer description of the data:  The evolution can give rise to shocks, rarefaction waves, contact discontinuities, interaction with each other and domain walls.</p> <p>Associated paper: Paper.</p> <p>Domain experts: Marsha Berger(Flatiron Institute &amp; NYU), Ruben Ohana (CCM, Flatiron Institute &amp; Polymathic AI), Michael McCabe (Polymathic AI).</p> <p>Code or software used to generate the data: Clawpack (AMRClaw).</p> <p>Equation: Euler equations for a compressible gas:</p> \\[ \\begin{align*} U_t + F(U)_x + G(U)_y &amp;= 0 \\\\ \\textrm{where} \\quad U = \\begin{bmatrix} \\rho \\\\ \\rho u \\\\ \\rho v \\\\ e \\end{bmatrix}, \\quad F(U) = \\begin{bmatrix} \\rho u \\\\ \\rho u^2 + p \\\\ \\rho u v \\\\ u(e + p) \\end{bmatrix},&amp; \\quad G(U) = \\begin{bmatrix} \\rho v \\\\ \\rho u v \\\\ \\rho v^2 + p \\\\ v(e + p) \\end{bmatrix}, \\quad \\\\ e = \\frac{p}{(\\gamma - 1)} + \\frac{\\rho (u^2 + v^2)}{2}&amp;, \\quad p = A\\rho^{\\gamma}. \\end{align*} \\] <p>with \\(\\rho\\) the density, \\(u\\) and \\(v\\) the \\(x\\) and \\(y\\) velocity components, \\(e\\) the energy, \\(p\\) the pressure, \\(\\gamma\\) the gas constant, and \\(A&gt;0\\) is a function of entropy.</p> <p></p>"},{"location":"datasets/euler_multi_quadrants_periodicBC/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 100 timesteps of 512x512 images.</p> <p>Fields available in the data: density (scalar field), energy (scalar field), pressure (scalar field), momentum (vector field).</p> <p>Number of trajectories: 500 per set of parameters, 10 000 in total.</p> <p>Estimated size of the ensemble of all simulations: 5.17 TB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Randomly generated initial quadrants.</p> <p>Boundary conditions: Periodic or open.</p> <p>Simulation time-step: variable.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 0.015s (1.5s for 100 timesteps).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max}=1.5s\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\)): \\(L_x = 1\\) and  \\(L_y = 1\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: all combinations of \\(\\gamma\\) constant of the gas at a certain temperature: \\(\\gamma=\\){1.13,1.22,1.3,1.33,1.365,1.4,1.404,1.453,1.597,1.76} and boundary conditions: {extrap, periodic}.</p> <p>Approximate time to generate the data: 80 hours on 160 CPU cores for all data (periodic and open BC).</p> <p>Hardware used to generate the data and precision used for generating the data: Icelake nodes, double precision.</p>"},{"location":"datasets/euler_multi_quadrants_periodicBC/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: capture the shock formations and interactions. Multiscale shocks.</p> <p>How to evaluate a new simulator operating in this space: the new simulator should predict the shock at the right location and time, and the right shock strength, as compared to a pressure gauge monitoring the exact solution.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{mandli2016clawpack,\n  title={Clawpack: building an open source ecosystem for solving hyperbolic PDEs},\n  author={Mandli, Kyle T and Ahmadia, Aron J and Berger, Marsha and Calhoun, Donna and George, David L and Hadjimichael, Yiannis and Ketcheson, David I and Lemoine, Grady I and LeVeque, Randall J},\n  journal={PeerJ Computer Science},\n  volume={2},\n  pages={e68},\n  year={2016},\n  publisher={PeerJ Inc.}\n}\n</code></pre>"},{"location":"datasets/gray_scott_reaction_diffusion/","title":"Pattern formation in the Gray-Scott reaction-diffusion equations","text":"<p>One line description of the data: Stable Turing patterns emerge from randomness, with drastic qualitative differences in pattern dynamics depending on the equation parameters.</p> <p>Longer description of the data: The Gray-Scott equations are a set of coupled reaction-diffusion equations describing two chemical species, \\(A\\) and \\(B\\), whose concentrations vary in space and time. The two parameters \\(f\\) and \\(k\\) control the \u201cfeed\u201d and \u201ckill\u201d rates in the reaction. A zoo of qualitatively different static and dynamic patterns in the solutions are possible depending on these two parameters. There is a rich landscape of pattern formation hidden in these equations.</p> <p>Associated paper: None.</p> <p>Domain expert: Daniel Fortunato, CCM and CCB, Flatiron Institute.</p> <p>Code or software used to generate the data: Github repository (MATLAB R2023a, using the stiff PDE integrator implemented in Chebfun. The Fourier spectral method is used in space (with nonlinear terms evaluated pseudospectrally), and the exponential time-differencing fourth-order Runge-Kutta scheme (ETDRK4) is used in time.)</p> <p>Equation describing the data</p> \\[ \\begin{align*} \\frac{\\partial A}{\\partial t} &amp;= \\delta_A\\Delta A - AB^2 + f(1-A) \\\\ \\frac{\\partial B}{\\partial t} &amp;= \\delta_B\\Delta B - AB^2 - (f+k)B \\end{align*} \\] <p>The dimensionless parameters describing the behavior are: \\(f\\) the rate at which \\(A\\) is replenished (feed rate), \\(k\\) the rate at which \\(B\\) is removed from the system, and  \\(\\delta_A, \\delta_B\\) the diffusion coefficients of both species.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>gray_scott_reaction_diffusion</code> \\(\\mathbf{0.1365}\\) 0.3633 0.2252 \\(\\mathbf{0.1761}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/gray_scott_reaction_diffusion/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 1001 time-steps of 128 \\(\\times\\) 128 images.</p> <p>Fields available in the data: The concentration of two chemical species \\(A\\) and \\(B\\).</p> <p>Number of trajectories: 6 sets of parameters, 200 initial conditions per set = 1200.</p> <p>Estimated size of the ensemble of all simulations: 153.8 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Two types of initial conditions generated: random Fourier series and random clusters of Gaussians.</p> <p>Boundary conditions: periodic.</p> <p>Simulation time-step: 1 second.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 10 seconds.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} =0\\), \\(t_{max} = 10,000\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\)): \\([-1,1]\\times[-1,1]\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: All simulations used \\(\\delta_u = 2.10^{-5}\\) and \\(\\delta_v = 1.10^{-5}\\). \"Gliders\": \\(f = 0.014, k = 0.054\\). \"Bubbles\": \\(f = 0.098, k =0.057\\). \"Maze\": \\(f= 0.029, k = 0.057\\). \"Worms\": \\(f= 0.058, k = 0.065\\). \"Spirals\": \\(f=0.018, k = 0.051\\). \"Spots\": \\(f= 0.03, k=0.062\\).</p> <p>Approximate time to generate the data: 5.5 hours per set of parameters, 33 hours total.</p> <p>Hardware used to generate the data: 40 CPU cores.</p>"},{"location":"datasets/gray_scott_reaction_diffusion/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: Pattern formation: by sweeping the two parameters \\(f\\) and \\(k\\), a multitude of steady and dynamic patterns can form from random initial conditions.</p> <p>How to evaluate a new simulator operating in this space: It would be impressive if a simulator\u2014trained only on some of the patterns produced by a subset of the \\((f, k)\\) parameter space\u2014could perform well on an unseen set of parameter values \\((f, k)\\) that produce fundamentally different patterns. Stability for steady-state patterns over long rollout times would also be impressive.</p> <p>Warning: Due to the nature of the problem and the possibility to reach an equilibrium for certain values of the kill and feed parameters, a constant stationary behavior can be reached. Here are the trajectories for which a stationary behavior was identified for specy \\(A\\) as well as the corresponding time at which it was reached: - Validation set:     - \\(f=0.014, k=0.054\\) :         - Trajectory 7, time = 123         - Trajectory 8, time = 125         - Trajectory 10, time = 123         - Trajectory 11, time = 125         - Trajectory 12, time = 121         - Trajectory 14, time = 121         - Trajectory 15, time = 129         - Trajectory 16, time = 124         - Trajectory 17, time = 122         - Trajectory 18, time = 121         - Trajectory 19, time = 155     - \\(f=0.018, k=0.051\\) :         - Trajectory 14, time = 109</p> <ul> <li> <p>Training set:</p> <ul> <li>\\(f=0.014,k=0.054\\) :<ul> <li>Trajectory 81, time = 126</li> <li>Trajectory 82, time = 126</li> <li>Trajectory 83, time = 123</li> <li>Trajectory 85, time = 123</li> <li>Trajectory 86, time = 124</li> <li>Trajectory 87, time = 127</li> <li>Trajectory 88, time = 121</li> <li>Trajectory 90, time = 123</li> <li>Trajectory 91, time = 121</li> <li>Trajectory 92, time = 126</li> <li>Trajectory 93, time = 121</li> <li>Trajectory 94, time = 126</li> <li>Trajectory 95, time = 125</li> <li>Trajectory 96, time = 123</li> <li>Trajectory 97, time = 126</li> <li>Trajectory 98, time = 121</li> <li>Trajectory 99, time = 125</li> <li>Trajectory 100, time = 126</li> <li>Trajectory 101, time = 125</li> <li>Trajectory 102, time = 159</li> <li>Trajectory 103, time = 129</li> <li>Trajectory 105, time = 125</li> <li>Trajectory 107, time = 122</li> <li>Trajectory 108, time = 126</li> <li>Trajectory 110, time = 127</li> <li>Trajectory 111, time = 122</li> <li>Trajectory 112, time = 121</li> <li>Trajectory 113, time = 122</li> <li>Trajectory 114, time = 126</li> <li>Trajectory 115, time = 126</li> <li>Trajectory 116, time = 126</li> <li>Trajectory 117, time = 122</li> <li>Trajectory 118, time = 123</li> <li>Trajectory 119, time = 123</li> <li>Trajectory 120, time = 125</li> <li>Trajectory 121, time = 126</li> <li>Trajectory 122, time = 121</li> <li>Trajectory 123, time = 122</li> <li>Trajectory 125, time = 125</li> <li>Trajectory 126, time = 127</li> <li>Trajectory 127, time = 125</li> <li>Trajectory 129, time = 125</li> <li>Trajectory 130, time = 122</li> <li>Trajectory 131, time = 125</li> <li>Trajectory 132, time = 131</li> <li>Trajectory 133, time = 126</li> <li>Trajectory 134, time = 159</li> <li>Trajectory 135, time = 121</li> <li>Trajectory 136, time = 126</li> <li>Trajectory 137, time = 125</li> <li>Trajectory 138, time = 126</li> <li>Trajectory 139, time = 123</li> <li>Trajectory 140, time = 128</li> <li>Trajectory 141, time = 126</li> <li>Trajectory 142, time = 123</li> <li>Trajectory 144, time = 122</li> <li>Trajectory 145, time = 125</li> <li>Trajectory 146, time = 123</li> <li>Trajectory 147, time = 126</li> <li>Trajectory 148, time = 121</li> <li>Trajectory 149, time = 122</li> <li>Trajectory 150, time = 125</li> <li>Trajectory 151, time = 126</li> <li>Trajectory 152, time = 152</li> <li>Trajectory 153, time = 127</li> <li>Trajectory 154, time = 122</li> <li>Trajectory 155, time = 124</li> <li>Trajectory 156, time = 122</li> <li>Trajectory 158, time = 126</li> <li>Trajectory 159, time = 121</li> </ul> </li> <li>\\(f=0.018,k=0.051\\):<ul> <li>Trajectory 97, time = 109</li> <li>Trajectory 134, time = 107</li> <li>Trajectory 147, time = 109</li> <li>Trajectory 153, time = 112</li> </ul> </li> </ul> </li> <li> <p>Test set:</p> <ul> <li>\\(f=0.014,k=0.054\\):<ul> <li>Trajectory 12, time = 127</li> <li>Trajectory 13, time = 125</li> <li>Trajectory 14, time = 123</li> <li>Trajectory 15, time = 126</li> <li>Trajectory 16, time = 126</li> <li>Trajectory 17, time = 123</li> <li>Trajectory 18, time = 128</li> <li>Trajectory 19, time = 125</li> </ul> </li> <li>\\(f=0.018,k=0.051\\):<ul> <li>Trajectory 11, time = 113</li> </ul> </li> </ul> </li> </ul>"},{"location":"datasets/helmholtz_staircase/","title":"Helmholtz equation on a 2D staircase","text":"<p>One line description of the data: First high-order accurate solution of acoustic scattering from a nonperiodic source by a periodic surface, relevant for its use in waveguide applications (antennae, diffraction from gratings, photonic/phononic crystals, noise cancellation, seismic filtering, etc.).</p> <p>Longer description of the data:  Accurate solution of PDEs near infinite, periodic boundaries poses a numerical challenge due these surfaces serving as waveguides, allowing modes to propagate for long distances from the source. This property makes numerical truncation of the (infinite) solution domain unfeasible, as it would induce large artificial reflections and therefore errors. Periodization (reducing the computational domain to one unit cell) is only possible if the incident wave is also periodic, such as plane waves, but not for nonperiodic sources, e.g. a point source. Computing a high-order accurate scattering solution from a point source, however, would be of scientific interest as it models applications such as remote sensing, diffraction from gratings, antennae, or acoustic/photonic metamaterials. We use a combination of the Floquet\u2014Bloch transform (also known as array scanning method) and boundary integral equation methods to alleviate these challenges and recover the scattered solution as an integral over a family of quasiperiodic solutions parameterized by their on-surface wavenumber. The advantage of this approach is that each of the quasiperiodic solutions may be computed quickly by periodization, and accurately via high-order quadrature.</p> <p>Associated paper: Paper.</p> <p>Domain expert: Fruzsina Julia Agocs, Center for Computational Mathematics, Flatiron Institute \\&amp; University of Colorado, Boulder.</p> <p>Code or software used to generate the data: Github repository.</p> <p>Equations:</p> <p>While we solve equations in the frequency domain, the original time-domain problem is:</p> \\[ \\frac{\\partial^2 U(t, \\mathbf{x})}{\\partial t^2} - \\Delta U(t, \\mathbf{x}) = \\delta(t)\\delta(\\mathbf{x} - \\mathbf{x}_0), \\] <p>where \\(\\Delta = \\nabla \\cdot \\nabla\\) is the spatial Laplacian and \\(U\\) the accoustic pressure. The sound-hard boundary \\(\\partial \\Omega\\) imposes Neumann boundary conditions,</p> \\[ U_n(t, \\mathbf{x}) = \\mathbf{n} \\cdot \\nabla U = 0, \\quad t \\in \\mathbb{R}, \\quad \\mathbf{x} \\in \\partial \\Omega. \\] <p>Upon taking the temporal Fourier transform, we get the inhomogeneous Helmholtz Neumann boundary value problem</p> \\[ \\begin{align*} -(\\Delta + \\omega^2)u &amp;= \\delta_{\\mathbf{x}_0}, \\quad \\text{in } \\Omega,\\\\ u_n &amp;= 0 \\quad \\text{on } \\partial \\Omega, \\end{align*} \\] <p>with outwards radiation conditions as described in [1]. The region \\(\\Omega\\) lies above a corrugated boundary \\(\\partial \\Omega\\), extending with spatial period \\(d\\) in the \\(x_1\\) direction, and is unbounded in the positive \\(x_2\\) direction. The current example is a right-angled staircase whose unit cell consists of two equal-length line segments at \\(\\pi/2\\) angle to each other.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>helmholtz_staircase</code> \\(\\textbf{0.00046}\\) 0.00346 0.01931 0.02758 <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/helmholtz_staircase/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 50 time-steps of 1024 \\(\\times\\) 256 images.</p> <p>Fields available in the data: real and imaginary part of accoustic pressure (scalar field), the staircase mask (scalar field, stationary).</p> <p>Number of trajectories: \\(512\\) (combinations of \\(16\\) input parameter \\(\\omega\\) and \\(32\\) source positions \\(\\mathbf{x}_0\\)).</p> <p>Size of the ensemble of all simulations: 52.4 GB.</p> <p>Grid type: uniform.</p> <p>Initial conditions: The time-dependence is analytic in this case: \\(U(t, \\mathbf{x}) = u(\\mathbf{x})e^{-i\\omega t}.\\) Therefore any spatial solution may serve as an initial condition.</p> <p>Boundary conditions: Neumann conditions (normal derivative of the pressure \\(u\\) vanishes, with the normal defined as pointing up from the boundary) are enforced at the boundary.</p> <p>Simulation time-step: continuous in time (time-dependence is analytic).</p> <p>Data are stored separated by (\\(\\Delta t\\)): \\(\\Delta t =\\frac{2\\pi}{\\omega N}\\), with \\(N = 50\\).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{\\mathrm{min}} = 0\\), \\(t_{\\mathrm{max}} = \\frac{2\\pi}{\\omega}\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): \\(-8.0 \\leq x_1 \\leq 8.0\\) horizontally, and \\(-0.5 \\geq x_2 \\geq 3.5\\) vertically.</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(\\omega\\)={0.06283032, 0.25123038, 0.43929689, 0.62675846, 0.81330465, 0.99856671, 1.18207893, 1.36324313, 1.5412579, 1.71501267, 1.88295798, 2.04282969, 2.19133479, 2.32367294, 2.4331094,  2.5110908}, with the sources coordinates being all combinations of \\(x\\)={-0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4} and \\(y\\)={-0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4}.</p> <p>Approximate time to generate the data: per input parameter: \\(\\sim 400s\\), total: \\(\\sim 50\\) hours.</p> <p>Hardware used to generate the data: 64 CPU cores.</p>"},{"location":"datasets/helmholtz_staircase/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are captured in the data: The simulations capture the existence of trapped acoustic waves \u2013 modes that are guided along the corrugated surface. They also show that the on-surface wavenumber of trapped modes is different than the frequency of the incident radiation, i.e. they capture the trapped modes\u2019 dispersion relation.</p> <p>How to evaluate a new simulator operating in this space: The (spatial) accuracy of a new simulator/method could be checked by requiring that it conserves flux \u2013 whatever the source injects into the system also needs to come out. The trapped modes\u2019 dispersion relation may be another metric, my method generates this to 7-8 digits of accuracy at the moment, but 10-12 digits may also be obtained. The time-dependence learnt by a machine learning algorithm can be compared to the analytic solution \\(e^{-i\\omega t}\\), this can be used to evaluate temporal accuracy.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{agocs2023trapped,\n  title={Trapped acoustic waves and raindrops: high-order accurate integral equation method for localized excitation of a periodic staircase},\n  author={Agocs, Fruzsina J and Barnett, Alex H},\n  journal={arXiv preprint arXiv:2310.12486},\n  year={2023}\n}\n</code></pre>"},{"location":"datasets/planetswe/","title":"PlanetSWE","text":"<p>One line description of the data: Forced hyperviscous rotating shallow water on a sphere with earth-like topography and daily/annual periodic forcings.</p> <p>Longer description of the data: The shallow water equations are fundamentally a 2D approximation of a 3D flow in the case where horizontal length scales are significantly longer than vertical length scales. They are derived from depth-integrating the incompressible Navier-Stokes equations. The integrated dimension then only remains in the equation as a variable describing the height of the pressure surface above the flow. These equations have long been used as a simpler approximation of the primitive equations in atmospheric modeling of a single pressure level, most famously in the Williamson test problems. This scenario can be seen as similar to Williamson Problem 7 as we derive initial conditions from the hPa 500 pressure level in ERA5. These are then simulated with realistic topography and two levels of periodicity.</p> <p>Associated paper: Paper.</p> <p>Domain expert: Michael McCabe, Polymathic AI.</p> <p>Code or software used to generate the data: Dedalus, adapted from this example.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{ \\partial \\vec{u}}{\\partial t} &amp;= - \\vec{u} \\cdot \\nabla u - g \\nabla h - \\nu \\nabla^4 \\vec{u} - 2\\Omega \\times \\vec{u} \\\\ \\frac{ \\partial h }{\\partial t} &amp;= -H \\nabla \\cdot \\vec{u} - \\nabla \\cdot (h\\vec{u}) - \\nu \\nabla^4h + F \\end{align*} \\] <p>with \\(h\\) the deviation of pressure surface height from the mean, \\(H\\) the mean height, \\(\\vec{u}\\) the 2D velocity, \\(\\Omega\\) the Coriolis parameter, and F the forcing which is defined:</p> <pre><code>def find_center(t):\n    time_of_day = t / day\n    time_of_year = t / year\n    max_declination = .4 # Truncated from estimate of earth's solar decline\n    lon_center = time_of_day*2*np.pi # Rescale sin to 0-1 then scale to np.pi\n    lat_center = np.sin(time_of_year*2*np.pi)*max_declination\n    lon_anti = np.pi + lon_center  #2*np.((np.sin(-time_of_day*2*np.pi)+1) / 2)*pi\n    return lon_center, lat_center, lon_anti, lat_center\n\ndef season_day_forcing(phi, theta, t, h_f0):\n    phi_c, theta_c, phi_a, theta_a = find_center(t)\n    sigma = np.pi/2\n    coefficients = np.cos(phi - phi_c) * np.exp(-(theta-theta_c)**2 / sigma**2)\n    forcing = h_f0 * coefficients\n    return forcing\n</code></pre> <p>Visualization:</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>planetswe</code> 0.1727 \\(\\mathbf{0.0853}\\) 0.3620 0.3724 <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/planetswe/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 3024 timesteps of 256x512 images with \"day\" defined as 24 steps and \"year\" defined as 1008 in model time.</p> <p>Fields available in the data: height (scalar field), velocity (vector field).</p> <p>Number of trajectories: 40 trajectories of 3 model years.</p> <p>Estimated size of the ensemble of all simulations: 185.8 GB.</p> <p>Grid type: Equiangular grid, polar coordinates.</p> <p>Initial conditions: Sampled from hPa 500 level of ERA5, filtered for stable initialization and burned-in for half a simulation year.</p> <p>Boundary conditions: Spherical.</p> <p>Simulation time-step (\\(\\Delta t\\)): CFL-based step size with safety factor of 0.4.</p> <p>Data are stored separated by (\\(\\delta t\\)): 1 hour in simulation time units.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max} = 3024\\).</p> <p>Spatial domain size: \\(\\phi \\in [0, 2 \\pi]\\), \\(\\theta \\in [0, \\pi]\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(\\nu\\) normalized to mode 224.</p> <p>Approximate time to generate the data: 45 minutes using 64 icelake cores for one simulation.</p> <p>Hardware used to generate the data: 64 Icelake CPU cores.</p>"},{"location":"datasets/planetswe/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>Spherical geometry and planet-like topography and forcing make for a proxy for real-world atmospheric dynamics where true dynamics are known. The dataset has annual and daily periodicity forcing models to either process a sufficient context length to learn these patterns or to be explicitly time aware. Furthermore, the system becomes stable making this a good system for exploring long run stability of models.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{mccabe2023towards,\n  title={Towards stability of autoregressive neural operators},\n  author={McCabe, Michael and Harrington, Peter and Subramanian, Shashank and Brown, Jed},\n  journal={arXiv preprint arXiv:2306.10619},\n  year={2023}\n}\n</code></pre>"},{"location":"datasets/post_neutron_star_merger/","title":"Post neutron star merger","text":"<p>One line description of the data: Simulations of the aftermath of a neutron star merger.</p> <p>Longer description of the data: The simulations presented here are axisymmetrized snapshots of full three-dimensional general relativistic neutrino radiation magnetohydrodynamics. The plasma physics is treated with finite volumes with constrained transport for the magnetic field on a curvilinear grid. The system is closed by a tabulated nuclear equation of state assuming nuclear statistical equilibrium (NSE). The radiation field is treated via Monte Carlo transport, which is a particle method. The particles are not included in this dataset, however their effects are visible as source terms on the fluid.</p> <p>Associated paper: The simulations included here are from a series of papers: Paper 1, Paper 2, Paper 3, Paper 4, Paper 5.</p> <p>Domain expert: Jonah Miller, Los Alamos National Laboratory.</p> <p>Code or software used to generate the data: Open source software nublight.</p> <p>Equation: See equations 1-5 and 16 of Miller, Ryan, Dolence (2019).</p> <p>The fluid sector consists of the following system of equations.</p> \\[ \\begin{align*}   \\partial_t \\left(\\sqrt{g}\\rho_0 u^t\\right) + \\partial_i\\left(\\sqrt{g}\\rho_0u^i\\right) &amp;= 0 \\\\   \\partial_t\\left[\\sqrt{g} \\left(T^t_{\\ \\nu} + \\rho_0u^t \\delta^t_\\nu\\right)\\right] + \\partial_i\\left[\\sqrt{g}\\left(T^i_{\\ \\nu} + \\rho_0 u^i \\delta^t_\\nu\\right)\\right] &amp;= \\sqrt{g} \\left(T^\\kappa_{\\ \\lambda} \\Gamma^\\lambda_{\\nu\\kappa} + G_\\nu\\right)\\,\\,\\,\\, \\forall \\nu = 0,1,\\ldots,4 \\end{align*} \\] \\[ \\begin{align*}   \\partial_t \\left(\\sqrt{g} B^i\\right) + \\partial_j \\left[\\sqrt{g}\\left(b^ju^i - b^i u^j\\right)\\right] &amp;= 0 \\\\   \\partial_t\\left(\\sqrt{g}\\rho_0 Y_e u^t\\right) + \\partial_i\\left(\\sqrt{g}\\rho_0Y_eu^i\\right) &amp;= \\sqrt{g} G_{\\text{ye}} \\end{align*} \\] <p>The standard radiative transfer equation is</p> \\[ \\frac{D}{d\\lambda}\\left(\\frac{h^3\\mathcal{I}_{\\nu,f}}{\\varepsilon^3}\\right) = \\left(\\frac{h^2\\eta _{\\nu,f}}{\\varepsilon^2}\\right) - \\left(\\frac{\\varepsilon \\chi _{\\nu,f}}{h}\\right) \\left(\\frac{h^3\\mathcal{I} _{\\nu,f}}{\\varepsilon^3}\\right) \\] <p> </p> Dataset FNO TFNO Unet CNextU-net <code>post_neutron_star_merger</code> 0.3866 \\(\\mathbf{0.3793}\\) - - <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1. Unet and CNextU-net results are not available as these architectures require all the dimensions of the data to be multiples of 2.</p>"},{"location":"datasets/post_neutron_star_merger/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 181 time-steps of 192 \\(\\times\\) 128 \\(\\times\\) 66 snapshots.</p> <p>Fields available in the data: fluid density (scalar field), fluid internal energy (scalar field), electron fraction (scalar field), temperate (scalar field), entropy (scalar field), velocity (vector field), magnetic field (vector field), contravariant tensor metric of space-time (tensor field, no time-dependency). A description of fields available in an output file can be found here.</p> <p>Number of trajectories: 8 full simulations.</p> <p>Size of the ensemble of all simulations: 110.1 GB.</p> <p>Grid type: Uniform grid, log-spherical coordinates.</p> <p>Initial conditions: Constant entropy torus in hydrostatic equilibrium orbiting a black hole. Black hole mass and spin, as well as torus mass, spin, electron fraction, and entropy vary.</p> <p>Boundary conditions: open.</p> <p>Simulation time-step: approximately 0.01 in code units. Physical time varies; roughly 147 nanoseconds for fiducial model.</p> <p>Data are stored separated by (\\(\\Delta t\\)): 50 in code units. Physical time varies; roughly 0.6 milliseconds for fiducial model.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): 10000 in code units. Physical time varies; roughly 127 milliseocnds for fudicial model</p> <p>Spatial domain size: Spherical coordinates. Radius roughly 2 to 1000 in code units. Physical values vary. Outer boundary is at roughly 4000 for fiducial model. Polar angle 0 to pi. Azimuthal angle 0 to 2*pi. Note that the coordinates are curvilinear. In Cartesian space, spacing is logarithmic in radius and there is a focusing of grid lines near the equator.</p> <p>Set of coefficients or non-dimensional parameters evaluated: Black hole spin parameter a, ranges 0 to 1. Initial mass and angular momentum of torus. In dimensionless units, evaluated as inner radius Rin and radius of maximum pressure Rmax. Torus initial electron fraction Ye and entropy kb. Black hole mass in solar masses.</p> <p>Approximate time to generate the data: Roughly 3 weeks per simulation on 300 cores.</p> <p>Hardware used to generate the data and precision used for generating the data: Data generated at double precision on several different supercomputers. All calculations were CPU calculations parallelized with a hybrid MPI + OpenMP strategy. 1 MPI rank per socket. Oldest calculations performed on the Los Alamos Badger cluster, now decommissioned. Intel Xeon E5-2695v5 2.1 GHz. 12 cores per socket, 24 core cores per node. Simulations run on 33 nodes. Some newer simulations run on Los Alamos Capulin cluster, now decomissioned. ARM ThunderX2 nodes. 56 cores per node. Simulation run on 33 nodes.</p>"},{"location":"datasets/post_neutron_star_merger/#simulation-index","title":"Simulation Index","text":"Scenario Shorthand name Description 0 collapsar_hi Disk resulting from collapse of massive rapidly rotating star. 1 torus_b10 Disk inspired by 2017 observation of a neutron star merger. Highest magnetic field strength. 2 torus_b30 Disk inspired by 2017 observation of a neutron star merger. Intermediate magnetic field strength. 3 torus_gw170817 Disk inspired by 2017 observation of a neutron star merger. Weakest magnetic field strength. 4 torus_MBH_10 Disk from black hole-neutron star merger. 10 solar mass black hole. 5 torus_MBH_2p31 Disk from black hole-neutron star merger. 2.31 solar mass black hole. 6 torus_MBH_2p67 Disk from black hole-neutron star merger. 2.76 solar mass black hole. 7 torus_MBH_2p69 Disk from black hole-neutron star merger. 2.79 solar mass black hole. 8 torus_MBH_6 Disk from black hole-neutron star merger. 6 solar mass black hole."},{"location":"datasets/post_neutron_star_merger/#general-relativistic-quantities","title":"General relativistic quantities","text":"<p>The core quantity that describes the curvature of spacetime and its impact on a simulation is <code>['t0_fields']['gcon']</code> of the HDF5 file. From this, other quantities can be computed.</p>"},{"location":"datasets/post_neutron_star_merger/#to-reproduce","title":"To reproduce","text":"<p>The values in <code>simulation_parameters.json</code> are sufficient to reproduce a simulation using nubhlight using the <code>torus_cbc</code> problem generator, with one exception. You must provide tabulated equation of state and opacity data. We use the SFHo equation of state provided on the stellar collapse website. Tabulated neutrino opacities were originally computed for the Fornax code and are not public. However adequate open source substitutes may be generated by the nulib library.</p>"},{"location":"datasets/post_neutron_star_merger/#explanation-of-simulation-parameters","title":"Explanation of simulation parameters","text":"<p>Here we include, for completeness, a description of the different simulation parameters. which cover the simulation parameters chosen. Their value for each simulation is stored in <code>simulation_parameters.json</code>.</p> <ul> <li><code>B_unit</code>, the unit of magnetic field strength. Multiplying code quantity by <code>B_unit</code> converts the quantity to units of Gauss.</li> <li><code>DTd</code>, dump time cadence.</li> <li><code>DTl</code>, log output time cadence.</li> <li><code>DTp</code>, permanent restart file time cadence.</li> <li><code>DTr</code>, temporary restart file time cadence.</li> <li><code>Ledd</code>, (Photon) Eddington luminosity based on black hole mass.</li> <li><code>L_unit</code>, length unit. Multiplying code quantity by <code>L_unit</code> converts it into units of cm.</li> <li><code>M_unit</code>, mass unit. Multiplying code quantity by <code>M_unit</code> converts it into units of g.</li> <li><code>Mbh</code>, black hole mass in units of g.</li> <li><code>MdotEdd</code>, (Photon) Eddington accretion rate based on black hole mass.</li> <li><code>N1</code>, number of grid points in X1 (radial) direction.</li> <li><code>N2</code>, number of grid points in X2 (polar) direction.</li> <li><code>N3</code>, number of grid points in X3 (azimuthal) direction.</li> <li><code>PATH</code>, output directory for the original simulation.</li> <li><code>RHO_unit</code>, density unit. Multiplying code quantity by <code>RHO_unit</code> converts it into units of g/cm^3.</li> <li><code>Reh</code>, radius of the event horizon in code units.</li> <li><code>Rin</code>, radius of the inner boundary in code units.</li> <li><code>Risco</code>, radius of the innermost stable circular orbit in code units.</li> <li><code>Rout_rad</code>, outer radius of neutrino transport.</li> <li><code>Rout_vis</code>, radius used for 3D volume rendering.</li> <li><code>TEMP_unit</code>, temperature unit. Converts from MeV (code units) to Kelvin.</li> <li><code>T_unit</code>, time unit. Converts from code units to seconds.</li> <li><code>U_unit</code>, energy density unit. Multiplying code quantity by <code>U_unit</code> converts it into units of erg/cm^3.</li> <li><code>a</code>, dimensionless black hole spin.</li> <li><code>cour</code>, dimensionless CFL factor used to set the timestep based on the grid spacing.</li> <li><code>dx</code>, array of grid spacing in code coordinates. (Uniform.)</li> <li><code>maxnscatt</code>, maximum number of scattering events per superphoton particle</li> <li><code>mbh</code>, black hole mass in solar masses.</li> <li><code>hslope</code>, <code>mks_smooth</code>, <code>poly_alpha</code>, <code>poly_xt</code> focusing terms used for coordinate transforms</li> <li><code>startx</code>, array of starting coordinate values for <code>X1</code>,<code>X2</code>,<code>X3</code> in code coordinates.</li> <li><code>stopx</code>, array of ending coordinate values for <code>X1</code>,<code>X2</code>,<code>X3</code> in code coordinates.</li> <li><code>tf</code>, final simulation time.</li> <li><code>variables</code> list of names of primitive state vector.</li> </ul>"},{"location":"datasets/post_neutron_star_merger/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: The 2017 detection of the in-spiral and merger of two neutron stars was a landmark discovery in astrophysics. Through a wealth of multi-messenger data, we now know that the merger of these ultracompact stellar remnants is a central engine of short gamma ray bursts and a site of r-process nucleosynthesis, where the heaviest elements in our universe are formed. The radioactive decay of unstable heavy elements produced in such mergers powers an optical and infra-red transient: The kilonova.</p> <p>One key driver of nucleosynthesis and resultant electromagnetic afterglow is wind driven by an accretion disk formed around the compact remnant. Neutrino transport plays a key role in setting the electron fraction in this outflow, thus controlling the nucleosynthesis.</p> <p>Collapsars are black hole accretion disks formed after the core of a massive, rapidly rotating star collapses to a black hole. These dramatic systems rely on much the same physics and modeling as post-merger disks, and can also be a key driver of r-processes nucleosynthesis.</p> <p>How to evaluate a new simulator operating in this space: The electron fraction Ye of material blown off from the disk is the core \"delivarable.\" It determines how heavy elements are synthesized, which in turn determines the electromagnetic counterpart as observed on Earth. This is the most important piece to get right from an emulator.</p> <p>Please cite these associated papers if you use this data in your research:</p> <pre><code>@article{miller2019nubhlight,\n  title={$\\nu$bhlight: radiation GRMHD for neutrino-driven accretion flows},\n  author={Miller, Jonah M and Ryan, Ben R and Dolence, Joshua C},\n  journal={The Astrophysical Journal Supplement Series},\n  volume={241},\n  number={2},\n  pages={30},\n  year={2019},\n  publisher={IOP Publishing}\n}\n@article{miller2019full,\n  title={Full transport model of GW170817-like disk produces a blue kilonova},\n  author={Miller, Jonah M and Ryan, Benjamin R and Dolence, Joshua C and Burrows, Adam and Fontes, Christopher J and Fryer, Christopher L and Korobkin, Oleg and Lippuner, Jonas and Mumpower, Matthew R and Wollaeger, Ryan T},\n  journal={Physical Review D},\n  volume={100},\n  number={2},\n  pages={023008},\n  year={2019},\n  publisher={APS}\n}\n@article{miller2020full,\n  title={Full transport general relativistic radiation magnetohydrodynamics for nucleosynthesis in collapsars},\n  author={Miller, Jonah M and Sprouse, Trevor M and Fryer, Christopher L and Ryan, Benjamin R and Dolence, Joshua C and Mumpower, Matthew R and Surman, Rebecca},\n  journal={The Astrophysical Journal},\n  volume={902},\n  number={1},\n  pages={66},\n  year={2020},\n  publisher={IOP Publishing}\n}\n@article{curtis2023nucleosynthesis,\n  title={Nucleosynthesis in outflows from black hole--neutron star merger disks with full gr ($\\nu$) rmhd},\n  author={Curtis, Sanjana and Miller, Jonah M and Fr{\\\"o}hlich, Carla and Sprouse, Trevor and Lloyd-Ronning, Nicole and Mumpower, Matthew},\n  journal={The Astrophysical Journal Letters},\n  volume={945},\n  number={1},\n  pages={L13},\n  year={2023},\n  publisher={IOP Publishing}\n}\n@article{lund2024magnetic,\n  title={Magnetic Field Strength Effects on Nucleosynthesis from Neutron Star Merger Outflows},\n  author={Lund, Kelsey A and McLaughlin, Gail C and Miller, Jonah M and Mumpower, Matthew R},\n  journal={The Astrophysical Journal},\n  volume={964},\n  number={2},\n  pages={111},\n  year={2024},\n  publisher={IOP Publishing}\n}\n</code></pre>"},{"location":"datasets/rayleigh_benard/","title":"Rayleigh-B\u00e9nard convection","text":"<p>One line description of the data: 2D horizontally-periodic Rayleigh-Benard convection.</p> <p>Longer description of the data: Rayleigh-B\u00e9nard convection involves fluid dynamics and thermodynamics, seen in a horizontal fluid layer heated from below, forming convective cells due to a temperature gradient. With the lower plate heated and the upper cooled, thermal energy creates density variations, initiating fluid motion. This results in B\u00e9nard cells, showcasing warm fluid rising and cool fluid descending. The interplay of buoyancy, conduction, and viscosity leads to complex fluid motion, including vortices and boundary layers.</p> <p>Note the <code>rayleigh_benard</code> and the <code>rayleigh_benard_uniform</code> are distinct only by resampling the data to different grids.</p> <p>Associated paper: Paper 1, Paper 2.</p> <p>Data generated by: Rudy Morel, Center for Computational Mathematics, Flatiron Institute.</p> <p>Code or software used to generate the data: Github repository, based on the software Dedalus.</p> <p>Equation:</p> <p>While we solve equations in the frequency domain, the original time-domain problem is</p> \\[ \\begin{align*} \\frac{\\partial b}{\\partial t} - \\kappa \\Delta b &amp; = -u\\nabla b, \\\\ \\frac{\\partial u}{\\partial t} - \\nu \\Delta u + \\nabla p - b \\vec{e}_y &amp; = -u \\nabla u, \\end{align*} \\] <p>where \\(\\Delta = \\nabla \\cdot \\nabla\\) is the spatial Laplacian, \\(b\\) is the buoyancy, \\(u = (u_x,u_y)\\) the (horizontal and vertical) velocity, and \\(p\\) is the pressure, \\(\\vec{e}_y\\) is the unit vector in the vertical direction, with the additional constraints \\(\\int p = 0\\) (pressure gauge).</p> <p>The boundary conditions vertically are as follows:</p> \\[ \\begin{align*} b(y=0) = Ly ~~~,~~~ b(y=Ly) = 0 \\\\ u(y=0) = u(y=Ly) = 0 \\end{align*} \\] <p>These PDE are parameterized by the Rayleigh and Prandtl numbers through \\(\\kappa\\) and \\(\\nu\\).</p> \\[ \\begin{align*} \\text{(thermal diffusivity)} ~~~~~~~ \\kappa &amp; = \\big(\\text{Rayleigh} * \\text{Prandtl}\\big)^{-\\frac12} \\\\ \\text{(viscosity)} ~~~~~~~ \\nu &amp; = \\bigg(\\frac{\\text{Rayleigh}}{\\text{Prandtl}}\\bigg)^{-\\frac12}. \\end{align*} \\] <p></p> Dataset FNO TFNO Unet CNextU-net <code>rayleigh_benard</code> 0.8395 \\(\\mathbf{0.6566}\\) 1.4860 0.6699 <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/rayleigh_benard/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 200 timesteps of 512 \\(\\times\\) 128 images.</p> <p>Fields are available in the data: buoyancy (scalar field), pressure (scalar field), velocity (vector field).</p> <p>Number of simulations: 1750 (35 PDE parameters \\(\\times\\) 50 initial conditions).</p> <p>Size of the ensemble of all simulations: 342 GB.</p> <p>Grid type: Cartesian coordinates. Sampled uniformly along the \\(x\\) axis. Sampled at Chebyshev nodes along the \\(y\\) axis.</p> <p>Initial conditions: the buoyancy is composed of a dumped noise added to a linear background  \\(b(t=0) = (Ly-y)\\times\\delta b_0 + y(Ly-y) \\times\\epsilon\\) where \\(\\epsilon\\) is a Gaussian white noise of scale \\(10^{-3}\\). The other fields \\(u\\) and \\(p\\) are initialized to \\(0\\).</p> <p>Boundary conditions: periodic on the horizontal direction, Dirichlet conditions on the vertical direction.</p> <p>Simulation time-step: 0.25.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max} = 50\\).</p> <p>Spatial domain size: \\(0 \\leq x \\leq 4\\) horizontally, and \\(0 \\leq y \\leq 1\\) vertically.</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(\\text{Rayleigh}\\in[1e6,1e7,1e8,1e9,1e10], \\text{Prandtl}\\in[0.1,0.2,0.5,1.0,2.0,5.0,10.0]\\). For initial conditions \\(\\delta b_0\\in[0.2,0.4,0.6,0.8,1.0]\\), the seed used to generate the initial Gaussian white noise are \\(40,\\ldots,49\\).</p> <p>Approximate time to generate the data: per input parameter from \\(\\sim6\\,000s\\) to \\(\\sim 50\\,000s\\) (high Rayleigh numbers take longer), total: \\(\\sim 60\\) hours.</p> <p>Hardware used to generate the data and precision used for generating the data: 12 nodes of 64 CPU cores with 8 processes per node, in single precision.</p>"},{"location":"datasets/rayleigh_benard/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>Rayleigh-B\u00e9nard convection datasets offer valuable insights into fluid dynamics under thermal gradients, revealing phenomena like turbulent eddies and convection cells. The position of such cells are highly sensitive to small variations in the initial conditions. Understanding these dynamics is crucial for applications in engineering and environmental science.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{burns2020dedalus,\n  title={Dedalus: A flexible framework for numerical simulations with spectral methods},\n  author={Burns, Keaton J and Vasil, Geoffrey M and Oishi, Jeffrey S and Lecoanet, Daniel and Brown, Benjamin P},\n  journal={Physical Review Research},\n  volume={2},\n  number={2},\n  pages={023068},\n  year={2020},\n  publisher={APS}\n}\n</code></pre>"},{"location":"datasets/rayleigh_taylor_instability/","title":"Rayleigh-Taylor instability","text":"<p>One line description of the data: Effect of spectral shape and component phases on development of Rayleigh-Taylor turbulence.</p> <p>Longer description of the data: We consider the Rayleigh-Taylor instability for a range of Atwood numbers and initial perturbations, all of which have a log\u2014normal horizontal energy spectrum with random phase. The dataset examines how varying the mean, standard deviation and the disparity of the random phase effects the transition to and statistics of the ensuing turbulent flow.</p> <p>Associated paper: Paper.</p> <p>Domain experts: Stefan Nixon, University of Cambridge, Romain Watteaux, CEA DAM, Suart B. Dalziel, University of Cambridge.</p> <p>Code or software used to generate the data: TurMix3D.</p> <p>Equation:The flow is governed by equations for continuity, momentum and incompressibility in the case of miscible fluids with common molecular diffusivity:</p> \\[ \\begin{align*}     \\partial_t\\rho + \\nabla\\cdot(\\rho \\vec{u}) &amp;= 0,\\\\     \\partial_t(\\rho \\vec{u})+\\nabla\\cdot(\\rho \\vec{u} \\vec{u}) &amp;= -\\nabla p + \\nabla\\cdot\\vec{\\tau}+\\rho \\vec{g},\\\\      \\nabla\\cdot\\vec{u} &amp;= -\\kappa\\nabla\\cdot\\left(\\frac{\\nabla\\rho}{\\rho}\\right). \\end{align*} \\] <p>Here, \\(\\rho\\) is density, \\(\\vec{u}\\) is velocity, \\(p\\) is pressure, \\(\\vec{g}\\) is gravity, \\(\\kappa\\) is the coefficient of molecular diffusivity and \\(\\vec{\\tau}\\) is the deviatoric stress tensor</p> \\[   \\vec{\\tau}= \\rho\\nu\\left(\\nabla\\vec{u}+\\left(\\nabla\\vec{u}\\right)^T-\\frac{2}{3}\\left(\\nabla\\cdot\\vec{u} \\right)\\vec{I}\\right), \\] <p>where \\(\\nu\\) is the kinematic viscosity and \\(\\vec{I}\\) is the identity matrix.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>rayleigh_taylor_instability</code> (At = 0.25) &gt;10 &gt;10 &gt;10 &gt;10 <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/rayleigh_taylor_instability/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 119 time-steps of 128 \\(\\times\\) 128 \\(\\times\\) 128 cubes.</p> <p>Fields available in the data: Density (scalar field), velocity (vector field).</p> <p>Number of trajectories: 45 trajectories.</p> <p>Estimated size of the ensemble of all simulations: 255.6 GB.</p> <p>Grid type: uniform grid, cartesian coordinates.</p> <p>Initial conditions: Initial conditions have been set by imposing a log\u2014normal profile for the shape of energy spectrum in wavenumber space, such that:</p> \\[ A(k) = \\frac{1}{k\\sigma\\sqrt{2\\pi}} \\exp\\Big(-\\frac{(\\ln (k) - \\mu)^2}{2\\sigma^2}\\Big) \\quad\\textrm{with}\\quad k = \\sqrt{k^2_x+k^2_y} \\] <p>where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the profile. Furthermore, we have imposed a random phase to the corresponding complex Fourier component (i.e. a random value for the argument of the complex Fourier component) between zero and a varied maximum (\\(\\phi_{max}\\)), finally after Fourier transforming to physical space the mean of the resulting profile is normalized to \\(3.10^5\\) to ensure comparable power.</p> <p>Boundary conditions: Periodic boundary conditions on sides walls and slip conditions on the top and bottom walls.</p> <p>Simulation time-step: \\(\\Delta t\\) is set such that the maximum Courant number is \\(\\frac12(CFL_{max}=0.5)\\). Therefore, the time step decreases as the flow accelerates.</p> <p>Data are stored separated by (\\(\\Delta t\\)):  varies with the Atwood number.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): Varies from \\(t_{min}=0\\) to \\(t_{max}\\) between \\(\\sim 30s\\) and \\(\\sim 100s\\), depending on Atwood number.</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): \\([0,1]\\times[0,1]\\times[0,1]\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: We run simulations with 13 different initializations for five different Atwood number values \\(At\\in \\{\\frac34, \\frac12, \\frac14, \\frac18, \\frac{1}{16}\\}\\). The first set on initial conditions considers varying the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of the profile \\(A(k)\\) with \\(\\mu\\in{1, 4, 16}\\) and \\(\\sigma\\in\\{\\frac14, \\frac12, 1\\}\\), the phase (argument of the complex Fourier component) \\(\\phi\\) was set randomly in the range \\([0,2\\pi)\\). The second set of initial conditions considers a fixed mean (\\(\\mu=16\\)) and standard deviation (\\(\\sigma =0.25\\)) and a varieed range of random phases (complex arguments \\(\\phi\\in[0,\\phi_{max}\\))) given to each Fourier component. The four cases considered are specified by \\(\\phi_{max}\\in \\{ \\frac{\\pi}{128}, \\frac{\\pi}{8}, \\frac{\\pi}{2}, \\pi\\}\\).</p> <p>Approximate time to generate the data: 1 hour on 128 CPU cores for 1 simulation. 65 hours on 128 CPU cores for all simulations.</p> <p>Hardware used to generate the data: 128 CPU core on the Ocre supercomputer at CEA, Bruy\u00e8res-le-Ch\u00e2tel, France.</p>"},{"location":"datasets/rayleigh_taylor_instability/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: In this dataset, there are three key aspects of physical interest. Firstly, impact of coherence on otherwise random initial conditions. Secondly, the effect of the shape of the initial energy spectrum on the structure of the flow. Finally, the transition from the Boussinesq to the non-Boussinesq regime where the mixing width transitions from symmetric to asymmetric growth.</p> <p>How to evaluate a new simulator operating in this space:</p> <p>From a fundamental standpoint, we would expect the density field to be advected and mixed rather than created or destroyed to give appropriate statistics. From a qualitative perspective, given that the underlying simulations are of comparable spatial resolution to the simulations run by the alpha group (Dimonte et. al. 2003), we would consider a good emulator to produce a comparable value for \u03b1 as reported in their paper for an appropriately similar set of initial conditions. This parameter is derived by considering the flow after the initial transient. At this stage, the width of the turbulent mixing zone, \\(L\\), is self-similar and grows as \\(L= \\alpha * At * g * t^2\\). They reported a value of \\(\\alpha\\)=0.025\u00b10.003. In addition, during this self-regime, we would expect to observe energy spectra with a similar shape to those reported in Cabot and Cook 2006, specifically exhibiting an appropriate \\(k^{-\\frac53}\\) cascade. From a structural perspective, we would expect that for an initialization with a large variety of modes in the initial spectrum to observe a range of bubbles and spikes (upward and downward moving structures), whereas in the other limit (where this only on mode in the initial spectrum) we expect to observe a single bubble and spike.  In addition, a good emulator would exhibit symmetric mixing with for low Atwood numbers in the Boussinesq regime (defined as \\(At\\) &lt; 0.1 by Andrews and Dalziel 2010) and asymmetries in the mixing with for large Atwood number.</p> <p>Please cite the associated paper if you use this data in your research: <pre><code>@article{cabot2006reynolds,\n  title={Reynolds number effects on Rayleigh--Taylor instability with possible implications for type Ia supernovae},\n  author={Cabot, William H and Cook, Andrew W},\n  journal={Nature Physics},\n  volume={2},\n  number={8},\n  pages={562--568},\n  year={2006},\n  publisher={Nature Publishing Group UK London}\n}\n</code></pre></p>"},{"location":"datasets/shear_flow/","title":"Periodic shear flow","text":"<p>One line description of the data: 2D periodic incompressible shear flow.</p> <p>Longer description of the data: A shear flow is a type of fluid characterized by the continuous deformation of adjacent fluid layers sliding past each other with different velocities. This phenomenon is commonly observed in various natural and engineered systems, such as rivers, atmospheric boundary layers, and industrial processes involving fluid transport. The dataset explores a 2D periodic shearflow governed by incompressible Navier-Stokes equation.</p> <p>Associated paper: Paper 1, Paper 2, Paper 3.</p> <p>Data generated by: Rudy Morel, CCM, Flatiron Institute.</p> <p>Code or software used to generate the data: Github repository, based on the software Dedalus.</p> <p>Equation:</p> <p>While we solve equations in the frequency domain, the original time-domain problem is</p> \\[ \\begin{align*} \\frac{\\partial u}{\\partial t} + \\nabla p - \\nu \\Delta u &amp; = -u\\cdot\\nabla u\\,, \\\\ \\frac{\\partial s}{\\partial t} - D \\Delta s &amp; = -u \\cdot\\nabla s\\,, \\end{align*} \\] <p>where \\(\\Delta = \\nabla \\cdot \\nabla\\) is the spatial Laplacian, \\(u = (u_x,u_y)\\) is the (horizontal and vertical) velocity, \\(s\\) is the tracer, and \\(p\\) is the pressure, with the additional constraints \\(\\int p = 0\\) (pressure gauge).</p> <p>These PDEs are parameterized by the Reynolds and Schmidt numbers through \\(\\nu\\) and \\(D\\).</p> \\[ \\begin{align*} \\text{(viscosity)} ~~~~~~~ \\nu &amp; = 1 / \\text{Reynolds} \\\\ \\text{(diffusivity)} ~~~~~~~ D &amp; = \\nu / \\text{Schmidt} \\end{align*} \\] <p></p> Dataset FNO TFNO Unet CNextU-net <code>shear_flow</code> 1.189 1.472 3.447 \\(\\mathbf{0.8080}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/shear_flow/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 200 time-steps of 256 \\(\\times\\) 512 images.</p> <p>Fields available in the data: tracer (scalar field), pressure (scalar field), velocity (vector field).</p> <p>Number of simulations: 1120 (28 PDE parameters \\(\\times\\) 40 initial conditions).</p> <p>Size of the ensemble of all simulations: 547 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: the shear field \\(u_x\\) is composed of \\(n_\\text{shear}\\) shears uniformly spaced along the \\(y\\) direction (called \"\\(z\\)\" in the generation script). Each shear is implemented with a tanh (hyperbolic tangent) \\(\\text{tanh}(5\\frac{y-y_k}{n_\\text{shear}w})\\) where \\(y_k\\) is the vertical position of the shear and \\(w\\) is a width factor. The velocity field \\(u_y\\) is composed of sinusoids along the \\(x\\) direction located at the shear. These sinusoids have an exponential decay away from the shear in the \\(y\\) direction \\(\\text{sin}(n_\\text{blobs}\\pi x)\\,e^{\\frac{25}{w^2}|y-y_k|^2}\\). The tracer matches the shear at initialization. The pressure is initialized to zero. The initial condition is thus indexed by \\(n_\\text{shear},n_\\text{blobs},w\\).</p> <p>Boundary conditions: periodic.</p> <p>Simulation time-step: 0.1 (simulation time unit).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max} = 20\\).</p> <p>Spatial domain size: \\(0\\leq x \\leq 1\\) horizontally, and \\(-1 \\leq y \\leq 1\\) vertically.</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(\\text{Reynolds}\\in[1e4, 5e4, 1e5, 5e5], \\text{Schmidt}\\in[0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]\\). For initial conditions \\(n_\\text{shear}\\in[2,4]\\), \\(n_\\text{blobs}\\in[2,3,4,5]\\), \\(w\\in[0.25, 0.5, 1.0, 2.0, 4.0]\\).</p> <p>Approximate time to generate the data: per input parameter: \\(\\sim 5h\\), total: \\(\\sim 25\\) hours.</p> <p>Hardware used to generate the data and precision used for generating the data: 512 nodes of 96 CPU cores each with 96 tasks running in parallel, in double precision.</p>"},{"location":"datasets/shear_flow/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>Shear flow are non-linear phenomena arrising in fluid mechanics and turbulence. Predicting the behavior of the shear flow under different Reynolds and Schmidt numbers is essential for a number of applications in aerodynamics, automotive, biomedical. Furthermore, such flow are unstable at large Reynolds number.</p> <p>Please cite this associated paper if you use this data in your research:</p> <pre><code>@article{burns2020dedalus,\n  title={Dedalus: A flexible framework for numerical simulations with spectral methods},\n  author={Burns, Keaton J and Vasil, Geoffrey M and Oishi, Jeffrey S and Lecoanet, Daniel and Brown, Benjamin P},\n  journal={Physical Review Research},\n  volume={2},\n  number={2},\n  pages={023068},\n  year={2020},\n  publisher={APS}\n}\n</code></pre>"},{"location":"datasets/supernova_explosion_128/","title":"Supernova Explosion in Turbulent Interstellar medium in galaxies","text":"<p>One line description of the data: Blastwave in a dense cool gas cloud.</p> <p>Longer description of the data: The simulations solve an explosion inside a compression of a monatomic ideal gas, which follows the equation of state with the specific heat ratio \\(\\gamma=5/3\\). The gas in these simulations mocks the interstellar medium in the Milky Way Galaxy. At the beginning of the simulations, the thermal energy of a supernova is dumped at the center of the simulation box. The hot (\\(\\sim 10^7\\) K) gas is immediately accelerated and makes the blastwave. Because velocities of the hot gas become supersonic, much fine resolution and small timestep are required to resolve the dynamics. The physical quantities are also distributed in seven orders of magnitude, which requires a large number of simulation steps.</p> <p>Associated paper: Paper 1, Paper 2.</p> <p>Domain expert:Keiya Hirashima, University of Tokyo &amp; CCA, Flatiron Institute.</p> <p>Code or software used to generate the data: ASURA-FDPS (Smoothed Particle Hydrodynamics), Github repository.</p> <p>Equation:</p> \\[ \\begin{align*} P&amp;=(\\gamma-1) \\rho u \\\\ \\frac{d \\rho}{dt} &amp;= -\\rho \\nabla \\cdot \\boldsymbol{v} \\\\ \\frac{d^2 \\boldsymbol{r}}{dt^2}  &amp;= -\\frac{\\nabla P}{\\rho} + \\boldsymbol{a}_{\\rm visc}-\\nabla \\Phi \\\\ \\frac{d u}{dt} &amp;= -\\frac{P}{\\rho} \\nabla \\cdot \\boldsymbol{v} + \\frac{\\Gamma-\\Lambda}{\\rho} \\end{align*} \\] <p>where \\(P\\), \\(\\rho\\), and \\(u\\) are the pressure. \\(r\\) is the position, \\(a_{\\rm visc}\\) is the acceleration generated by the viscosity, \\(\\Phi\\) is the gravitational potential, \\(\\Gamma\\) is the radiative heat influx per unit volume, and \\(\\Lambda\\) is the radiative heat outflux per unit volume.</p> <p></p>"},{"location":"datasets/supernova_explosion_128/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data 59 time-steps of  128 \\(\\times\\) 128 \\(\\times\\) 128 cubes.</p> <p>Fields available in the data: Pressure (scalar field), density (scalar field), temperature(scalar field), velocity (tensor field).</p> <p>Number of trajectories: 260.</p> <p>Estimated size of the ensemble of all simulations: 754 GB</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: \\(820\\) random seeds generated using https://github.com/amusecode/amuse/blob/main/src/amuse/ext/molecular_cloud.py (Virialized isothermal gas sphere with turbulence following the velocity spectrum \\(E(k) \\propto k^{-2}\\), which is Burgers turbulence (Burgers 1948 and Kupilas+2021 for reference)).</p> <p>Boundary conditions: open.</p> <p>Data are stored separated by (\\(\\Delta t\\)): \\(100\\) ~ \\(10\\,000\\) years (variable timesteps).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(0\\) yr to \\(0.2\\) Myr.</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): 60 pc.</p> <p>Set of coefficients or non-dimensional parameters evaluated: Initial temperature \\(T_0\\)={100K}, Initial number density of hydrogen \\(\\rho_0=\\){44.5/cc}, metallicity (effectively strength of cooling) \\(Z=\\{Z_0\\}\\).</p> <p>Approximate time to generate the data (CPU hours):</p> 0.1 M \\(\\odot\\) \\(3\\,500\\) <p>Hardware used to generate the data and precision used for generating the data: up to 1040 CPU cores per run.</p>"},{"location":"datasets/supernova_explosion_128/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: The simulations are designed as an supernova explosion, which is the explosion at the last moment of massive stars, in a high-density starforming molecular cloud with a large density contrast. An adiabatic compression of a monatomic ideal gas is assumed. To mimic the explosion, the huge thermal energy (\\(10^{51}\\) erg) is injected at the center of the calculation box and going to make the blastwave, which sweeps out the ambient gas and shells called as supernova feedback. These interactions between supernovae and surrounding gas are interesting because stars are formed in dense and cold regions.</p> <p>However, calclatig the propagation of blastwaves requires tiny timesteps to calculate and numerous integration steps. When supernova feedback is incorporated in a galaxy simulation, some functions fitted using local high resolution simulations have been used.</p> <p>How to evaluate a new simulator operating in this space: In context of galaxy simulations, the time evolution of thermal energy and momentum are important. We note that those physical quantities are not necessarily conserved because radiative cooling and heating are considered and thermal energy is seamlessly being converted into momentum.</p> <p>Please cite these associated papers if you use this data in your research:</p> <pre><code>@article{hirashima20233d,\n  title={3D-Spatiotemporal forecasting the expansion of supernova shells using deep learning towards high-resolution galaxy simulations},\n  author={Hirashima, Keiya and Moriwaki, Kana and Fujii, Michiko S and Hirai, Yutaka and Saitoh, Takayuki R and Makino, Junichiro},\n  journal={Monthly Notices of the Royal Astronomical Society},\n  volume={526},\n  number={3},\n  pages={4054--4066},\n  year={2023},\n  publisher={Oxford University Press}\n}\n@article{hirashima2023surrogate,\n  title={Surrogate Modeling for Computationally Expensive Simulations of Supernovae in High-Resolution Galaxy Simulations},\n  author={Hirashima, Keiya and Moriwaki, Kana and Fujii, Michiko S and Hirai, Yutaka and Saitoh, Takayuki R and Makino, Junichiro and Ho, Shirley},\n  journal={arXiv preprint arXiv:2311.08460},\n  year={2023}\n}\n</code></pre>"},{"location":"datasets/supernova_explosion_64/","title":"Supernova Explosion in a Turbulent Interstellar Medium in Galaxies","text":"<p>One line description of the data: Blastwave in a dense cool gas cloud.</p> <p>Longer description of the data: The simulations solve an explosion inside a compression of a monatomic ideal gas, which follows the equation of state with the specific heat ratio \\(\\gamma=5/3\\). The gas in these simulations mocks the interstellar medium in the Milky Way Galaxy. At the beginning of the simulations, the thermal energy of a supernova is dumped at the center of the simulation box. The hot (\\(\\sim 10^7\\) K) gas is immediately accelerated and makes the blastwave. Because velocities of the hot gas become supersonic, much fine resolution and small timestep are required to resolve the dynamics. The physical quantities are also distributed in seven orders of magnitude, which requires a large number of simulation steps.</p> <p>Associated paper: Paper 1, Paper 2.</p> <p>Domain expert: Keiya Hirashima, University of Tokyo &amp; CCA, Flatiron Institute.</p> <p>Code or software used to generate the data: ASURA-FDPS (Smoothed Particle Hydrodynamics), Github repository.</p> <p>Equation:</p> \\[ \\begin{align*} P&amp;=(\\gamma-1) \\rho u \\\\ \\frac{d \\rho}{dt} &amp;= -\\rho \\nabla \\cdot \\mathbf{v} \\\\ \\frac{d^2 \\mathbf{r}}{dt^2}  &amp;= -\\frac{\\nabla P}{\\rho} + \\mathbf{a}_{\\rm visc}-\\nabla \\Phi \\\\ \\frac{d u}{dt} &amp;= -\\frac{P}{\\rho} \\nabla \\cdot \\mathbf{v} + \\frac{\\Gamma-\\Lambda}{\\rho} \\end{align*} \\] <p>where \\(P\\), \\(\\rho\\), and \\(u\\) are the pressure. \\(r\\) is the position, \\(a_{\\rm visc}\\) is the acceleration generated by the viscosity, \\(\\Phi\\) is the gravitational potential, \\(\\Gamma\\) is the radiative heat influx per unit volume, and \\(\\Lambda\\) is the radiative heat outflux per unit volume.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>supernova_explosion_64</code> 0.3783 0.3785 \\(\\mathbf{0.3063}\\) 0.3181 <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/supernova_explosion_64/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data 59 time-steps of  64 \\(\\times\\) 64 \\(\\times\\) 64 cubes.</p> <p>Fields available in the data: Pressure (scalar field), density (scalar field), temperature(scalar field), velocity (tensor field).</p> <p>Number of trajectories: 740.</p> <p>Estimated size of the ensemble of all simulations: 268.2 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: \\(820\\) random seeds generated using https://github.com/amusecode/amuse/blob/main/src/amuse/ext/molecular_cloud.py (Virialized isothermal gas sphere with turbulence following the velocity spectrum \\(E(k) \\propto k^{-2}\\), which is Burgers turbulence (Burgers 1948 and Kupilas+2021 for reference)).</p> <p>Boundary conditions: open.</p> <p>Data are stored separated by (\\(\\Delta t\\)): \\(100\\) ~ \\(10\\,000\\) years (variable timesteps).</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(0\\) yr to \\(0.2\\) Myr.</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): 60 pc.</p> <p>Set of coefficients or non-dimensional parameters evaluated: Initial temperature \\(T_0\\)={100K}, Initial number density of hydrogen \\(\\rho_0=\\){44.5/cc}, metallicity (effectively strength of cooling) \\(Z=\\{Z_0\\}\\).</p> <p>Approximate time to generate the data (CPU hours):</p> 1M \\(_\\odot\\) 0.1 M \\(\\odot\\) \\(300\\) \\(3\\,500\\) <p>Hardware used to generate the data and precision used for generating the data: up to 1040 CPU cores per run.</p>"},{"location":"datasets/supernova_explosion_64/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: The simulations are designed as an supernova explosion, which is the explosion at the last moment of massive stars, in a high-density starforming molecular cloud with a large density contrast. An adiabatic compression of a monatomic ideal gas is assumed. To mimic the explosion, the huge thermal energy (\\(10^{51}\\) erg) is injected at the center of the calculation box and going to make the blastwave, which sweeps out the ambient gas and shells called as supernova feedback. These interactions between supernovae and surrounding gas are interesting because stars are formed in dense and cold regions.</p> <p>However, calclating the propagation of blastwaves requires tiny timesteps to calculate and numerous integration steps. When supernova feedback is incorporated in a galaxy simulation, some functions fitted using local high resolution simulations have been used.</p> <p>How to evaluate a new simulator operating in this space: In context of galaxy simulations, the time evolution of thermal energy and momentum are important. We note that those physical quantities are not necessarily conserved because radiative cooling and heating are considered and thermal energy is seamlessly being converted into momentum.</p> <p>Please cite these associated papers if you use this data in your research:</p> <pre><code>@article{hirashima20233d,\n  title={3D-Spatiotemporal forecasting the expansion of supernova shells using deep learning towards high-resolution galaxy simulations},\n  author={Hirashima, Keiya and Moriwaki, Kana and Fujii, Michiko S and Hirai, Yutaka and Saitoh, Takayuki R and Makino, Junichiro},\n  journal={Monthly Notices of the Royal Astronomical Society},\n  volume={526},\n  number={3},\n  pages={4054--4066},\n  year={2023},\n  publisher={Oxford University Press}\n}\n@article{hirashima2023surrogate,\n  title={Surrogate Modeling for Computationally Expensive Simulations of Supernovae in High-Resolution Galaxy Simulations},\n  author={Hirashima, Keiya and Moriwaki, Kana and Fujii, Michiko S and Hirai, Yutaka and Saitoh, Takayuki R and Makino, Junichiro and Ho, Shirley},\n  journal={arXiv preprint arXiv:2311.08460},\n  year={2023}\n}\n</code></pre>"},{"location":"datasets/turbulence_gravity_cooling/","title":"Turbulent Interstellar Medium in Galaxies","text":"<p>One line description of the data:  Turbulence in an interstellar medium in various evolution stages of galaxies.</p> <p>Longer description of the data:  These simulations are a turbulent fluid with gravity modeling interstellar medium in galaxies. These fluids make dense filaments, which will form new stars. The timescale and frequency of making new filaments are varied depending on the strength of cooling. It is parametrized by the amount of metal (metallicity), density, and temperature.</p> <p>Associated paper: Paper.</p> <p>Domain expert: Keiya Hirashima, University of Tokyo &amp; CCA, Flatiron Institute.</p> <p>Code or software used to generate the data: ASURA-FDPS (Smoothed Particle Hydrodynamics), Github repository.</p> <p>Equation:</p> \\[ \\begin{align*} P&amp;=(\\gamma-1) \\rho u \\\\ \\frac{d \\rho}{dt} &amp;= -\\rho \\nabla \\cdot \\mathbf{v} \\\\ \\frac{d^2 \\mathbf{r}}{dt^2}  &amp;= -\\frac{\\nabla P}{\\rho} + \\mathbf{a}_{\\rm visc}-\\nabla \\Phi \\\\ \\frac{d u}{dt} &amp;= -\\frac{P}{\\rho} \\nabla \\cdot \\mathbf{v} + \\frac{\\Gamma-\\Lambda}{\\rho} \\end{align*} \\] <p>where \\(P\\), \\(\\rho\\), and \\(u\\) are the pressure. \\(r\\) is the position, \\(a_{\\rm visc}\\) is the acceleration generated by the viscosity, \\(\\Phi\\) is the gravitational potential, \\(\\Gamma\\) is the radiative heat influx per unit volume, and \\(\\Lambda\\) is the radiative heat outflux per unit volume.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>turbulence_gravity_cooling</code> 0.2429 0.2673 0.6753 \\(\\mathbf{0.2096}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/turbulence_gravity_cooling/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 50 time-steps of  64 \\(\\times\\) 64 \\(\\times\\) 64 cubes.</p> <p>Fields available in the data: Pressure (scalar field), density (scalar field), temperature (scalar field), velocity (tensor field).</p> <p>Number of trajectories: 2700 (27 parameters sets \\(\\times\\) 100 runs).</p> <p>Estimated size of the ensemble of all simulations: 829.4 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: 2700 random seeds generated using https://github.com/amusecode/amuse/blob/main/src/amuse/ext/molecular_cloud.py (Virialized isothermal gas sphere with turbulence following the velocity spectrum \\(E(k) \\propto k^{-2}\\), which is Burgers turbulence (Burgers 1948 and Kupilas+2021 for reference)).</p> <p>Boundary conditions: open.</p> <p>Simulation time-step: \\(2,000\\) ~ \\(10,000\\) years (variable timesteps).</p> <p>Data are stored separated by (\\(\\Delta t\\)): 0.02 free fall time.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): 1 Free Fall time (= \\(L^3/GM\\) ); \\(L=(\\rho / \\rho_0)^{1/3} \\times 60\\) pc, \\(\\rho_0=44.5/\\rm{cc}\\), \\(M=1,000,000\\) M \\(_\\odot\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)):</p> Domain Length (\\(L\\)) Free Fall Time Snapshot (\\(\\delta t\\)) Dense (44.5 cm \\(^{-3}\\)) 60 pc 6.93 Myr 0.14 Myr Moderate (4.45 cm \\(^{-3}\\)) 129 pc 21.9 Myr 0.44 Myr Sparse (0.445 cm \\(^{-3}\\)) 278 pc 69.3 Myr 1.4 Myr <p>Set of coefficients or non-dimensional parameters evaluated: Initial temperature \\(T_0=\\{10K, 100K, 1000K\\}\\), Initial number density of hydrogen \\(\\rho_0=\\{44.5/cc, 4.45/cc, 0.445/cc\\}\\), metallicity (effectively strength of cooling) \\(Z=\\{Z_0, 0.1\\,Z_0, 0\\}\\).</p> <p>Approximate time to generate the data: \\(600\\,000\\) node hours for all simulations.</p>"},{"location":"datasets/turbulence_gravity_cooling/#for-dense-dataset-cpu-hours","title":"For dense dataset (CPU hours)","text":"Strong (1Z \\(_\\odot\\)) Weak (0.1 Z \\(_\\odot\\)) Adiabatic (0 Z \\(_\\odot\\)) \\(10\\) K \\(240\\) \\(167\\) \\(77\\) \\(100\\) K \\(453\\) \\(204\\) \\(84\\) \\(1000\\) K \\(933\\) \\(186\\) \\(46\\)"},{"location":"datasets/turbulence_gravity_cooling/#for-moderate-dataset-cpu-hours","title":"For moderate dataset (CPU hours)","text":"Strong (1Z \\(_\\odot\\)) Weak (0.1 Z \\(_\\odot\\)) Adiabatic (0 Z \\(_\\odot\\)) \\(10\\) K \\(214\\) \\(75\\) \\(62\\) \\(100\\) K \\(556\\) \\(138\\) \\(116\\) \\(1000\\) K \\(442\\) \\(208\\) \\(82\\)"},{"location":"datasets/turbulence_gravity_cooling/#for-sparse-dataset-cpu-hours","title":"For sparse dataset (CPU hours)","text":"Strong (1Z \\(_\\odot\\)) Weak (0.1 Z \\(_\\odot\\)) Adiabatic (0 Z \\(_\\odot\\)) \\(10\\) K \\(187\\) \\(102\\) \\(110\\) \\(100\\) K \\(620\\) \\(101\\) \\(92\\) \\(1000\\) K \\(286\\) \\(129\\) \\(93\\) <p>Hardware used to generate the data and precision used for generating the data: up to 1040 CPU cores per run.</p>"},{"location":"datasets/turbulence_gravity_cooling/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: Gravity, hydrodynamics and radiative cooling/heating are considered in the simulations. Radiative cooling/heating is parameterized with metallicity, which the ratio of heavier elements than helium. The larger and metallicity corresponds to the later and early stage of galaxies and universe, respectively. It also affects the time scale of cooling/heating and star formation rate. For instance, star formation happens at dense and cold region. With the strong cooling/heating rate, dense regions are quickly cooled down and generates new stars. Inversely, in the case of a weak cooling/heating, when gas is compressed, it is heated up and prevent new stars from being generated.</p> <p>In the case of cold gas with strong cooling/heating, it can easily make dense regions, which require small timesteps and a lot of integration steps. That makes it difficult to get the resolution higher.</p> <p>How to evaluate a new simulator operating in this space: The new simulator should be able to detect potential regions of star formation / potential number of newborn stars, because star formation regions are very dense and need very small timesteps, which results in a massive number of calculation steps.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{hirashima20233d,\n  title={3D-Spatiotemporal forecasting the expansion of supernova shells using deep learning towards high-resolution galaxy simulations},\n  author={Hirashima, Keiya and Moriwaki, Kana and Fujii, Michiko S and Hirai, Yutaka and Saitoh, Takayuki R and Makino, Junichiro},\n  journal={Monthly Notices of the Royal Astronomical Society},\n  volume={526},\n  number={3},\n  pages={4054--4066},\n  year={2023},\n  publisher={Oxford University Press}\n}\n</code></pre>"},{"location":"datasets/turbulent_radiative_layer_2D/","title":"Turbulent Radiative Layer - 2D","text":"<p>One line description of the data: Everywhere in astrophysical systems hot gas moves relative to cold gas, which leads to mixing, and mixing populates intermediate temperature gas that is highly reactive\u2014in this case it is rapidly cooling.</p> <p>Longer description of the data: In this simulation, there is cold, dense gas on the bottom and hot dilute gas on the top. They are moving relative to each other at highly subsonic velocities. This set up is unstable to the Kelvin Helmholtz instability, which is seeded with small scale noise that is varied between the simulations. The hot gas and cold gas are both in thermal equilibrium in the sense that the heating and cooling are exactly balanced. However, once mixing occurs as a result of the turbulence induced by the Kelvin Helmholtz instability the intermediate temperatures become populated. This intermediate temperature gas is not in thermal equilibrium and cooling beats heating. This leads to a net mass flux from the hot phase to the cold phase. This process occurs in the interstellar medium, and in the Circum-Galactic medium when cold clouds move through the ambient, hot medium. By understanding how the total cooling and mass transfer scale with the cooling rate we are able to constrain how this process controls the overall phase structure, energetics and dynamics of the gas in and around galaxies.</p> <p>Associated paper: Paper.</p> <p>Domain expert: Drummond Fielding, CCA, Flatiron Institute &amp; Cornell University.</p> <p>Code or software used to generate the data: Athena++.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{ \\partial \\rho}{\\partial t} + \\nabla \\cdot \\left( \\rho \\vec{v} \\right) &amp;= 0 \\\\ \\frac{ \\partial \\rho \\vec{v} }{\\partial t} + \\nabla \\cdot \\left( \\rho \\vec{v}\\vec{v} + P \\right) &amp;= 0 \\\\ \\frac{ \\partial E }{\\partial t} + \\nabla \\cdot \\left( (E + P) \\vec{v} \\right) &amp;= - \\frac{E}{t_{\\rm cool}} \\\\ E = P / (\\gamma -1) \\, \\, \\gamma &amp;= 5/3 \\end{align*} \\] <p>with \\(\\rho\\) the density, \\(\\vec{v}\\) the 2D velocity, \\(P\\) the pressure, \\(E\\) the total energy, and \\(t_{\\rm cool}\\) the cooling time.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>turbulent_radiative_layer_2D</code> 0.5001 0.5016 0.2418 \\(\\mathbf{0.1956}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/turbulent_radiative_layer_2D/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 101 timesteps of 384x128 images.</p> <p>Fields available in the data: Density (scalar field), pressure (scalar field), velocity (vector field).</p> <p>Number of trajectories: 90 (10 different seeds for each of the 9 \\(t_{cool}\\) values).</p> <p>Estimated size of the ensemble of all simulations: 6.9 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Analytic, described in the paper.</p> <p>Boundary conditions: Periodic in the x-direction, zero-gradient for the y-direction.</p> <p>Simulation time-step (\\(\\Delta t\\)): varies with \\(t_{cool}\\). Smallest \\(t_{cool}\\) has \\(\\Delta t = 1.36\\times10^{-2}\\) and largest \\(t_{cool}\\) has \\(\\Delta t = 1.74\\times10^{-2}\\). Not that this is not in seconds. This is in dimensionless simulation time.</p> <p>Data are stored separated by (\\(\\delta t\\)): 1.597033 in simulation time.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max} = 159.7033\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): \\(x \\in [-0.5, 0.5]\\), \\(y \\in [-1, 2]\\) giving \\(L_x = 1\\) and \\(L_y = 3\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(t_{cool} = \\{0.03, 0.06, 0.1, 0.18, 0.32, 0.56, 1.00, 1.78, 3.16\\}\\).</p> <p>Approximate time to generate the data: 84 seconds using 48 cores for one simulation. 100 CPU hours for everything.</p> <p>Hardware used to generate the data: 48 CPU cores.</p>"},{"location":"datasets/turbulent_radiative_layer_2D/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: -   The mass flux from hot to cold phase. -   The turbulent velocities. -   Amount of mass per temperature bin (T = press/dens).</p> <p>How to evaluate a new simulator operating in this space: See whether it captures the right mass flux, the right turbulent velocities, and the right amount of mass per temperature bin.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{fielding2020multiphase,\n  title={Multiphase gas and the fractal nature of radiative turbulent mixing layers},\n  author={Fielding, Drummond B and Ostriker, Eve C and Bryan, Greg L and Jermyn, Adam S},\n  journal={The Astrophysical Journal Letters},\n  volume={894},\n  number={2},\n  pages={L24},\n  year={2020},\n  publisher={IOP Publishing}\n}\n</code></pre>"},{"location":"datasets/turbulent_radiative_layer_3D/","title":"Turbulent Radiative Mixing Layers - 3D","text":"<p>One line description of the data: In many astrophysical systems, hot gas moves relative to cold gas, which leads to mixing. Mixing populates intermediate temperature gas that is highly reactive \u2014 in this case it is rapidly cooling.</p> <p>Longer description of the data: In this simulation, there is cold, dense gas on the bottom and hot dilute gas on the top. They are moving relative to each other at highly subsonic velocities. This set up is unstable to the Kelvin Helmholtz instability, which is seeded with small scale noise that is varied between the simulations. The hot gas and cold gas are both in thermal equilibrium in the sense that the heating and cooling are exactly balanced. However, once mixing occurs as a result of the turbulence induced by the Kelvin Helmholtz instability, the intermediate temperatures become populated. This intermediate temperature gas is not in thermal equilibrium, and cooling beats heating. This leads to a net mass flux from the hot phase to the cold phase. This process occurs in the interstellar medium, and in the Circum-Galactic medium when cold clouds move through the ambient, hot medium. By understanding how the total cooling and mass transfer scale with the cooling rate, we are able to constrain how this process controls the overall phase structure, energetics and dynamics of the gas in and around galaxies.</p> <p>Associated paper: Paper.</p> <p>Domain expert: Drummond Fielding, CCA, Flatiron Institute &amp; Cornell University.</p> <p>Code or software used to generate the data: Athena++.</p> <p>Equation:</p> \\[ \\begin{align*} \\frac{ \\partial \\rho}{\\partial t} + \\nabla \\cdot \\left( \\rho \\vec{v} \\right) &amp;= 0 \\\\ \\frac{ \\partial \\rho \\vec{v} }{\\partial t} + \\nabla \\cdot \\left( \\rho \\vec{v}\\vec{v} + P \\right) &amp;= 0 \\\\ \\frac{ \\partial E }{\\partial t} + \\nabla \\cdot \\left( (E + P) \\vec{v} \\right) &amp;= - \\frac{E}{t_{\\rm cool}} \\\\ E = P / (\\gamma -1) \\, \\, \\gamma &amp;= 5/3 \\end{align*} \\] <p>with \\(\\rho\\) the density, \\(\\vec{v}\\) the 3D velocity, \\(P\\) the pressure, \\(E\\) the total energy, and \\(t_{\\rm cool}\\) the cooling time.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>turbulent_radiative_layer_3D</code> 0.5278 0.5187 0.3728 \\(\\mathbf{0.3667}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/turbulent_radiative_layer_3D/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: 101 timesteps of 256 \\(\\times\\) 128 \\(\\times\\) 128 cubes.</p> <p>Fields available in the data: Density (scalar field), pressure (scalar field), velocity (vector field).</p> <p>Number of trajectories: 90 trajectories (10 different seeds for each of the 9 \\(t_{cool}\\) variations).</p> <p>Estimated size of the ensemble of all simulations: 744.6 GB.</p> <p>Grid type: uniform, cartesian coordinates.</p> <p>Initial conditions: Analytic, described in the paper.</p> <p>Boundary conditions: periodic for the 128x128 directions (\\(x,y\\)), and zero-gradient for the 256 direction (\\(z\\)).</p> <p>Simulation time-step: varies with \\(t_{cool}\\). Smallest \\(t_{cool}\\) is \\(1.32.10^{-2}\\), largest \\(t_{cool}\\) is \\(1.74.10^{-2}\\). This is not in seconds, as this is a dimensionless simulation time. To convert, the code time is \\(L_{box}/cs_{hot}\\), where \\(L_{box}\\)= 1 parsec and cs_{hot}=100km/s.</p> <p>Data are stored separated by (\\(\\Delta t\\)): data is separated by intervals of simulation time of 2.661722.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): \\(t_{min} = 0\\), \\(t_{max} = 266.172178\\).</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): \\(x,y\\in[-0.5,0.5]\\), \\(z\\in[-1,1]\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: \\(t_{cool} = \\{0.03, 0.06, 0.1, 0.18, 0.32, 0.56, 1.00, 1.78, 3.16\\}\\).</p> <p>Approximate time to generate the data: \\(34\\,560\\) CPU hours for all simulations.</p> <p>Hardware used to generate the data: each simulation was generated on a 128 core \"Rome\" node.</p>"},{"location":"datasets/turbulent_radiative_layer_3D/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: Capte the mass flux from hot to cold phase. Capture turbulent velocities. Capture the amount of mass per temperature bin (\\(T = \\frac{P}{\\rho}\\)).</p> <p>How to evaluate a new simulator operating in this space: Check whether the above physical phenomena are captured by the algorithm.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{fielding2020multiphase,\n  title={Multiphase gas and the fractal nature of radiative turbulent mixing layers},\n  author={Fielding, Drummond B and Ostriker, Eve C and Bryan, Greg L and Jermyn, Adam S},\n  journal={The Astrophysical Journal Letters},\n  volume={894},\n  number={2},\n  pages={L24},\n  year={2020},\n  publisher={IOP Publishing}\n}\n</code></pre>"},{"location":"datasets/viscoelastic_instability/","title":"Multistability of viscoelastic fluids in a 2D channel flow","text":"<p>One line description of the data: Multistability in viscoelastic flows, i.e. four different attractors (statistically stable states) are observed for the same set of parameters depending on the initial conditions.</p> <p>Longer description of the data: Elasto-inertial turbulence (EIT) is a recently discovered two-dimensional chaotic flow state observed in dilute polymer solutions. Two-dimensional direct numerical simulations show (up to) four coexistent attractors: the laminar state (LAM), a steady arrowhead regime (SAR), Elasto-inertial turbulence (EIT) and a \u2018chaotic arrowhead regime\u2019 (CAR). The SAR is stable for all parameters considered here, while the final pair of (chaotic) flow states are visually very similar and can be distinguished only by the presence of a weak polymer arrowhead structure in the CAR regime. Both chaotic regimes are maintained by an identical near-wall mechanism and the weak arrowhead does not play a role. The data set includes snapshots on the four attractors as well as two edge states. An edge state is an unstable state that exists on the boundary between two basins of attractors, the so-called edge manifold. Edge states have a single unstable direction out of the manifold and are relevant since the lie exactly on the boundary separating qualitatively different behaviours of the flow. The edge states in the present data set are obtained through edge tracking between the laminar state and EIT and between EIT and SAR.</p> <p>Associated paper: Paper.</p> <p>Domain experts: Miguel Beneitez and Richard Kerswell, DAMTP, University of Cambridge, UK.</p> <p>Code or software used to generate the data: Dedalus.</p> <p>Equation:</p> \\[ \\begin{align*} Re(\\partial_t \\mathbf{u^\\ast} + (\\mathbf{u^\\ast}\\cdot\\nabla)\\mathbf{u^\\ast} ) + \\nabla p^\\ast &amp;= \\beta \\Delta \\mathbf{u^\\ast} + (1-\\beta)\\nabla\\cdot \\mathbf{T}(\\mathbf{C^\\ast}),\\\\ \\partial_t \\mathbf{C^\\ast} + (\\mathbf{u^\\ast}\\cdot\\nabla)\\mathbf{C^\\ast} +\\mathbf{T}(\\mathbf{C^\\ast}) &amp;= \\mathbf{C^\\ast}\\cdot\\nabla \\mathbf{u^\\ast} + (\\nabla \\mathbf{u^\\ast})^T \\cdot \\mathbf{C^\\ast} + \\epsilon \\Delta \\mathbf{C^\\ast}, \\\\ \\nabla \\mathbf{u^\\ast} &amp;= 0, \\end{align*} \\] \\[ \\begin{align*} \\textrm{with} \\quad \\mathbf{T}(\\mathbf{C^\\ast}) &amp;= \\frac{1}{\\text{Wi}}(f(\\textrm{tr}(\\mathbf{C^\\ast}))\\mathbf{C^\\ast} - \\mathbf{I}),\\\\ \\textrm{and} \\quad f(s) &amp;:= \\left(1- \\frac{s-3}{L^2_{max}}\\right)^{-1}. \\end{align*} \\] <p>where \\(\\mathbf{u^\\ast} = (u^\\ast,v^\\ast)\\) is the streamwise and wall-normal velocity components, \\(p^\\ast\\) is the pressure, \\(\\mathbf{C^\\ast}\\) is the positive definite conformation tensor which represents the ensemble average of the produce of the end-to-end vector of the polymer molecules. In 2D, 4 components of the tensor are solved: \\(c_{xx}^\\ast, c_{yy}^\\ast, c_{zz}^\\ast, c_{xy}^\\ast\\). \\(\\mathbf{T}(\\mathbf{C^\\ast})\\) is the polymer stress tensor given by the FENE-P model.</p> <p></p> Dataset FNO TFNO Unet CNextU-net <code>viscoelastic_instability</code> 0.7212 0.7102 0.4185 \\(\\mathbf{0.2499}\\) <p>Table: VRMSE metrics on test sets (lower is better). Best results are shown in bold. VRMSE is scaled such that predicting the mean value of the target field results in a score of 1.</p>"},{"location":"datasets/viscoelastic_instability/#about-the-data","title":"About the data","text":"<p>Dimension of discretized data: - EIT: 34 trajectories with 60 timesteps, 512x512 images (chaotic solution). - CAR: 39 trajectories with 60 timesteps, 512x512 images (chaotic solution). - SAR: 20 trajectories with 20 timesteps, 512x512 images (simple periodic solutions). - Transition to chaos between EIT and SAR: 36 snapshots with 20 timesteps of 512x512 images. - Transition to non-chaotic state between EIT and SAR: 38 snapshots with 20 timesteps of 512x512 images. - Transition to chaos between EIT and Laminar: 43 snapshots with 20 timesteps of 512x512 images. - Transition to non-chaotic state between EIT and Laminar: 49 snapshots with 20 timesteps of 512x512 images.</p> <p>Fields available in the data: pressure (scalar field), velocity (vector field), positive conformation tensor ( \\(c_{xx}^\\ast, c_{yy}^\\ast, c_{xy}^\\ast\\) are in tensor fields, \\(c_{zz}^\\ast\\) in scalar fields).</p> <p>Number of trajectories: 260 trajectories.</p> <p>Estimated size of the ensemble of all simulations: 66 GB.</p> <p>Grid type: uniform cartesian coordinates.</p> <p>Initial conditions: - Edge trajectory: linear interpolation between a chaotic and a non-chaotic state. - SAR: continuation of the solution obtained through a linear instability at a different parameter set using time-stepping. - EIT: laminar state + blowing and suction at the walls. - CAR: SAR + blowing and suction at the walls.</p> <p>Boundary conditions: no slip conditions for the velocity \\((u^\\ast,v^\\ast)=(0,0)\\) at the wall and \\(\\epsilon=0\\) at the wall for the equation for \\(\\mathbf{C^\\ast}\\).</p> <p>Simulation time-step: various in the different states, but typically \\(\\sim 10^{-4}\\).</p> <p>Data are stored separated by (\\(\\Delta t\\)): various at different states, but typically 1.</p> <p>Total time range (\\(t_{min}\\) to \\(t_{max}\\)): depends on the simulation.</p> <p>Spatial domain size (\\(L_x\\), \\(L_y\\), \\(L_z\\)): \\(0 \\leq x \\leq 2\\pi\\), \\(-1 \\leq y \\leq 1\\).</p> <p>Set of coefficients or non-dimensional parameters evaluated: Reynold number \\(Re=1000\\), Weissenberg number \\(Wi = 50\\), \\(\\beta =0.9\\), \\(\\epsilon=2.10^{-6}\\), \\(L_{max}=70\\).</p> <p>Approximate time to generate the data: 3 months to generate all the data. It takes typically 1 day to generate \\(\\sim 50\\) snapshots.</p> <p>Hardware used to generate the data: typically 32 or 64 cores.</p>"},{"location":"datasets/viscoelastic_instability/#what-is-interesting-and-challenging-about-the-data","title":"What is interesting and challenging about the data:","text":"<p>What phenomena of physical interest are catpured in the data: The phenomena of interest in the data is: (i) chaotic dynamics in viscoelastic flows in EIT and CAR. Also note that they are separate states. (ii) multistability for the same set of parameters, the flow has four different behaviours depending on the initial conditions.</p> <p>How to evaluate a new simulator operating in this space: A new simulator would need to capture EIT/CAR adequately for a physically relevant parameter range.</p> <p>Please cite the associated paper if you use this data in your research:</p> <pre><code>@article{beneitez2024multistability,\n  title={Multistability of elasto-inertial two-dimensional channel flow},\n  author={Beneitez, Miguel and Page, Jacob and Dubief, Yves and Kerswell, Rich R},\n  journal={Journal of Fluid Mechanics},\n  volume={981},\n  pages={A30},\n  year={2024},\n  publisher={Cambridge University Press}\n}\n</code></pre>"},{"location":"tutorials/dataset/","title":"Dataset tutorial","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install the_well[benchmark]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom einops import rearrange\nfrom neuralop.models import FNO\nfrom tqdm import tqdm\n\nfrom the_well.benchmark.metrics import VRMSE\nfrom the_well.data import WellDataset\nfrom the_well.utils.download import well_download\n\ndevice = \"cuda\"\nbase_path = \"./datasets\"  # path/to/storage\n</pre> # !pip install the_well[benchmark]  import matplotlib.pyplot as plt import numpy as np import torch from einops import rearrange from neuralop.models import FNO from tqdm import tqdm  from the_well.benchmark.metrics import VRMSE from the_well.data import WellDataset from the_well.utils.download import well_download  device = \"cuda\" base_path = \"./datasets\"  # path/to/storage In\u00a0[2]: Copied! <pre>well_download(base_path=base_path, dataset=\"turbulent_radiative_layer_2D\", split=\"train\")\n</pre> well_download(base_path=base_path, dataset=\"turbulent_radiative_layer_2D\", split=\"train\") <pre>Downloading turbulent_radiative_layer_2D/train to /mnt/home/frozet/the_well/docs/tutorials/datasets/turbulent_radiative_layer_2D/data/train\n</pre> <pre>** Resuming transfer from byte position 635467072\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   197  100   197    0     0  10944      0 --:--:-- --:--:-- --:--:-- 10944\n100   197  100   197    0     0  49250      0 --:--:-- --:--:-- --:--:-- 49250\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n100   197  100   197    0     0  65666      0 --:--:-- --:--:-- --:--:-- 65666\n</pre> In\u00a0[3]: Copied! <pre>well_download(base_path=base_path, dataset=\"turbulent_radiative_layer_2D\", split=\"valid\")\n</pre> well_download(base_path=base_path, dataset=\"turbulent_radiative_layer_2D\", split=\"valid\") <pre>Downloading turbulent_radiative_layer_2D/valid to /mnt/home/frozet/the_well/docs/tutorials/datasets/turbulent_radiative_layer_2D/data/valid\n</pre> <pre>** Resuming transfer from byte position 79459648\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   197  100   197    0     0  16416      0 --:--:-- --:--:-- --:--:-- 16416\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n100   197  100   197    0     0   192k      0 --:--:-- --:--:-- --:--:--  192k\n</pre> In\u00a0[4]: Copied! <pre>dataset = WellDataset(\n    well_base_path=base_path,\n    well_dataset_name=\"turbulent_radiative_layer_2D\",\n    well_split_name=\"train\",\n    n_steps_input=4,\n    n_steps_output=1,\n    use_normalization=False,\n)\n</pre> dataset = WellDataset(     well_base_path=base_path,     well_dataset_name=\"turbulent_radiative_layer_2D\",     well_split_name=\"train\",     n_steps_input=4,     n_steps_output=1,     use_normalization=False, ) <p>The dataset object is an instance of a PyTorch dataset (<code>torch.utils.data.Dataset</code>). Each item in the dataset is a dictionnary that contains 6 elements.</p> In\u00a0[5]: Copied! <pre>item = dataset[0]\n\nlist(item.keys())\n</pre> item = dataset[0]  list(item.keys()) Out[5]: <pre>['input_fields',\n 'output_fields',\n 'constant_scalars',\n 'boundary_conditions',\n 'space_grid',\n 'input_time_grid',\n 'output_time_grid']</pre> <p>The most important elements are <code>input_fields</code> and <code>output_fields</code>. They represent the time-varying physical fields of the dynamical system and are generally the input and target of our models. For a dynamical system that has 2 spatial dimensions $x$ and $y$, <code>input_fields</code> would have a shape $(T_{in}, L_x, L_y, F)$ and <code>output_fields</code> would have a shape $(T_{out}, L_x, L_y, F)$. The number of input and output timesteps $T_{in}$ and $T_{out}$ are specified at the instantiation of the dataset with the arguments <code>n_steps_input</code> and <code>n_steps_output</code>. $L_x$ and $L_y$ are the lengths of the spatial dimensions. $F$ represents the number of physical fields, where vector fields $v = (v_x, v_y)$ and tensor fields $t = (t_{xx}, t_{xy}, t_{yx}, t_{yy})$ are flattened.</p> In\u00a0[6]: Copied! <pre>item[\"input_fields\"].shape\n</pre> item[\"input_fields\"].shape Out[6]: <pre>torch.Size([4, 128, 384, 4])</pre> In\u00a0[7]: Copied! <pre>item[\"output_fields\"].shape\n</pre> item[\"output_fields\"].shape Out[7]: <pre>torch.Size([1, 128, 384, 4])</pre> <p>One can access the names of the fields in <code>dataset.metadata.field_names</code>. The names are organized by the fields' tensor-order. In this dataset, the momentum is a vector field (first-order tensor).</p> In\u00a0[8]: Copied! <pre>dataset.metadata.field_names\n</pre> dataset.metadata.field_names Out[8]: <pre>{0: ['density', 'pressure'], 1: ['velocity_x', 'velocity_y'], 2: []}</pre> In\u00a0[9]: Copied! <pre>field_names = [\n    name for group in dataset.metadata.field_names.values() for name in group\n]\nfield_names\n</pre> field_names = [     name for group in dataset.metadata.field_names.values() for name in group ] field_names Out[9]: <pre>['density', 'pressure', 'velocity_x', 'velocity_y']</pre> <p>In an item, the input and output form a time-contiguous window in the trajectories. The total number of available windows in the dataset depends on the number of files, trajectories per file and timesteps per trajectory.</p> In\u00a0[10]: Copied! <pre>window_size = dataset.n_steps_input + dataset.n_steps_output\n\ntotal_windows = 0\nfor i in range(dataset.metadata.n_files):\n    windows_per_trajectory = (\n        dataset.metadata.n_steps_per_trajectory[i] - window_size + 1\n    )\n    total_windows += (\n        windows_per_trajectory * dataset.metadata.n_trajectories_per_file[i]\n    )\n\nprint(total_windows)\n</pre> window_size = dataset.n_steps_input + dataset.n_steps_output  total_windows = 0 for i in range(dataset.metadata.n_files):     windows_per_trajectory = (         dataset.metadata.n_steps_per_trajectory[i] - window_size + 1     )     total_windows += (         windows_per_trajectory * dataset.metadata.n_trajectories_per_file[i]     )  print(total_windows) <pre>6984\n</pre> <p>Conveniently, this corresponds to the length of the dataset.</p> In\u00a0[11]: Copied! <pre>len(dataset)\n</pre> len(dataset) Out[11]: <pre>6984</pre> In\u00a0[12]: Copied! <pre>F = dataset.metadata.n_fields\n</pre> F = dataset.metadata.n_fields In\u00a0[13]: Copied! <pre>x = dataset[42][\"input_fields\"]\nx = rearrange(x, \"T Lx Ly F -&gt; F T Lx Ly\")\n\nfig, axs = plt.subplots(F, 4, figsize=(4 * 2.4, F * 1.2))\n\nfor field in range(F):\n    vmin = np.nanmin(x[field])\n    vmax = np.nanmax(x[field])\n\n    axs[field, 0].set_ylabel(f\"{field_names[field]}\")\n\n    for t in range(4):\n        axs[field, t].imshow(\n            x[field, t], cmap=\"RdBu_r\", interpolation=\"none\", vmin=vmin, vmax=vmax\n        )\n        axs[field, t].set_xticks([])\n        axs[field, t].set_yticks([])\n\n        axs[0, t].set_title(f\"$x_{t}$\")\n\nplt.tight_layout()\n</pre> x = dataset[42][\"input_fields\"] x = rearrange(x, \"T Lx Ly F -&gt; F T Lx Ly\")  fig, axs = plt.subplots(F, 4, figsize=(4 * 2.4, F * 1.2))  for field in range(F):     vmin = np.nanmin(x[field])     vmax = np.nanmax(x[field])      axs[field, 0].set_ylabel(f\"{field_names[field]}\")      for t in range(4):         axs[field, t].imshow(             x[field, t], cmap=\"RdBu_r\", interpolation=\"none\", vmin=vmin, vmax=vmax         )         axs[field, t].set_xticks([])         axs[field, t].set_yticks([])          axs[0, t].set_title(f\"$x_{t}$\")  plt.tight_layout() In\u00a0[14]: Copied! <pre>xs = []\n\nfor i in range(0, 1000, 100):\n    x = dataset[i][\"input_fields\"]\n    xs.append(x)\n\nxs = torch.stack(xs)\n</pre> xs = []  for i in range(0, 1000, 100):     x = dataset[i][\"input_fields\"]     xs.append(x)  xs = torch.stack(xs) In\u00a0[15]: Copied! <pre>mu = xs.reshape(-1, F).mean(dim=0).to(device)\nsigma = xs.reshape(-1, F).std(dim=0).to(device)\n</pre> mu = xs.reshape(-1, F).mean(dim=0).to(device) sigma = xs.reshape(-1, F).std(dim=0).to(device) In\u00a0[16]: Copied! <pre>def preprocess(x):\n    return (x - mu) / sigma\n\n\ndef postprocess(x):\n    return sigma * x + mu\n</pre> def preprocess(x):     return (x - mu) / sigma   def postprocess(x):     return sigma * x + mu In\u00a0[17]: Copied! <pre>model = FNO(\n    n_modes=(16, 16),\n    in_channels=4 * F,\n    out_channels=1 * F,\n    hidden_channels=128,\n    n_layers=5,\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n</pre> model = FNO(     n_modes=(16, 16),     in_channels=4 * F,     out_channels=1 * F,     hidden_channels=128,     n_layers=5, ).to(device)  optimizer = torch.optim.Adam(model.parameters(), lr=5e-3) In\u00a0[18]: Copied! <pre>train_loader = torch.utils.data.DataLoader(\n    dataset=dataset,\n    shuffle=True,\n    batch_size=4,\n    num_workers=4,\n)\n\nfor epoch in range(1):\n    for batch in (bar := tqdm(train_loader)):\n        x = batch[\"input_fields\"]\n        x = x.to(device)\n        x = preprocess(x)\n        x = rearrange(x, \"B Ti Lx Ly F -&gt; B (Ti F) Lx Ly\")\n\n        y = batch[\"output_fields\"]\n        y = y.to(device)\n        y = preprocess(y)\n        y = rearrange(y, \"B To Lx Ly F -&gt; B (To F) Lx Ly\")\n\n        fx = model(x)\n\n        mse = (fx - y).square().mean()\n        mse.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n\n        bar.set_postfix(loss=mse.detach().item())\n</pre> train_loader = torch.utils.data.DataLoader(     dataset=dataset,     shuffle=True,     batch_size=4,     num_workers=4, )  for epoch in range(1):     for batch in (bar := tqdm(train_loader)):         x = batch[\"input_fields\"]         x = x.to(device)         x = preprocess(x)         x = rearrange(x, \"B Ti Lx Ly F -&gt; B (Ti F) Lx Ly\")          y = batch[\"output_fields\"]         y = y.to(device)         y = preprocess(y)         y = rearrange(y, \"B To Lx Ly F -&gt; B (To F) Lx Ly\")          fx = model(x)          mse = (fx - y).square().mean()         mse.backward()          optimizer.step()         optimizer.zero_grad()          bar.set_postfix(loss=mse.detach().item()) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1746/1746 [01:18&lt;00:00, 22.23it/s, loss=0.335] \n</pre> In\u00a0[19]: Copied! <pre>validset = WellDataset(\n    well_base_path=base_path,\n    well_dataset_name=\"turbulent_radiative_layer_2D\",\n    well_split_name=\"valid\",\n    n_steps_input=4,\n    n_steps_output=1,\n    use_normalization=False,\n)\n</pre> validset = WellDataset(     well_base_path=base_path,     well_dataset_name=\"turbulent_radiative_layer_2D\",     well_split_name=\"valid\",     n_steps_input=4,     n_steps_output=1,     use_normalization=False, ) In\u00a0[20]: Copied! <pre>item = validset[123]\n\nx = item[\"input_fields\"]\nx = x.to(device)\nx = preprocess(x)\nx = rearrange(x, \"Ti Lx Ly F -&gt; 1 (Ti F) Lx Ly\")\n\ny = item[\"output_fields\"]\ny = y.to(device)\n\nwith torch.no_grad():\n    fx = model(x)\n    fx = rearrange(fx, \"1 (To F) Lx Ly -&gt; To Lx Ly F\", F=F)\n    fx = postprocess(fx)\n\nVRMSE.eval(fx, y, meta=validset.metadata)\n</pre> item = validset[123]  x = item[\"input_fields\"] x = x.to(device) x = preprocess(x) x = rearrange(x, \"Ti Lx Ly F -&gt; 1 (Ti F) Lx Ly\")  y = item[\"output_fields\"] y = y.to(device)  with torch.no_grad():     fx = model(x)     fx = rearrange(fx, \"1 (To F) Lx Ly -&gt; To Lx Ly F\", F=F)     fx = postprocess(fx)  VRMSE.eval(fx, y, meta=validset.metadata) Out[20]: <pre>tensor([[0.2311, 1.2917, 0.5378, 0.5457]], device='cuda:0')</pre>"},{"location":"tutorials/dataset/#dataset-tutorial","title":"Dataset tutorial\u00b6","text":"<p>This notebook walks you through loading the datasets from The Well, processing the data and using a dataset to train a simple neural network.</p>"},{"location":"tutorials/dataset/#download-data","title":"Download data\u00b6","text":"<p>First let's download the data relevant for this tutorial. In this notebook, we use <code>turbulent_radiative_layer_2D</code> as it is the smallest dataset in The Well.</p>"},{"location":"tutorials/dataset/#dataset-object","title":"Dataset object\u00b6","text":"<p>To load a dataset from The Well, the easiest way is to use the <code>WellDataset</code> class.</p>"},{"location":"tutorials/dataset/#visualize-the-data","title":"Visualize the data\u00b6","text":"<p>The easiest way to visualize the data is to plot the fields separately.</p>"},{"location":"tutorials/dataset/#processing","title":"Processing\u00b6","text":"<p>In most datasets of The Well, some quantities are always positive and/or vary greatly in magnitude. These quantities should be preprocessed before being passed to a neural network. In this notebook, we standardize the fields with respect to their mean and standard deviation over a subset of the training set. Alternatively one could set <code>use_normalization=True</code> when instantiating the dataset, which would standardize the fields with respect to their mean and standard deviation over the entire training set.</p>"},{"location":"tutorials/dataset/#training","title":"Training\u00b6","text":"<p>We train a small Fourier Neural Operator (FNO) to predict the $T_{out} = 1$ next states given the $T_{in} = 4$ previous states. We concatenate the input steps along their channels, such that the model expects $T_{in} \\times F$ channels as input and $T_{out} \\times F$ channels as output. Because <code>WellDataset</code> is a PyTorch dataset, we can use it conveniently with PyTorch data-loaders.</p>"},{"location":"tutorials/dataset/#evaluation","title":"Evaluation\u00b6","text":"<p>Now that our model is trained, we can use it to make predictions. We evaluate the prediction with the variance-scaled root mean squared error (VRMSE) per field. In the manuscript, we report the VRMSE averaged over all fields.</p>"}]}